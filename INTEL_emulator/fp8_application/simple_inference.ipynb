{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of Simple Networks using FP8 Emulator\n",
    "Use two networks:\n",
    "- CNN for MNIST \n",
    "- AlexNet for CIFAR10  \n",
    "\n",
    "References:\n",
    "1. https://github.com/IntelLabs/FP8-Emulation-Toolkit/blob/main/examples/inference/classifier/imagenet_test.py \n",
    "2. https://github.com/pytorch/examples/blob/main/mnist/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asbc/anaconda3/envs/fp8_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "# import the emulator\n",
    "from mpemu import mpt_emu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "# Set CPU or GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Running on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train CNN Network\n",
    "\"\"\"\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set network for training\n",
    "    model.train()\n",
    "    \n",
    "    # Training Loop\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Define optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "    \n",
    "        # Loss Calculation\n",
    "        # The negative log likelihood loss --> It is useful to train a classification problem with C classes.\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]')\n",
    "            print(f'tLoss: {loss.item():.6f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test CNN Network\n",
    "\"\"\"\n",
    "def test(model, device, test_loader):\n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize values\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Test loop\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            # Data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  \n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)  \n",
    "            # Calculate correct predictions\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # Total test loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}')\n",
    "    print(f'Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "cnn_s = Net().to(device)\n",
    "print(cnn_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments for training and testing\n",
    "train_kwargs = {'batch_size': 64}\n",
    "test_kwargs = {'batch_size': 1000}\n",
    "\n",
    "# Include arguments for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforms for dataset\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "# Get dataset\n",
    "dataset1 = datasets.MNIST('./MNIST_data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('./MNIST_data', train=False,\n",
    "                    transform=transform)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "lr = 1.0\n",
    "gamma = 0.7\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adadelta(cnn_s.parameters(), lr=lr)\n",
    "# Scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\n",
      "tLoss: 2.310377\n",
      "Train Epoch: 1 [6400/60000 (11%)]\n",
      "tLoss: 0.313422\n",
      "Train Epoch: 1 [12800/60000 (21%)]\n",
      "tLoss: 0.260981\n",
      "Train Epoch: 1 [19200/60000 (32%)]\n",
      "tLoss: 0.180671\n",
      "Train Epoch: 1 [25600/60000 (43%)]\n",
      "tLoss: 0.319536\n",
      "Train Epoch: 1 [32000/60000 (53%)]\n",
      "tLoss: 0.073190\n",
      "Train Epoch: 1 [38400/60000 (64%)]\n",
      "tLoss: 0.092991\n",
      "Train Epoch: 1 [44800/60000 (75%)]\n",
      "tLoss: 0.067335\n",
      "Train Epoch: 1 [51200/60000 (85%)]\n",
      "tLoss: 0.179203\n",
      "Train Epoch: 1 [57600/60000 (96%)]\n",
      "tLoss: 0.303795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0437\n",
      "Accuracy: 9849/10000 (98%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/60000 (0%)]\n",
      "tLoss: 0.349314\n",
      "Train Epoch: 2 [6400/60000 (11%)]\n",
      "tLoss: 0.018424\n",
      "Train Epoch: 2 [12800/60000 (21%)]\n",
      "tLoss: 0.106881\n",
      "Train Epoch: 2 [19200/60000 (32%)]\n",
      "tLoss: 0.165347\n",
      "Train Epoch: 2 [25600/60000 (43%)]\n",
      "tLoss: 0.025091\n",
      "Train Epoch: 2 [32000/60000 (53%)]\n",
      "tLoss: 0.087487\n",
      "Train Epoch: 2 [38400/60000 (64%)]\n",
      "tLoss: 0.035303\n",
      "Train Epoch: 2 [44800/60000 (75%)]\n",
      "tLoss: 0.074044\n",
      "Train Epoch: 2 [51200/60000 (85%)]\n",
      "tLoss: 0.068245\n",
      "Train Epoch: 2 [57600/60000 (96%)]\n",
      "tLoss: 0.042863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0379\n",
      "Accuracy: 9877/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/60000 (0%)]\n",
      "tLoss: 0.008922\n",
      "Train Epoch: 3 [6400/60000 (11%)]\n",
      "tLoss: 0.101310\n",
      "Train Epoch: 3 [12800/60000 (21%)]\n",
      "tLoss: 0.016313\n",
      "Train Epoch: 3 [19200/60000 (32%)]\n",
      "tLoss: 0.056904\n",
      "Train Epoch: 3 [25600/60000 (43%)]\n",
      "tLoss: 0.007783\n",
      "Train Epoch: 3 [32000/60000 (53%)]\n",
      "tLoss: 0.144041\n",
      "Train Epoch: 3 [38400/60000 (64%)]\n",
      "tLoss: 0.069690\n",
      "Train Epoch: 3 [44800/60000 (75%)]\n",
      "tLoss: 0.029539\n",
      "Train Epoch: 3 [51200/60000 (85%)]\n",
      "tLoss: 0.068990\n",
      "Train Epoch: 3 [57600/60000 (96%)]\n",
      "tLoss: 0.040939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0348\n",
      "Accuracy: 9882/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/60000 (0%)]\n",
      "tLoss: 0.140477\n",
      "Train Epoch: 4 [6400/60000 (11%)]\n",
      "tLoss: 0.019292\n",
      "Train Epoch: 4 [12800/60000 (21%)]\n",
      "tLoss: 0.127054\n",
      "Train Epoch: 4 [19200/60000 (32%)]\n",
      "tLoss: 0.024709\n",
      "Train Epoch: 4 [25600/60000 (43%)]\n",
      "tLoss: 0.019319\n",
      "Train Epoch: 4 [32000/60000 (53%)]\n",
      "tLoss: 0.021368\n",
      "Train Epoch: 4 [38400/60000 (64%)]\n",
      "tLoss: 0.090063\n",
      "Train Epoch: 4 [44800/60000 (75%)]\n",
      "tLoss: 0.128324\n",
      "Train Epoch: 4 [51200/60000 (85%)]\n",
      "tLoss: 0.056537\n",
      "Train Epoch: 4 [57600/60000 (96%)]\n",
      "tLoss: 0.046776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0323\n",
      "Accuracy: 9887/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/60000 (0%)]\n",
      "tLoss: 0.094660\n",
      "Train Epoch: 5 [6400/60000 (11%)]\n",
      "tLoss: 0.122817\n",
      "Train Epoch: 5 [12800/60000 (21%)]\n",
      "tLoss: 0.020085\n",
      "Train Epoch: 5 [19200/60000 (32%)]\n",
      "tLoss: 0.017906\n",
      "Train Epoch: 5 [25600/60000 (43%)]\n",
      "tLoss: 0.004348\n",
      "Train Epoch: 5 [32000/60000 (53%)]\n",
      "tLoss: 0.032048\n",
      "Train Epoch: 5 [38400/60000 (64%)]\n",
      "tLoss: 0.048920\n",
      "Train Epoch: 5 [44800/60000 (75%)]\n",
      "tLoss: 0.038294\n",
      "Train Epoch: 5 [51200/60000 (85%)]\n",
      "tLoss: 0.054126\n",
      "Train Epoch: 5 [57600/60000 (96%)]\n",
      "tLoss: 0.016549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0288\n",
      "Accuracy: 9910/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/60000 (0%)]\n",
      "tLoss: 0.006779\n",
      "Train Epoch: 6 [6400/60000 (11%)]\n",
      "tLoss: 0.003196\n",
      "Train Epoch: 6 [12800/60000 (21%)]\n",
      "tLoss: 0.004638\n",
      "Train Epoch: 6 [19200/60000 (32%)]\n",
      "tLoss: 0.031004\n",
      "Train Epoch: 6 [25600/60000 (43%)]\n",
      "tLoss: 0.113164\n",
      "Train Epoch: 6 [32000/60000 (53%)]\n",
      "tLoss: 0.002338\n",
      "Train Epoch: 6 [38400/60000 (64%)]\n",
      "tLoss: 0.029165\n",
      "Train Epoch: 6 [44800/60000 (75%)]\n",
      "tLoss: 0.016110\n",
      "Train Epoch: 6 [51200/60000 (85%)]\n",
      "tLoss: 0.079549\n",
      "Train Epoch: 6 [57600/60000 (96%)]\n",
      "tLoss: 0.001859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0301\n",
      "Accuracy: 9896/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/60000 (0%)]\n",
      "tLoss: 0.000274\n",
      "Train Epoch: 7 [6400/60000 (11%)]\n",
      "tLoss: 0.002541\n",
      "Train Epoch: 7 [12800/60000 (21%)]\n",
      "tLoss: 0.048294\n",
      "Train Epoch: 7 [19200/60000 (32%)]\n",
      "tLoss: 0.035202\n",
      "Train Epoch: 7 [25600/60000 (43%)]\n",
      "tLoss: 0.084909\n",
      "Train Epoch: 7 [32000/60000 (53%)]\n",
      "tLoss: 0.005557\n",
      "Train Epoch: 7 [38400/60000 (64%)]\n",
      "tLoss: 0.109337\n",
      "Train Epoch: 7 [44800/60000 (75%)]\n",
      "tLoss: 0.041536\n",
      "Train Epoch: 7 [51200/60000 (85%)]\n",
      "tLoss: 0.001656\n",
      "Train Epoch: 7 [57600/60000 (96%)]\n",
      "tLoss: 0.048554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0298\n",
      "Accuracy: 9912/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/60000 (0%)]\n",
      "tLoss: 0.030545\n",
      "Train Epoch: 8 [6400/60000 (11%)]\n",
      "tLoss: 0.001909\n",
      "Train Epoch: 8 [12800/60000 (21%)]\n",
      "tLoss: 0.005710\n",
      "Train Epoch: 8 [19200/60000 (32%)]\n",
      "tLoss: 0.014598\n",
      "Train Epoch: 8 [25600/60000 (43%)]\n",
      "tLoss: 0.094214\n",
      "Train Epoch: 8 [32000/60000 (53%)]\n",
      "tLoss: 0.004013\n",
      "Train Epoch: 8 [38400/60000 (64%)]\n",
      "tLoss: 0.019102\n",
      "Train Epoch: 8 [44800/60000 (75%)]\n",
      "tLoss: 0.000692\n",
      "Train Epoch: 8 [51200/60000 (85%)]\n",
      "tLoss: 0.019070\n",
      "Train Epoch: 8 [57600/60000 (96%)]\n",
      "tLoss: 0.038610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0278\n",
      "Accuracy: 9908/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/60000 (0%)]\n",
      "tLoss: 0.065393\n",
      "Train Epoch: 9 [6400/60000 (11%)]\n",
      "tLoss: 0.077645\n",
      "Train Epoch: 9 [12800/60000 (21%)]\n",
      "tLoss: 0.056082\n",
      "Train Epoch: 9 [19200/60000 (32%)]\n",
      "tLoss: 0.018979\n",
      "Train Epoch: 9 [25600/60000 (43%)]\n",
      "tLoss: 0.038143\n",
      "Train Epoch: 9 [32000/60000 (53%)]\n",
      "tLoss: 0.092074\n",
      "Train Epoch: 9 [38400/60000 (64%)]\n",
      "tLoss: 0.009750\n",
      "Train Epoch: 9 [44800/60000 (75%)]\n",
      "tLoss: 0.018649\n",
      "Train Epoch: 9 [51200/60000 (85%)]\n",
      "tLoss: 0.017778\n",
      "Train Epoch: 9 [57600/60000 (96%)]\n",
      "tLoss: 0.008522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0267\n",
      "Accuracy: 9914/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/60000 (0%)]\n",
      "tLoss: 0.027950\n",
      "Train Epoch: 10 [6400/60000 (11%)]\n",
      "tLoss: 0.003279\n",
      "Train Epoch: 10 [12800/60000 (21%)]\n",
      "tLoss: 0.010466\n",
      "Train Epoch: 10 [19200/60000 (32%)]\n",
      "tLoss: 0.003281\n",
      "Train Epoch: 10 [25600/60000 (43%)]\n",
      "tLoss: 0.016939\n",
      "Train Epoch: 10 [32000/60000 (53%)]\n",
      "tLoss: 0.128306\n",
      "Train Epoch: 10 [38400/60000 (64%)]\n",
      "tLoss: 0.048794\n",
      "Train Epoch: 10 [44800/60000 (75%)]\n",
      "tLoss: 0.027070\n",
      "Train Epoch: 10 [51200/60000 (85%)]\n",
      "tLoss: 0.008950\n",
      "Train Epoch: 10 [57600/60000 (96%)]\n",
      "tLoss: 0.010081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0277\n",
      "Accuracy: 9910/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [0/60000 (0%)]\n",
      "tLoss: 0.077925\n",
      "Train Epoch: 11 [6400/60000 (11%)]\n",
      "tLoss: 0.012583\n",
      "Train Epoch: 11 [12800/60000 (21%)]\n",
      "tLoss: 0.007758\n",
      "Train Epoch: 11 [19200/60000 (32%)]\n",
      "tLoss: 0.011331\n",
      "Train Epoch: 11 [25600/60000 (43%)]\n",
      "tLoss: 0.003577\n",
      "Train Epoch: 11 [32000/60000 (53%)]\n",
      "tLoss: 0.019993\n",
      "Train Epoch: 11 [38400/60000 (64%)]\n",
      "tLoss: 0.006149\n",
      "Train Epoch: 11 [44800/60000 (75%)]\n",
      "tLoss: 0.004458\n",
      "Train Epoch: 11 [51200/60000 (85%)]\n",
      "tLoss: 0.083719\n",
      "Train Epoch: 11 [57600/60000 (96%)]\n",
      "tLoss: 0.097696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0266\n",
      "Accuracy: 9913/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/60000 (0%)]\n",
      "tLoss: 0.007980\n",
      "Train Epoch: 12 [6400/60000 (11%)]\n",
      "tLoss: 0.002048\n",
      "Train Epoch: 12 [12800/60000 (21%)]\n",
      "tLoss: 0.009649\n",
      "Train Epoch: 12 [19200/60000 (32%)]\n",
      "tLoss: 0.160508\n",
      "Train Epoch: 12 [25600/60000 (43%)]\n",
      "tLoss: 0.032507\n",
      "Train Epoch: 12 [32000/60000 (53%)]\n",
      "tLoss: 0.010444\n",
      "Train Epoch: 12 [38400/60000 (64%)]\n",
      "tLoss: 0.001407\n",
      "Train Epoch: 12 [44800/60000 (75%)]\n",
      "tLoss: 0.030772\n",
      "Train Epoch: 12 [51200/60000 (85%)]\n",
      "tLoss: 0.011948\n",
      "Train Epoch: 12 [57600/60000 (96%)]\n",
      "tLoss: 0.076268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 17.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0271\n",
      "Accuracy: 9916/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [0/60000 (0%)]\n",
      "tLoss: 0.001584\n",
      "Train Epoch: 13 [6400/60000 (11%)]\n",
      "tLoss: 0.001450\n",
      "Train Epoch: 13 [12800/60000 (21%)]\n",
      "tLoss: 0.139662\n",
      "Train Epoch: 13 [19200/60000 (32%)]\n",
      "tLoss: 0.028051\n",
      "Train Epoch: 13 [25600/60000 (43%)]\n",
      "tLoss: 0.010259\n",
      "Train Epoch: 13 [32000/60000 (53%)]\n",
      "tLoss: 0.008088\n",
      "Train Epoch: 13 [38400/60000 (64%)]\n",
      "tLoss: 0.002229\n",
      "Train Epoch: 13 [44800/60000 (75%)]\n",
      "tLoss: 0.013270\n",
      "Train Epoch: 13 [51200/60000 (85%)]\n",
      "tLoss: 0.017565\n",
      "Train Epoch: 13 [57600/60000 (96%)]\n",
      "tLoss: 0.018406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0269\n",
      "Accuracy: 9917/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [0/60000 (0%)]\n",
      "tLoss: 0.213277\n",
      "Train Epoch: 14 [6400/60000 (11%)]\n",
      "tLoss: 0.018455\n",
      "Train Epoch: 14 [12800/60000 (21%)]\n",
      "tLoss: 0.161404\n",
      "Train Epoch: 14 [19200/60000 (32%)]\n",
      "tLoss: 0.110782\n",
      "Train Epoch: 14 [25600/60000 (43%)]\n",
      "tLoss: 0.077451\n",
      "Train Epoch: 14 [32000/60000 (53%)]\n",
      "tLoss: 0.147759\n",
      "Train Epoch: 14 [38400/60000 (64%)]\n",
      "tLoss: 0.047308\n",
      "Train Epoch: 14 [44800/60000 (75%)]\n",
      "tLoss: 0.010143\n",
      "Train Epoch: 14 [51200/60000 (85%)]\n",
      "tLoss: 0.001111\n",
      "Train Epoch: 14 [57600/60000 (96%)]\n",
      "tLoss: 0.014321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0262\n",
      "Accuracy: 9919/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "epochs = 14\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model=cnn_s, device=device, train_loader=train_loader, optimizer=optimizer, epoch=epoch)\n",
    "    test(model=cnn_s, device=device, test_loader=test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the model\n",
    "torch.save(cnn_s.state_dict(), './models/mnist_cnn.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP8 Quantization\n",
    "- To E4M3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple CNN Model's state_dict:\n",
      "\n",
      "conv1.weight \t torch.Size([32, 1, 3, 3])\n",
      "conv1.bias \t torch.Size([32])\n",
      "conv2.weight \t torch.Size([64, 32, 3, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "fc1.weight \t torch.Size([128, 9216])\n",
      "fc1.bias \t torch.Size([128])\n",
      "fc2.weight \t torch.Size([10, 128])\n",
      "fc2.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Print the model's state_dict\n",
    "print(\"Simple CNN Model's state_dict:\\n\")\n",
    "for param_tensor in cnn_s.state_dict():\n",
    "    print(param_tensor, \"\\t\", cnn_s.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample weight (Original): conv2.weight\n",
      "Dimension: torch.Size([64, 32, 3, 3])\n",
      "Type: torch.float32\n",
      "tensor([[[[-0.0717,  0.0022, -0.0746],\n",
      "          [-0.0517, -0.0383, -0.0650],\n",
      "          [-0.1691, -0.1093, -0.0253]],\n",
      "\n",
      "         [[ 0.0159, -0.0175,  0.0524],\n",
      "          [-0.0760,  0.0156,  0.0738],\n",
      "          [-0.1006, -0.0111, -0.0306]],\n",
      "\n",
      "         [[-0.0450, -0.1207, -0.0332],\n",
      "          [-0.0546, -0.0301, -0.0562],\n",
      "          [-0.0882, -0.0450, -0.1424]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0561,  0.0307,  0.0729],\n",
      "          [-0.0534, -0.0089,  0.0227],\n",
      "          [ 0.0345,  0.0804,  0.0289]],\n",
      "\n",
      "         [[-0.0624, -0.0255, -0.0708],\n",
      "          [-0.0785, -0.1008,  0.0169],\n",
      "          [-0.0506, -0.1112,  0.0070]],\n",
      "\n",
      "         [[-0.0272, -0.0596, -0.1872],\n",
      "          [-0.0752,  0.0216, -0.0031],\n",
      "          [ 0.0261,  0.0010,  0.0249]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0203, -0.0349, -0.0786],\n",
      "          [-0.0359, -0.0193, -0.0436],\n",
      "          [-0.0825, -0.0650, -0.0501]],\n",
      "\n",
      "         [[-0.0522, -0.0199, -0.0134],\n",
      "          [-0.0366, -0.0169, -0.0858],\n",
      "          [-0.0869, -0.0892,  0.0774]],\n",
      "\n",
      "         [[-0.1020, -0.0665, -0.1000],\n",
      "          [-0.0664, -0.0329, -0.0571],\n",
      "          [-0.0450,  0.0060, -0.0944]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0213, -0.0344,  0.0169],\n",
      "          [-0.0533, -0.0667,  0.0579],\n",
      "          [-0.0800,  0.0167,  0.0816]],\n",
      "\n",
      "         [[-0.0687, -0.0772, -0.0297],\n",
      "          [-0.0661, -0.0062,  0.0724],\n",
      "          [-0.1044, -0.0875,  0.0597]],\n",
      "\n",
      "         [[-0.0499, -0.0448, -0.0529],\n",
      "          [-0.0314, -0.0146, -0.0178],\n",
      "          [ 0.0045, -0.0464,  0.0125]]],\n",
      "\n",
      "\n",
      "        [[[-0.0204,  0.0021, -0.0772],\n",
      "          [-0.0863, -0.0743, -0.0821],\n",
      "          [-0.0487, -0.0027, -0.0823]],\n",
      "\n",
      "         [[-0.0300,  0.0503, -0.0220],\n",
      "          [-0.0467,  0.0106, -0.0217],\n",
      "          [ 0.0280, -0.0186, -0.0954]],\n",
      "\n",
      "         [[-0.0175,  0.0175, -0.0912],\n",
      "          [ 0.1435,  0.0558,  0.0858],\n",
      "          [ 0.0007, -0.0466, -0.0307]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0418,  0.0975,  0.0395],\n",
      "          [-0.0632, -0.0225, -0.1023],\n",
      "          [-0.0776,  0.0025, -0.0357]],\n",
      "\n",
      "         [[-0.0024, -0.0850, -0.0431],\n",
      "          [ 0.0343, -0.0604, -0.0491],\n",
      "          [-0.0848, -0.0597, -0.1127]],\n",
      "\n",
      "         [[-0.0028,  0.0016, -0.0762],\n",
      "          [ 0.0374, -0.0592, -0.0617],\n",
      "          [-0.0701,  0.0327, -0.0499]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0024, -0.0246, -0.0423],\n",
      "          [-0.1549, -0.0134,  0.0022],\n",
      "          [-0.1204, -0.0352, -0.0141]],\n",
      "\n",
      "         [[ 0.0173,  0.1219, -0.0078],\n",
      "          [-0.0450,  0.0681,  0.0713],\n",
      "          [ 0.0365, -0.1116,  0.0370]],\n",
      "\n",
      "         [[ 0.0432, -0.0305, -0.0684],\n",
      "          [ 0.1149,  0.0308,  0.0258],\n",
      "          [ 0.0777, -0.0567, -0.0173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0874,  0.0661,  0.1335],\n",
      "          [-0.1809, -0.0844,  0.1707],\n",
      "          [ 0.0423, -0.0507,  0.1120]],\n",
      "\n",
      "         [[ 0.0496, -0.0258, -0.1464],\n",
      "          [ 0.0263,  0.0073, -0.0327],\n",
      "          [-0.0384,  0.0074,  0.0643]],\n",
      "\n",
      "         [[-0.0373, -0.0371, -0.1680],\n",
      "          [ 0.0542,  0.0505,  0.0230],\n",
      "          [ 0.0735,  0.0430,  0.0321]]],\n",
      "\n",
      "\n",
      "        [[[-0.0189, -0.0334,  0.0117],\n",
      "          [-0.0926,  0.0279, -0.0742],\n",
      "          [ 0.0481,  0.0644, -0.0676]],\n",
      "\n",
      "         [[ 0.2102, -0.0567, -0.1157],\n",
      "          [ 0.1999, -0.0502, -0.0430],\n",
      "          [ 0.0344, -0.0133, -0.0148]],\n",
      "\n",
      "         [[-0.1089, -0.1714, -0.0751],\n",
      "          [-0.1442, -0.1248, -0.0977],\n",
      "          [-0.2862, -0.1386, -0.1236]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0463, -0.0099, -0.0549],\n",
      "          [-0.0399, -0.0091, -0.0616],\n",
      "          [-0.1081, -0.0508, -0.0889]],\n",
      "\n",
      "         [[-0.1188, -0.0732, -0.0467],\n",
      "          [-0.0757, -0.1152, -0.0676],\n",
      "          [-0.0369, -0.1682, -0.0824]],\n",
      "\n",
      "         [[-0.0852, -0.0576, -0.0150],\n",
      "          [ 0.0168,  0.0074, -0.0043],\n",
      "          [ 0.0294,  0.0077, -0.0461]]],\n",
      "\n",
      "\n",
      "        [[[-0.1159, -0.1646,  0.0396],\n",
      "          [-0.0610, -0.0545,  0.0211],\n",
      "          [-0.0459,  0.0372, -0.0215]],\n",
      "\n",
      "         [[-0.0175,  0.1603,  0.0530],\n",
      "          [ 0.0514,  0.0649, -0.1122],\n",
      "          [-0.0299, -0.0751,  0.0096]],\n",
      "\n",
      "         [[-0.0598, -0.0311, -0.0085],\n",
      "          [ 0.0233,  0.0141, -0.0196],\n",
      "          [ 0.0854,  0.0178, -0.0175]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0623,  0.0651, -0.0349],\n",
      "          [ 0.1156, -0.1945, -0.1576],\n",
      "          [-0.1180, -0.1189, -0.0269]],\n",
      "\n",
      "         [[-0.2037, -0.0191, -0.0016],\n",
      "          [-0.0607, -0.0084, -0.0745],\n",
      "          [-0.0796, -0.1321, -0.1549]],\n",
      "\n",
      "         [[-0.1237,  0.0152, -0.0790],\n",
      "          [-0.0441, -0.0208,  0.0226],\n",
      "          [-0.0291, -0.0374, -0.0274]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print one weight sample\n",
    "sample = \"conv2.weight\"\n",
    "print(f'Sample weight (Original): {sample}')\n",
    "print(f'Dimension: {cnn_s.state_dict()[sample].shape}')\n",
    "print(f'Type: {cnn_s.state_dict()[sample].dtype}')\n",
    "print(cnn_s.state_dict()[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a deep copy of the model since the function overwrite it\n",
    "cnns_to_e4m3 = copy.deepcopy(cnn_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers exempt from conversion\n",
    "list_exempt_layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4m3 : quantizing model weights..\n",
      "[weights: [e4m3_rne, scale: per-channel, method: max], inputs: [e4m3_rne, scale: per-tensor, method: max], output: None] conv1                                   \n",
      "[weights: [e4m3_rne, scale: per-channel, method: max], inputs: [e4m3_rne, scale: per-tensor, method: max], output: None] conv2                                   \n",
      "[weights: [e4m3_rne, scale: per-channel, method: max], inputs: [e4m3_rne, scale: per-tensor, method: max], output: None] fc1                                     \n",
      "[weights: [e4m3_rne, scale: per-channel, method: max], inputs: [e4m3_rne, scale: per-tensor, method: max], output: None] fc2                                     \n"
     ]
    }
   ],
   "source": [
    "# It needs the outputs even though it overwrites in model\n",
    "cnns_e4m3, emulator = mpt_emu.quantize_model (model=cnns_to_e4m3, dtype=\"E4M3\",\n",
    "                               list_exempt_layers=list_exempt_layers, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0265\n",
      "Accuracy: 9915/10000 (99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for quantized model\n",
    "test(model=cnns_e4m3, device=device, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AlexNet for ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load the Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most up-to-date weigths\n",
    "alexnet_test = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "\n",
    "# Set the evaluation mode for inference\n",
    "# set dropout and batch normalization layers to evaluation mode before running inference. \n",
    "# Failing to do this will yield inconsistent inference results.\n",
    "alexnet_test.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Quantize the model\n",
    "- E4M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a deep copy of the model since the function overwrite it\n",
    "alexnet_to_e4m3 = copy.deepcopy(alexnet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers exempt from conversion\n",
    "list_exempt_layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4m3 : quantizing model weights..\n"
     ]
    }
   ],
   "source": [
    "# It needs the outputs even though it overwrites in model\n",
    "model_e4m3, emulator = mpt_emu.quantize_model (model=alexnet_to_e4m3, dtype=\"E4M3\",\n",
    "                               list_exempt_layers=list_exempt_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluation\n",
    "'''\n",
    "def eval_model(model, testloader):\n",
    "    # Initialize variables\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Testing Loop\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy {correct}/{total}: {100 * correct/total:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing for ImageNet\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Like INTEL example\n",
    "batch_size = 256\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./CIFAR_data', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.ImageNet(root='./Imagenet_data', split='val',\n",
    "                                        transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageNet\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./Imagenet_data\n",
      "    Split: val\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               CenterCrop(size=(224, 224))\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Result\n",
    "- https://github.com/pytorch/examples/issues/987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [03:15<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 28261/50000: 56.52 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_model(model=alexnet_test, testloader=testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FP8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [03:45<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 28124/50000: 56.25 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_model(model=model_e4m3, testloader=testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
