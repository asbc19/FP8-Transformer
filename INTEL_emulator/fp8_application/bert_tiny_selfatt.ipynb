{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-Tiny Self-Attention Layer\n",
    "- Reproduce the self-attention layer to obtain intermediate features.  \n",
    "- Use INTEL FP8 emulator for quantization.  \n",
    "\n",
    "References:\n",
    "1. https://github.com/IntelLabs/FP8-Emulation-Toolkit.git\n",
    "2. https://huggingface.co/learn/nlp-course/en/chapter6/3b?fw=pt  \n",
    "3. https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "# INTEL emulator\n",
    "from mpemu import mpt_emu\n",
    "\n",
    "import collections\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "# Set CPU or GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f'Running on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes\n",
    "From `pytorch_transformers.modeling_bert`:\n",
    "- BertEmbeddings() --> to get input encodings.\n",
    "- BertSelfAttention() --> to get Q-vectors, K-vectors, and score matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the Embedding Layer for BERT-Tiny\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Parameters from BERT-Tiny\n",
    "        self.vocab_size = 30522\n",
    "        self.hidden_size = 128\n",
    "        self.pad_token_id = 0\n",
    "        self.max_position_embeddings = 512\n",
    "        self.type_vocab_size = 2\n",
    "        self.layer_norm_eps = 1e-12\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.hidden_size, padding_idx=self.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(self.max_position_embeddings, self.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(self.type_vocab_size, self.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        # In the config.json, there is no attribute --> use the default\n",
    "        # self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.position_embedding_type = \"absolute\"\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(self.max_position_embeddings).expand((1, -1)), persistent=False\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values_length: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
    "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
    "        # issue #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Self-Attention Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Self-Attention Layer based on the original design from Hugging Face\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameters from BERT-Tiny\n",
    "        self.hidden_size = 128\n",
    "        self.num_attention_heads = 2\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        \n",
    "        # Verify the input dimensions\n",
    "        if self.hidden_size % self.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({self.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({self.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        # Calculate internal parameters\n",
    "        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Model Layers\n",
    "        self.query = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(self.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.attention_probs_dropout_prob)\n",
    "        \n",
    "        # In general, the absolute option is used --> Check positional encoding\n",
    "        # https://jaketae.github.io/study/relative-positional-encoding/\n",
    "        self.position_embedding_type = \"absolute\"\n",
    "        \n",
    "    # -> the symbol is used to indicate the type that function returns\n",
    "    # This function separates the Q, K, or V vectors for each head attention\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        # Change dimension: (batch_size, num_attention_heads, n_sample, vector_size)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "\n",
    "        # Calculate Q-vectors and separate in corresponding heads\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Calculate K-vector and separate in corresponding heads      \n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        # Calculate K-vector and separate in corresponding heads\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        # K-vector is transpose in the last two dimensions to get a column vector\n",
    "        raw_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        # Attention scores --> to ensures that the Euclidean length of the weight vectors will be approximately in the same magnitude\n",
    "        attention_scores = raw_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        # if attention_mask is not None:\n",
    "        #     # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        #     attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        # Last dimension is used because the probabilities comes from\n",
    "        # calculating one Q-vector with all K-vectors.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        ##################################################################################################\n",
    "        # # This is actually dropping out entire tokens to attend to, which might\n",
    "        # # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        # attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # # Mask heads if we want to\n",
    "        # if head_mask is not None:\n",
    "        #     attention_probs = attention_probs * head_mask\n",
    "\n",
    "        # context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        # new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        # context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        # outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        # return outputs\n",
    "        ###################################################################################################\n",
    "\n",
    "        # Return Q, K, and V vectors --> separated on head attentions\n",
    "        # Scores: Raw, Attention (scaled), Probabilities (softmax)\n",
    "        return query_layer, key_layer, value_layer, raw_scores, attention_scores, attention_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-process the validation dataset for testing\n",
    "\n",
    "IN:\n",
    "examples <-- text samples (validation dataset)\n",
    "tokenizer <-- corresponding tokenizer\n",
    "pad_on_right <-- question + context format\n",
    "max_length <-- max. length of feature\n",
    "doc_stride <-- overlap for split context\n",
    "\n",
    "OUT:\n",
    "tokenized_examples --> model inputs\n",
    "'''\n",
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differenciate between SQUAD v1 or 2\n",
    "squad_v2 = True\n",
    "\n",
    "# INTEL Example\n",
    "# model_checkpoint = \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# batch_size = 24\n",
    "\n",
    "# Model Definition\n",
    "model_checkpoint = \"mrm8488/bert-tiny-finetuned-squadv2\"\n",
    "# Reduce bacth size to get enough data for analysis\n",
    "# batch_size = 24\n",
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing definitions\n",
    "# Same in the INTEL example\n",
    "\n",
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='mrm8488/bert-tiny-finetuned-squadv2', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get the corresponding tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/bert-tiny-finetuned-squadv2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# BERT-tiny model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the corresponding dataset\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n",
      "Train Sample: \n",
      " {'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}} \n",
      "\n",
      "Test Sample: \n",
      " {'id': '56ddde6b9a695914005b9628', 'title': 'Normans', 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.', 'question': 'In what country is Normandy located?', 'answers': {'text': ['France', 'France', 'France', 'France'], 'answer_start': [159, 159, 159, 159]}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(datasets)\n",
    "print(f'Train Sample: \\n {datasets[\"train\"][0]} \\n')\n",
    "print(f'Test Sample: \\n {datasets[\"validation\"][0]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example\n",
    "- Find a short sample in the validation dataset.\n",
    "- Pre-process the example --> get the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of example\n",
    "len_low = 65\n",
    "len_up = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an example with length 69.\n",
      "Sample number 78: \n",
      " {'id': '56de1563cffd8e1900b4b5c4', 'title': 'Normans', 'context': 'The further decline of Byzantine state-of-affairs paved the road to a third attack in 1185, when a large Norman army invaded Dyrrachium, owing to the betrayal of high Byzantine officials. Some time later, Dyrrachium—one of the most important naval bases of the Adriatic—fell again to Byzantine hands.', 'question': 'Where was Dyrrachium located?', 'answers': {'text': ['the Adriatic', 'the Adriatic', 'Adriatic'], 'answer_start': [257, 257, 261]}}\n"
     ]
    }
   ],
   "source": [
    "# Find the example\n",
    "for i, example in enumerate(datasets[\"validation\"]):\n",
    "    len_f = len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) \n",
    "    if len_f > len_low and len_f < len_up:\n",
    "        print(f'Found an example with length {len_f}.')\n",
    "        break\n",
    "example = datasets[\"validation\"][i]\n",
    "print(f'Sample number {i}: \\n {example}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Question: \n",
      "Where was Dyrrachium located? \n",
      "\n",
      "Extracted Context: \n",
      "The further decline of Byzantine state-of-affairs paved the road to a third attack in 1185, when a large Norman army invaded Dyrrachium, owing to the betrayal of high Byzantine officials. Some time later, Dyrrachium—one of the most important naval bases of the Adriatic—fell again to Byzantine hands. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract question and context for inference\n",
    "question = example['question']\n",
    "print(f'Extracted Question: \\n{question} \\n')\n",
    "\n",
    "context = example['context']\n",
    "print(f'Extracted Context: \\n{context} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features: torch.Size([1, 69]): \n",
      " {'input_ids': tensor([[  101,   100,  2001,   100,  2284,  1029,   102,   100,  2582,  6689,\n",
      "          1997,   100,  2110,  1011,  1997,  1011,  3821, 12308,  1996,  2346,\n",
      "          2000,  1037,  2353,  2886,  1999, 12963,  2629,  1010,  2043,  1037,\n",
      "          2312,   100,  2390, 10836,   100,  1010, 11427,  2000,  1996, 14583,\n",
      "          1997,  2152,   100,  4584,  1012,   100,  2051,  2101,  1010,   100,\n",
      "          1517,  2028,  1997,  1996,  2087,  2590,  3987,  7888,  1997,  1996,\n",
      "           100,  1517,  3062,  2153,  2000,   100,  2398,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} \n",
      "\n",
      "Decoded Input Features:\n",
      "[CLS] [UNK] was [UNK] located? [SEP] [UNK] further decline of [UNK] state - of - affairs paved the road to a third attack in 1185, when a large [UNK] army invaded [UNK], owing to the betrayal of high [UNK] officials. [UNK] time later, [UNK] — one of the most important naval bases of the [UNK] — fell again to [UNK] hands. [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "print(f'Input Features: {inputs[\"input_ids\"].shape}: \\n {inputs} \\n')\n",
    "\n",
    "# Tokens [UNK] shows the words that are not part of the vocabulary.\n",
    "print('Decoded Input Features:')\n",
    "for x in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,   100,  2001,   100,  2284,  1029,   102,   100,  2582,  6689,\n",
      "          1997,   100,  2110,  1011,  1997,  1011,  3821, 12308,  1996,  2346,\n",
      "          2000,  1037,  2353,  2886,  1999, 12963,  2629,  1010,  2043,  1037,\n",
      "          2312,   100,  2390, 10836,   100,  1010, 11427,  2000,  1996, 14583,\n",
      "          1997,  2152,   100,  4584,  1012,   100,  2051,  2101,  1010,   100,\n",
      "          1517,  2028,  1997,  1996,  2087,  2590,  3987,  7888,  1997,  1996,\n",
      "           100,  1517,  3062,  2153,  2000,   100,  2398,  1012,   102]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 69])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# For the embedding layer --> only the ids\n",
    "input_features = inputs[\"input_ids\"]\n",
    "\n",
    "# Send to device (tensor)\n",
    "input_features = input_features.to(device)\n",
    "\n",
    "print(input_features)\n",
    "print(input_features.shape)\n",
    "print(input_features.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation Dataset\n",
    "- To determine the distribution inside the self-attention layer.\n",
    "- Take 24 random samples, each one with 384 tokens.\n",
    "    - Produce around 1M data points for vectors Q, K, and V.\n",
    "    - Produce around 7M data points for scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "    num_rows: 12106\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Format question + context\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "inputs_val = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")\n",
    "print(inputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 100, 2003, 1996, 3399, 1997, 6622, 2241, 2006, 1029, 102, 100, 6958, 1997, 15018, 2140, 8338, 2003, 2241, 2006, 1996, 3311, 1997, 11954, 1999, 25503, 5749, 1012, 100, 11767, 4839, 2012, 1996, 2168, 2051, 2558, 2802, 1996, 2088, 1010, 2037, 3739, 2030, 1006, 2823, 1007, 6438, 2089, 2022, 2109, 2000, 3073, 1037, 5816, 2287, 1997, 1996, 13197, 1999, 2029, 2027, 2024, 2179, 1012, 100, 2006, 6481, 4201, 2041, 2011, 100, 100, 2471, 1037, 3634, 2086, 2077, 1996, 4772, 1997, 100, 100, 1005, 1055, 3399, 1997, 6622, 1010, 1996, 6481, 1997, 8338, 2020, 2764, 9174, 1997, 12761, 2245, 1012, 100, 6958, 4150, 3243, 3375, 1010, 2174, 1010, 2445, 1996, 9662, 7368, 1997, 10725, 3989, 1010, 1996, 2334, 3989, 1997, 10725, 4127, 2349, 2000, 11457, 3431, 1999, 6552, 1006, 6904, 9243, 2689, 1999, 25503, 22913, 1007, 1010, 1998, 2008, 2025, 2035, 11954, 2089, 2022, 2179, 16452, 2012, 1996, 2168, 2051, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "24\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Prepare 24 random samples\n",
    "n_sample = 24\n",
    "input_features_val = []\n",
    "\n",
    "for i in range(n_sample):\n",
    "    # Choose a random sample from validation dataset\n",
    "    input_features_val.append(random.choice(inputs_val[\"input_ids\"]))\n",
    "\n",
    "# Debug\n",
    "print(input_features_val[0])\n",
    "print(len(input_features_val))\n",
    "print(type(input_features_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  101,   100,  2003,  ...,     0,     0,     0],\n",
      "         [  101,   100,  2003,  ...,     0,     0,     0],\n",
      "         [  101,   100,  4490,  ...,     0,     0,     0],\n",
      "         ...,\n",
      "         [  101,   100, 19470,  ...,     0,     0,     0],\n",
      "         [  101,   100,  2312,  ...,     0,     0,     0],\n",
      "         [  101,   100,  2116,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,   100,  2515,  ...,     0,     0,     0],\n",
      "         [  101,   100,  2095,  ...,     0,     0,     0],\n",
      "         [  101,   100,  3596,  ...,     0,     0,     0],\n",
      "         ...,\n",
      "         [  101,   100, 14788,  ...,     0,     0,     0],\n",
      "         [  101,   100,  2003,  ...,     0,     0,     0],\n",
      "         [  101,   100,  2116,  ...,     0,     0,     0]]], device='cuda:0')\n",
      "torch.Size([2, 12, 384])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# For the embedding layer --> only the ids\n",
    "# The results is a list and need to become a tensor\n",
    "input_features_val = torch.Tensor(input_features_val).type(torch.int64)\n",
    "\n",
    "# Arrange the input features in batches run inference --> avoid crashing the kernel or GPU\n",
    "# Dropping some elements at the end\n",
    "input_features_val = torch.reshape(input_features_val, (-1,batch_size,max_length))\n",
    "\n",
    "# Send to device\n",
    "input_features_val = input_features_val.to(device)\n",
    "\n",
    "print(input_features_val)\n",
    "print(input_features_val.shape)\n",
    "print(input_features_val.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embeddings\n",
    "- Load the correspoding weights from BERT-tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 128)\n",
      "  (token_type_embeddings): Embedding(2, 128)\n",
      "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate BERT-tiny Embedding Layer\n",
    "btiny_embed_layer = BertEmbeddings()\n",
    "print(btiny_embed_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load weigths for Word Embeddings Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004102</td>\n",
       "      <td>-0.030695</td>\n",
       "      <td>-0.003530</td>\n",
       "      <td>-0.411438</td>\n",
       "      <td>-0.014942</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.026952</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.051232</td>\n",
       "      <td>0.009227</td>\n",
       "      <td>-0.011709</td>\n",
       "      <td>0.008851</td>\n",
       "      <td>-0.014839</td>\n",
       "      <td>-0.167650</td>\n",
       "      <td>0.018925</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>-0.002923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000427</td>\n",
       "      <td>-0.036929</td>\n",
       "      <td>-0.017168</td>\n",
       "      <td>-0.423627</td>\n",
       "      <td>0.010980</td>\n",
       "      <td>0.010359</td>\n",
       "      <td>0.034110</td>\n",
       "      <td>-0.019472</td>\n",
       "      <td>0.055122</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050806</td>\n",
       "      <td>0.036249</td>\n",
       "      <td>-0.011282</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.018930</td>\n",
       "      <td>-0.042994</td>\n",
       "      <td>-0.151992</td>\n",
       "      <td>0.029314</td>\n",
       "      <td>-0.010398</td>\n",
       "      <td>0.026772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>-0.019566</td>\n",
       "      <td>-0.385447</td>\n",
       "      <td>0.013480</td>\n",
       "      <td>0.034551</td>\n",
       "      <td>0.014485</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.056799</td>\n",
       "      <td>0.033259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013886</td>\n",
       "      <td>0.032323</td>\n",
       "      <td>-0.003076</td>\n",
       "      <td>0.017598</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>-0.045568</td>\n",
       "      <td>-0.178516</td>\n",
       "      <td>0.016799</td>\n",
       "      <td>-0.027802</td>\n",
       "      <td>-0.006902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000417</td>\n",
       "      <td>-0.050860</td>\n",
       "      <td>-0.016444</td>\n",
       "      <td>-0.558925</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>0.029225</td>\n",
       "      <td>-0.021511</td>\n",
       "      <td>0.032243</td>\n",
       "      <td>0.031892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052648</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.005378</td>\n",
       "      <td>0.029141</td>\n",
       "      <td>-0.026603</td>\n",
       "      <td>-0.123795</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>-0.020112</td>\n",
       "      <td>0.008221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>-0.512898</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.019733</td>\n",
       "      <td>0.040316</td>\n",
       "      <td>-0.006930</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>0.036560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036957</td>\n",
       "      <td>0.033785</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>-0.015781</td>\n",
       "      <td>0.028073</td>\n",
       "      <td>-0.004094</td>\n",
       "      <td>-0.178751</td>\n",
       "      <td>0.011329</td>\n",
       "      <td>-0.032976</td>\n",
       "      <td>0.037174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30517</th>\n",
       "      <td>0.008556</td>\n",
       "      <td>-0.013608</td>\n",
       "      <td>0.016184</td>\n",
       "      <td>-0.466869</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>0.019206</td>\n",
       "      <td>0.024662</td>\n",
       "      <td>-0.006580</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.021744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032921</td>\n",
       "      <td>0.040517</td>\n",
       "      <td>0.016282</td>\n",
       "      <td>-0.006558</td>\n",
       "      <td>0.038389</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>-0.154879</td>\n",
       "      <td>0.026375</td>\n",
       "      <td>0.004540</td>\n",
       "      <td>0.013948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30518</th>\n",
       "      <td>-0.004366</td>\n",
       "      <td>-0.028112</td>\n",
       "      <td>-0.012219</td>\n",
       "      <td>-0.485433</td>\n",
       "      <td>-0.017654</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>0.013574</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.041075</td>\n",
       "      <td>0.037646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022182</td>\n",
       "      <td>0.057581</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>-0.018174</td>\n",
       "      <td>-0.138781</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>-0.002499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30519</th>\n",
       "      <td>0.035573</td>\n",
       "      <td>-0.015891</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>-0.441475</td>\n",
       "      <td>0.024180</td>\n",
       "      <td>-0.004004</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>-0.018132</td>\n",
       "      <td>0.043679</td>\n",
       "      <td>0.041178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019628</td>\n",
       "      <td>0.015768</td>\n",
       "      <td>-0.005383</td>\n",
       "      <td>0.016564</td>\n",
       "      <td>0.052644</td>\n",
       "      <td>-0.016092</td>\n",
       "      <td>-0.154145</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>-0.011270</td>\n",
       "      <td>-0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30520</th>\n",
       "      <td>-0.008702</td>\n",
       "      <td>-0.022516</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>-0.396877</td>\n",
       "      <td>0.022990</td>\n",
       "      <td>0.017911</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.031892</td>\n",
       "      <td>0.025294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018768</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>-0.000321</td>\n",
       "      <td>-0.187129</td>\n",
       "      <td>0.027591</td>\n",
       "      <td>-0.019554</td>\n",
       "      <td>0.002402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30521</th>\n",
       "      <td>-0.078904</td>\n",
       "      <td>-0.075407</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>-0.099882</td>\n",
       "      <td>0.032066</td>\n",
       "      <td>0.061156</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>-0.039407</td>\n",
       "      <td>0.050175</td>\n",
       "      <td>0.106586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>-0.078224</td>\n",
       "      <td>-0.031424</td>\n",
       "      <td>0.099277</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>-0.095441</td>\n",
       "      <td>-0.153173</td>\n",
       "      <td>-0.005334</td>\n",
       "      <td>-0.044993</td>\n",
       "      <td>0.059842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30522 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.004102 -0.030695 -0.003530 -0.411438 -0.014942  0.005746  0.023953   \n",
       "1     -0.000427 -0.036929 -0.017168 -0.423627  0.010980  0.010359  0.034110   \n",
       "2      0.005942  0.004212 -0.019566 -0.385447  0.013480  0.034551  0.014485   \n",
       "3      0.000417 -0.050860 -0.016444 -0.558925  0.003685  0.037643  0.029225   \n",
       "4      0.010791  0.001671  0.013703 -0.512898  0.007565  0.019733  0.040316   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30517  0.008556 -0.013608  0.016184 -0.466869  0.004890  0.019206  0.024662   \n",
       "30518 -0.004366 -0.028112 -0.012219 -0.485433 -0.017654  0.011989  0.013574   \n",
       "30519  0.035573 -0.015891  0.004995 -0.441475  0.024180 -0.004004  0.007571   \n",
       "30520 -0.008702 -0.022516  0.003199 -0.396877  0.022990  0.017911  0.018286   \n",
       "30521 -0.078904 -0.075407 -0.004666 -0.099882  0.032066  0.061156  0.019276   \n",
       "\n",
       "            7         8         9    ...       118       119       120  \\\n",
       "0      0.000162  0.026952  0.026791  ...  0.009847  0.051232  0.009227   \n",
       "1     -0.019472  0.055122  0.031442  ...  0.050806  0.036249 -0.011282   \n",
       "2      0.002080  0.056799  0.033259  ...  0.013886  0.032323 -0.003076   \n",
       "3     -0.021511  0.032243  0.031892  ...  0.052648  0.030534  0.006766   \n",
       "4     -0.006930  0.044723  0.036560  ...  0.036957  0.033785  0.007198   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "30517 -0.006580  0.021015  0.021744  ...  0.032921  0.040517  0.016282   \n",
       "30518  0.007398  0.041075  0.037646  ...  0.022182  0.057581  0.008038   \n",
       "30519 -0.018132  0.043679  0.041178  ...  0.019628  0.015768 -0.005383   \n",
       "30520  0.004663  0.031892  0.025294  ...  0.018768  0.017948 -0.003442   \n",
       "30521 -0.039407  0.050175  0.106586  ...  0.014634 -0.078224 -0.031424   \n",
       "\n",
       "            121       122       123       124       125       126       127  \n",
       "0     -0.011709  0.008851 -0.014839 -0.167650  0.018925  0.003740 -0.002923  \n",
       "1      0.014735  0.018930 -0.042994 -0.151992  0.029314 -0.010398  0.026772  \n",
       "2      0.017598  0.012769 -0.045568 -0.178516  0.016799 -0.027802 -0.006902  \n",
       "3      0.005378  0.029141 -0.026603 -0.123795  0.028228 -0.020112  0.008221  \n",
       "4     -0.015781  0.028073 -0.004094 -0.178751  0.011329 -0.032976  0.037174  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "30517 -0.006558  0.038389  0.004930 -0.154879  0.026375  0.004540  0.013948  \n",
       "30518  0.006264  0.002978 -0.018174 -0.138781  0.006097 -0.000765 -0.002499  \n",
       "30519  0.016564  0.052644 -0.016092 -0.154145  0.005407 -0.011270 -0.000070  \n",
       "30520  0.007416  0.019297 -0.000321 -0.187129  0.027591 -0.019554  0.002402  \n",
       "30521  0.099277  0.005686 -0.095441 -0.153173 -0.005334 -0.044993  0.059842  \n",
       "\n",
       "[30522 rows x 128 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1018e-03, -3.0695e-02, -3.5295e-03,  ...,  1.8925e-02,\n",
      "          3.7396e-03, -2.9233e-03],\n",
      "        [-4.2748e-04, -3.6929e-02, -1.7168e-02,  ...,  2.9314e-02,\n",
      "         -1.0398e-02,  2.6772e-02],\n",
      "        [ 5.9418e-03,  4.2119e-03, -1.9566e-02,  ...,  1.6799e-02,\n",
      "         -2.7802e-02, -6.9017e-03],\n",
      "        ...,\n",
      "        [ 3.5573e-02, -1.5891e-02,  4.9951e-03,  ...,  5.4071e-03,\n",
      "         -1.1270e-02, -6.9528e-05],\n",
      "        [-8.7018e-03, -2.2516e-02,  3.1993e-03,  ...,  2.7591e-02,\n",
      "         -1.9554e-02,  2.4023e-03],\n",
      "        [-7.8904e-02, -7.5407e-02, -4.6660e-03,  ..., -5.3340e-03,\n",
      "         -4.4993e-02,  5.9842e-02]])\n",
      "torch.Size([30522, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/w_emb_weight_embedding.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "w_embedding = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "w_embedding = torch.reshape(w_embedding, btiny_embed_layer.word_embeddings.weight.shape)\n",
    "\n",
    "\n",
    "print(w_embedding)\n",
    "print(w_embedding.shape)\n",
    "print(w_embedding.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_embed_layer.word_embeddings.weight = torch.nn.Parameter(w_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model with loaded params:\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-4.1018e-03, -3.0695e-02, -3.5295e-03,  ...,  1.8925e-02,\n",
      "          3.7396e-03, -2.9233e-03],\n",
      "        [-4.2748e-04, -3.6929e-02, -1.7168e-02,  ...,  2.9314e-02,\n",
      "         -1.0398e-02,  2.6772e-02],\n",
      "        [ 5.9418e-03,  4.2119e-03, -1.9566e-02,  ...,  1.6799e-02,\n",
      "         -2.7802e-02, -6.9017e-03],\n",
      "        ...,\n",
      "        [ 3.5573e-02, -1.5891e-02,  4.9951e-03,  ...,  5.4071e-03,\n",
      "         -1.1270e-02, -6.9528e-05],\n",
      "        [-8.7018e-03, -2.2516e-02,  3.1993e-03,  ...,  2.7591e-02,\n",
      "         -1.9554e-02,  2.4023e-03],\n",
      "        [-7.8904e-02, -7.5407e-02, -4.6660e-03,  ..., -5.3340e-03,\n",
      "         -4.4993e-02,  5.9842e-02]], requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print('\\n\\nModel with loaded params:')\n",
    "for name, param in btiny_embed_layer.word_embeddings.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load weigths for Position Embeddings Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048799</td>\n",
       "      <td>-0.021985</td>\n",
       "      <td>-0.512618</td>\n",
       "      <td>-0.007519</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>-0.013943</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.049354</td>\n",
       "      <td>-0.005497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>-0.002644</td>\n",
       "      <td>-0.046888</td>\n",
       "      <td>-0.019192</td>\n",
       "      <td>0.086546</td>\n",
       "      <td>-0.073178</td>\n",
       "      <td>0.080832</td>\n",
       "      <td>-0.009050</td>\n",
       "      <td>0.012282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.035620</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>-0.036291</td>\n",
       "      <td>-0.025621</td>\n",
       "      <td>-0.036160</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>-0.004186</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>-0.039869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022351</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>-0.016530</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>-0.035351</td>\n",
       "      <td>-0.068163</td>\n",
       "      <td>-0.060463</td>\n",
       "      <td>-0.052477</td>\n",
       "      <td>-0.009874</td>\n",
       "      <td>-0.051069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.052387</td>\n",
       "      <td>-0.007274</td>\n",
       "      <td>-0.025076</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.033572</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.032732</td>\n",
       "      <td>-0.040867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017271</td>\n",
       "      <td>-0.002181</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>-0.021568</td>\n",
       "      <td>-0.059764</td>\n",
       "      <td>-0.045783</td>\n",
       "      <td>-0.041533</td>\n",
       "      <td>0.027022</td>\n",
       "      <td>-0.009572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.035884</td>\n",
       "      <td>-0.006201</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>-0.005845</td>\n",
       "      <td>-0.022721</td>\n",
       "      <td>0.030334</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>0.014519</td>\n",
       "      <td>0.025118</td>\n",
       "      <td>-0.033847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002718</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>-0.002802</td>\n",
       "      <td>-0.018078</td>\n",
       "      <td>0.014845</td>\n",
       "      <td>-0.034942</td>\n",
       "      <td>-0.044802</td>\n",
       "      <td>-0.037966</td>\n",
       "      <td>0.031754</td>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036029</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>-0.013844</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>-0.003605</td>\n",
       "      <td>0.027647</td>\n",
       "      <td>0.023641</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>-0.039015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021709</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>-0.011703</td>\n",
       "      <td>-0.006383</td>\n",
       "      <td>0.030896</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>-0.070968</td>\n",
       "      <td>-0.045872</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.013205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>-0.019778</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>0.057292</td>\n",
       "      <td>-0.013457</td>\n",
       "      <td>-0.042316</td>\n",
       "      <td>-0.045250</td>\n",
       "      <td>-0.004120</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>-0.025485</td>\n",
       "      <td>-0.009929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021269</td>\n",
       "      <td>-0.025991</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>-0.018824</td>\n",
       "      <td>-0.025798</td>\n",
       "      <td>-0.037680</td>\n",
       "      <td>-0.032863</td>\n",
       "      <td>-0.057193</td>\n",
       "      <td>0.012679</td>\n",
       "      <td>-0.026734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>-0.031045</td>\n",
       "      <td>-0.021198</td>\n",
       "      <td>0.048681</td>\n",
       "      <td>-0.019791</td>\n",
       "      <td>-0.032540</td>\n",
       "      <td>-0.022947</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>0.016184</td>\n",
       "      <td>-0.031092</td>\n",
       "      <td>-0.015421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016641</td>\n",
       "      <td>-0.011229</td>\n",
       "      <td>0.013116</td>\n",
       "      <td>-0.031141</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>-0.013063</td>\n",
       "      <td>-0.049673</td>\n",
       "      <td>-0.068013</td>\n",
       "      <td>0.028564</td>\n",
       "      <td>-0.027271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>-0.027013</td>\n",
       "      <td>0.007404</td>\n",
       "      <td>0.029932</td>\n",
       "      <td>-0.011022</td>\n",
       "      <td>-0.018286</td>\n",
       "      <td>-0.004139</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.025712</td>\n",
       "      <td>-0.047099</td>\n",
       "      <td>-0.001434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>0.007969</td>\n",
       "      <td>0.015612</td>\n",
       "      <td>-0.017325</td>\n",
       "      <td>0.032569</td>\n",
       "      <td>0.021893</td>\n",
       "      <td>-0.061052</td>\n",
       "      <td>-0.065008</td>\n",
       "      <td>0.022377</td>\n",
       "      <td>-0.034998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>-0.034684</td>\n",
       "      <td>0.028806</td>\n",
       "      <td>-0.037017</td>\n",
       "      <td>0.030278</td>\n",
       "      <td>-0.031173</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>-0.062641</td>\n",
       "      <td>-0.019988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016922</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>-0.017013</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>0.050202</td>\n",
       "      <td>-0.165319</td>\n",
       "      <td>-0.011996</td>\n",
       "      <td>-0.021707</td>\n",
       "      <td>-0.100012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.083364</td>\n",
       "      <td>-0.086571</td>\n",
       "      <td>-0.417024</td>\n",
       "      <td>0.025538</td>\n",
       "      <td>0.061136</td>\n",
       "      <td>-0.036585</td>\n",
       "      <td>0.073882</td>\n",
       "      <td>0.011515</td>\n",
       "      <td>0.046984</td>\n",
       "      <td>-0.048719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025211</td>\n",
       "      <td>-0.073287</td>\n",
       "      <td>-0.018666</td>\n",
       "      <td>-0.032916</td>\n",
       "      <td>-0.022071</td>\n",
       "      <td>0.169587</td>\n",
       "      <td>-0.020448</td>\n",
       "      <td>0.041042</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>0.048359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    0.048799 -0.021985 -0.512618 -0.007519  0.004999 -0.001290 -0.013943   \n",
       "1   -0.035620  0.008837 -0.036291 -0.025621 -0.036160  0.008466 -0.004186   \n",
       "2   -0.052387 -0.007274 -0.025076 -0.008414 -0.033572  0.003951  0.004875   \n",
       "3   -0.035884 -0.006201 -0.015133 -0.005845 -0.022721  0.030334  0.029940   \n",
       "4   -0.036029  0.007621 -0.013844  0.007194 -0.003605  0.027647  0.023641   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "507 -0.019778 -0.025677  0.057292 -0.013457 -0.042316 -0.045250 -0.004120   \n",
       "508 -0.031045 -0.021198  0.048681 -0.019791 -0.032540 -0.022947  0.013226   \n",
       "509 -0.027013  0.007404  0.029932 -0.011022 -0.018286 -0.004139  0.023392   \n",
       "510 -0.034684  0.028806 -0.037017  0.030278 -0.031173  0.004782  0.031265   \n",
       "511  0.083364 -0.086571 -0.417024  0.025538  0.061136 -0.036585  0.073882   \n",
       "\n",
       "          7         8         9    ...       118       119       120  \\\n",
       "0    0.019414  0.049354 -0.005497  ...  0.000633  0.004712 -0.002644   \n",
       "1    0.002189  0.018750 -0.039869  ...  0.022351  0.008900 -0.016530   \n",
       "2    0.001596  0.032732 -0.040867  ...  0.017271 -0.002181  0.000754   \n",
       "3    0.014519  0.025118 -0.033847  ... -0.002718  0.006820 -0.002802   \n",
       "4    0.008893  0.002001 -0.039015  ... -0.021709  0.015600 -0.011703   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "507  0.000235 -0.025485 -0.009929  ...  0.021269 -0.025991  0.004758   \n",
       "508  0.016184 -0.031092 -0.015421  ...  0.016641 -0.011229  0.013116   \n",
       "509  0.025712 -0.047099 -0.001434  ...  0.008016  0.007969  0.015612   \n",
       "510  0.028934 -0.062641 -0.019988  ...  0.016922  0.000335  0.009324   \n",
       "511  0.011515  0.046984 -0.048719  ...  0.025211 -0.073287 -0.018666   \n",
       "\n",
       "          121       122       123       124       125       126       127  \n",
       "0   -0.046888 -0.019192  0.086546 -0.073178  0.080832 -0.009050  0.012282  \n",
       "1    0.001127 -0.035351 -0.068163 -0.060463 -0.052477 -0.009874 -0.051069  \n",
       "2   -0.000436 -0.021568 -0.059764 -0.045783 -0.041533  0.027022 -0.009572  \n",
       "3   -0.018078  0.014845 -0.034942 -0.044802 -0.037966  0.031754  0.007609  \n",
       "4   -0.006383  0.030896 -0.014286 -0.070968 -0.045872  0.014375  0.013205  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "507 -0.018824 -0.025798 -0.037680 -0.032863 -0.057193  0.012679 -0.026734  \n",
       "508 -0.031141  0.004250 -0.013063 -0.049673 -0.068013  0.028564 -0.027271  \n",
       "509 -0.017325  0.032569  0.021893 -0.061052 -0.065008  0.022377 -0.034998  \n",
       "510 -0.017013  0.009492  0.050202 -0.165319 -0.011996 -0.021707 -0.100012  \n",
       "511 -0.032916 -0.022071  0.169587 -0.020448  0.041042  0.013172  0.048359  \n",
       "\n",
       "[512 rows x 128 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0488, -0.0220, -0.5126,  ...,  0.0808, -0.0090,  0.0123],\n",
      "        [-0.0356,  0.0088, -0.0363,  ..., -0.0525, -0.0099, -0.0511],\n",
      "        [-0.0524, -0.0073, -0.0251,  ..., -0.0415,  0.0270, -0.0096],\n",
      "        ...,\n",
      "        [-0.0270,  0.0074,  0.0299,  ..., -0.0650,  0.0224, -0.0350],\n",
      "        [-0.0347,  0.0288, -0.0370,  ..., -0.0120, -0.0217, -0.1000],\n",
      "        [ 0.0834, -0.0866, -0.4170,  ...,  0.0410,  0.0132,  0.0484]])\n",
      "torch.Size([512, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/p_emb_weight_embedding.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "w_embedding = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "w_embedding = torch.reshape(w_embedding, btiny_embed_layer.position_embeddings.weight.shape)\n",
    "\n",
    "\n",
    "print(w_embedding)\n",
    "print(w_embedding.shape)\n",
    "print(w_embedding.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_embed_layer.position_embeddings.weight = torch.nn.Parameter(w_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model with loaded params:\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0488, -0.0220, -0.5126,  ...,  0.0808, -0.0090,  0.0123],\n",
      "        [-0.0356,  0.0088, -0.0363,  ..., -0.0525, -0.0099, -0.0511],\n",
      "        [-0.0524, -0.0073, -0.0251,  ..., -0.0415,  0.0270, -0.0096],\n",
      "        ...,\n",
      "        [-0.0270,  0.0074,  0.0299,  ..., -0.0650,  0.0224, -0.0350],\n",
      "        [-0.0347,  0.0288, -0.0370,  ..., -0.0120, -0.0217, -0.1000],\n",
      "        [ 0.0834, -0.0866, -0.4170,  ...,  0.0410,  0.0132,  0.0484]],\n",
      "       requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print('\\n\\nModel with loaded params:')\n",
    "for name, param in btiny_embed_layer.position_embeddings.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load weigths for Token Type Embeddings Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.012067</td>\n",
       "      <td>0.015102</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>0.030114</td>\n",
       "      <td>0.020067</td>\n",
       "      <td>-0.028070</td>\n",
       "      <td>-0.007965</td>\n",
       "      <td>-0.00288</td>\n",
       "      <td>-0.025338</td>\n",
       "      <td>-0.025838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003862</td>\n",
       "      <td>-0.002961</td>\n",
       "      <td>-0.011757</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.140390</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>-0.005593</td>\n",
       "      <td>-0.002244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.004326</td>\n",
       "      <td>-0.007665</td>\n",
       "      <td>-0.005287</td>\n",
       "      <td>0.015861</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>-0.008242</td>\n",
       "      <td>-0.006929</td>\n",
       "      <td>-0.00242</td>\n",
       "      <td>-0.016895</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011166</td>\n",
       "      <td>-0.005412</td>\n",
       "      <td>-0.002127</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>-0.162524</td>\n",
       "      <td>-0.019640</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.014885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.012067  0.015102  0.006183  0.030114  0.020067 -0.028070 -0.007965   \n",
       "1 -0.004326 -0.007665 -0.005287  0.015861  0.004550 -0.008242 -0.006929   \n",
       "\n",
       "       7         8         9    ...       118       119       120       121  \\\n",
       "0 -0.00288 -0.025338 -0.025838  ... -0.003862 -0.002961 -0.011757  0.000038   \n",
       "1 -0.00242 -0.016895 -0.000231  ... -0.011166 -0.005412 -0.002127  0.004642   \n",
       "\n",
       "        122       123       124       125       126       127  \n",
       "0 -0.001596 -0.001333 -0.140390  0.002437 -0.005593 -0.002244  \n",
       "1  0.002264  0.003555 -0.162524 -0.019640  0.001231  0.014885  \n",
       "\n",
       "[2 rows x 128 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2067e-02,  1.5102e-02,  6.1829e-03,  3.0114e-02,  2.0067e-02,\n",
      "         -2.8070e-02, -7.9652e-03, -2.8797e-03, -2.5338e-02, -2.5838e-02,\n",
      "         -1.4065e-02,  1.4301e-02, -1.3523e-02, -1.0546e-03,  4.1617e-03,\n",
      "         -7.8956e-03, -4.0018e-03, -8.2236e-03, -5.4406e-03, -1.1460e-02,\n",
      "         -5.1270e-03,  7.7333e-03,  8.0934e-03, -1.3538e-02,  4.9782e-03,\n",
      "          6.4786e-03, -1.3043e-02,  1.8160e-02, -1.7975e-02, -1.1640e-02,\n",
      "          4.5216e-03,  4.4604e-03, -1.3934e-03, -1.1518e-02, -7.6487e-03,\n",
      "         -2.4821e-02, -2.5442e-03, -7.0588e-03, -4.4629e-03,  2.5164e-03,\n",
      "          1.6534e-02, -2.9792e-03, -3.2118e-02, -8.8619e-03, -1.4890e-02,\n",
      "          6.6682e-03,  8.6343e-03, -1.4838e-02, -8.6091e-03, -4.9269e-04,\n",
      "          2.2465e-02,  2.5736e-03,  3.6779e-03, -4.6395e-03,  4.5225e-04,\n",
      "         -1.7632e-02, -4.2776e-05, -2.4077e-03,  2.0347e-03, -3.9920e-03,\n",
      "          8.7324e-03, -2.4069e-02, -1.0603e-03, -1.7653e-02, -1.0140e-02,\n",
      "         -1.9488e-03, -3.2371e-03, -1.9837e-02, -1.4148e-02, -8.9720e-03,\n",
      "         -1.2073e-02,  9.9851e-03,  9.2158e-03, -1.1178e-01, -1.2029e-02,\n",
      "         -9.7068e-03,  6.2270e-04,  1.2379e-02,  1.7082e-02,  9.2481e-02,\n",
      "         -2.0670e-03,  1.6317e-02,  5.5636e-03,  3.2535e-02, -1.3473e-02,\n",
      "         -1.0106e-02, -5.7518e-03,  3.1157e-02,  1.0916e-02, -1.5166e-02,\n",
      "         -1.1857e-02,  2.1325e-02, -1.5978e-02,  5.1089e-03,  5.5859e-04,\n",
      "         -1.2858e-02,  7.9467e-03, -2.7486e-04,  5.6039e-03,  2.0180e-02,\n",
      "         -1.2297e-03,  3.5208e-03, -3.7655e-03, -2.6131e-03,  8.0247e-03,\n",
      "         -7.4857e-03, -7.0202e-03,  1.1730e-02, -2.0803e-03, -7.5215e-03,\n",
      "          1.9061e-03,  2.3146e-02, -1.4119e-03,  1.4968e-02,  2.0359e-06,\n",
      "          8.8029e-03, -2.3216e-02, -1.0761e-02, -3.8619e-03, -2.9606e-03,\n",
      "         -1.1757e-02,  3.7989e-05, -1.5959e-03, -1.3326e-03, -1.4039e-01,\n",
      "          2.4370e-03, -5.5927e-03, -2.2443e-03],\n",
      "        [-4.3258e-03, -7.6647e-03, -5.2865e-03,  1.5861e-02,  4.5505e-03,\n",
      "         -8.2424e-03, -6.9290e-03, -2.4200e-03, -1.6895e-02, -2.3117e-04,\n",
      "          7.1532e-03,  1.2696e-02, -4.9573e-03, -3.5672e-02,  7.5814e-03,\n",
      "         -2.5778e-03,  8.7753e-03, -7.5228e-03, -4.1953e-02, -3.1304e-02,\n",
      "          1.3820e-02,  1.6332e-02,  2.1772e-02,  6.2161e-02, -6.8631e-03,\n",
      "          1.2557e-02,  1.2369e-02,  7.7688e-03,  3.9769e-04,  5.6060e-03,\n",
      "          5.9718e-04,  1.8001e-02, -2.0643e-02,  3.1277e-03,  1.3689e-02,\n",
      "          8.5553e-03, -1.0089e-02,  1.4819e-02, -1.1529e-02,  1.2011e-02,\n",
      "          1.3850e-02,  3.2507e-03,  2.1018e-01,  1.6643e-03,  1.1329e-02,\n",
      "         -8.4660e-03,  8.0801e-03,  1.2443e-02,  1.7920e-02,  5.9972e-03,\n",
      "          1.3364e-03, -7.2341e-03,  4.4539e-04,  2.0602e-02, -1.2526e-02,\n",
      "         -1.0808e-02,  5.8516e-04,  9.0737e-03,  4.8920e-03,  1.7930e-02,\n",
      "          1.8715e-02, -1.1968e-02, -9.7303e-03,  2.0027e-02, -9.7065e-03,\n",
      "         -1.6875e-02, -6.0136e-03, -6.1813e-03, -1.2970e-02, -3.2565e-03,\n",
      "          2.0113e-04,  1.2223e-03,  2.9561e-02,  6.0651e-02,  1.4662e-03,\n",
      "         -1.0672e-02, -1.9109e-02, -4.1235e-03,  1.2671e-02, -6.2536e-02,\n",
      "          1.7487e-02, -6.4521e-03,  1.6925e-02, -2.6421e-02,  1.1818e-02,\n",
      "         -1.5797e-02,  4.3715e-04, -9.9300e-03, -8.1714e-03,  4.4825e-03,\n",
      "         -9.9474e-03, -4.1118e-02, -1.3224e-02, -1.5428e-03, -8.7895e-03,\n",
      "          2.2726e-02,  1.0963e-02,  7.7999e-03,  1.8707e-02, -1.5911e-02,\n",
      "         -1.2517e-02,  5.2694e-03, -2.7770e-02, -1.5455e-02,  7.7477e-03,\n",
      "          1.9652e-03, -2.1531e-02, -2.9849e-03, -5.2545e-03, -1.2445e-02,\n",
      "         -2.0378e-02, -1.0797e-02,  1.2640e-02,  1.7123e-02,  1.0149e-02,\n",
      "         -2.2351e-02, -2.0025e-02, -1.1682e-02, -1.1166e-02, -5.4115e-03,\n",
      "         -2.1265e-03,  4.6415e-03,  2.2644e-03,  3.5554e-03, -1.6252e-01,\n",
      "         -1.9640e-02,  1.2313e-03,  1.4885e-02]])\n",
      "torch.Size([2, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/tt_emb_weight_embedding.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "w_embedding = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "w_embedding = torch.reshape(w_embedding, btiny_embed_layer.token_type_embeddings.weight.shape)\n",
    "\n",
    "\n",
    "print(w_embedding)\n",
    "print(w_embedding.shape)\n",
    "print(w_embedding.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_embed_layer.token_type_embeddings.weight = torch.nn.Parameter(w_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model with loaded params:\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-1.2067e-02,  1.5102e-02,  6.1829e-03,  3.0114e-02,  2.0067e-02,\n",
      "         -2.8070e-02, -7.9652e-03, -2.8797e-03, -2.5338e-02, -2.5838e-02,\n",
      "         -1.4065e-02,  1.4301e-02, -1.3523e-02, -1.0546e-03,  4.1617e-03,\n",
      "         -7.8956e-03, -4.0018e-03, -8.2236e-03, -5.4406e-03, -1.1460e-02,\n",
      "         -5.1270e-03,  7.7333e-03,  8.0934e-03, -1.3538e-02,  4.9782e-03,\n",
      "          6.4786e-03, -1.3043e-02,  1.8160e-02, -1.7975e-02, -1.1640e-02,\n",
      "          4.5216e-03,  4.4604e-03, -1.3934e-03, -1.1518e-02, -7.6487e-03,\n",
      "         -2.4821e-02, -2.5442e-03, -7.0588e-03, -4.4629e-03,  2.5164e-03,\n",
      "          1.6534e-02, -2.9792e-03, -3.2118e-02, -8.8619e-03, -1.4890e-02,\n",
      "          6.6682e-03,  8.6343e-03, -1.4838e-02, -8.6091e-03, -4.9269e-04,\n",
      "          2.2465e-02,  2.5736e-03,  3.6779e-03, -4.6395e-03,  4.5225e-04,\n",
      "         -1.7632e-02, -4.2776e-05, -2.4077e-03,  2.0347e-03, -3.9920e-03,\n",
      "          8.7324e-03, -2.4069e-02, -1.0603e-03, -1.7653e-02, -1.0140e-02,\n",
      "         -1.9488e-03, -3.2371e-03, -1.9837e-02, -1.4148e-02, -8.9720e-03,\n",
      "         -1.2073e-02,  9.9851e-03,  9.2158e-03, -1.1178e-01, -1.2029e-02,\n",
      "         -9.7068e-03,  6.2270e-04,  1.2379e-02,  1.7082e-02,  9.2481e-02,\n",
      "         -2.0670e-03,  1.6317e-02,  5.5636e-03,  3.2535e-02, -1.3473e-02,\n",
      "         -1.0106e-02, -5.7518e-03,  3.1157e-02,  1.0916e-02, -1.5166e-02,\n",
      "         -1.1857e-02,  2.1325e-02, -1.5978e-02,  5.1089e-03,  5.5859e-04,\n",
      "         -1.2858e-02,  7.9467e-03, -2.7486e-04,  5.6039e-03,  2.0180e-02,\n",
      "         -1.2297e-03,  3.5208e-03, -3.7655e-03, -2.6131e-03,  8.0247e-03,\n",
      "         -7.4857e-03, -7.0202e-03,  1.1730e-02, -2.0803e-03, -7.5215e-03,\n",
      "          1.9061e-03,  2.3146e-02, -1.4119e-03,  1.4968e-02,  2.0359e-06,\n",
      "          8.8029e-03, -2.3216e-02, -1.0761e-02, -3.8619e-03, -2.9606e-03,\n",
      "         -1.1757e-02,  3.7989e-05, -1.5959e-03, -1.3326e-03, -1.4039e-01,\n",
      "          2.4370e-03, -5.5927e-03, -2.2443e-03],\n",
      "        [-4.3258e-03, -7.6647e-03, -5.2865e-03,  1.5861e-02,  4.5505e-03,\n",
      "         -8.2424e-03, -6.9290e-03, -2.4200e-03, -1.6895e-02, -2.3117e-04,\n",
      "          7.1532e-03,  1.2696e-02, -4.9573e-03, -3.5672e-02,  7.5814e-03,\n",
      "         -2.5778e-03,  8.7753e-03, -7.5228e-03, -4.1953e-02, -3.1304e-02,\n",
      "          1.3820e-02,  1.6332e-02,  2.1772e-02,  6.2161e-02, -6.8631e-03,\n",
      "          1.2557e-02,  1.2369e-02,  7.7688e-03,  3.9769e-04,  5.6060e-03,\n",
      "          5.9718e-04,  1.8001e-02, -2.0643e-02,  3.1277e-03,  1.3689e-02,\n",
      "          8.5553e-03, -1.0089e-02,  1.4819e-02, -1.1529e-02,  1.2011e-02,\n",
      "          1.3850e-02,  3.2507e-03,  2.1018e-01,  1.6643e-03,  1.1329e-02,\n",
      "         -8.4660e-03,  8.0801e-03,  1.2443e-02,  1.7920e-02,  5.9972e-03,\n",
      "          1.3364e-03, -7.2341e-03,  4.4539e-04,  2.0602e-02, -1.2526e-02,\n",
      "         -1.0808e-02,  5.8516e-04,  9.0737e-03,  4.8920e-03,  1.7930e-02,\n",
      "          1.8715e-02, -1.1968e-02, -9.7303e-03,  2.0027e-02, -9.7065e-03,\n",
      "         -1.6875e-02, -6.0136e-03, -6.1813e-03, -1.2970e-02, -3.2565e-03,\n",
      "          2.0113e-04,  1.2223e-03,  2.9561e-02,  6.0651e-02,  1.4662e-03,\n",
      "         -1.0672e-02, -1.9109e-02, -4.1235e-03,  1.2671e-02, -6.2536e-02,\n",
      "          1.7487e-02, -6.4521e-03,  1.6925e-02, -2.6421e-02,  1.1818e-02,\n",
      "         -1.5797e-02,  4.3715e-04, -9.9300e-03, -8.1714e-03,  4.4825e-03,\n",
      "         -9.9474e-03, -4.1118e-02, -1.3224e-02, -1.5428e-03, -8.7895e-03,\n",
      "          2.2726e-02,  1.0963e-02,  7.7999e-03,  1.8707e-02, -1.5911e-02,\n",
      "         -1.2517e-02,  5.2694e-03, -2.7770e-02, -1.5455e-02,  7.7477e-03,\n",
      "          1.9652e-03, -2.1531e-02, -2.9849e-03, -5.2545e-03, -1.2445e-02,\n",
      "         -2.0378e-02, -1.0797e-02,  1.2640e-02,  1.7123e-02,  1.0149e-02,\n",
      "         -2.2351e-02, -2.0025e-02, -1.1682e-02, -1.1166e-02, -5.4115e-03,\n",
      "         -2.1265e-03,  4.6415e-03,  2.2644e-03,  3.5554e-03, -1.6252e-01,\n",
      "         -1.9640e-02,  1.2313e-03,  1.4885e-02]], requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print('\\n\\nModel with loaded params:')\n",
    "for name, param in btiny_embed_layer.token_type_embeddings.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load weigths and bias for Layer Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.495502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.270032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.068154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.390615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1.296766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.598228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.493568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1.247960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1.293882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    1.495502\n",
       "1    1.270032\n",
       "2    1.068154\n",
       "3    1.011700\n",
       "4    1.390615\n",
       "..        ...\n",
       "123  1.296766\n",
       "124  0.598228\n",
       "125  1.493568\n",
       "126  1.247960\n",
       "127  1.293882\n",
       "\n",
       "[128 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4955, 1.2700, 1.0682, 1.0117, 1.3906, 1.3169, 1.2661, 1.2740, 1.4124,\n",
      "        1.3872, 1.4296, 1.3988, 1.4541, 1.2889, 1.3758, 1.2285, 1.2625, 1.3703,\n",
      "        1.3042, 1.4490, 1.4388, 1.2392, 1.2543, 1.2615, 1.4229, 1.3853, 1.3823,\n",
      "        1.3094, 1.3063, 1.3316, 1.1687, 1.1703, 1.3073, 1.3924, 1.3614, 1.2987,\n",
      "        1.3036, 1.4355, 1.2533, 1.2706, 1.3984, 1.3938, 0.7760, 1.4566, 1.4024,\n",
      "        1.3288, 1.3139, 1.3905, 1.3256, 1.5053, 1.0005, 1.3667, 1.4348, 1.3132,\n",
      "        1.2438, 1.2984, 1.3491, 1.5547, 1.3657, 1.4399, 1.4244, 1.4467, 1.3415,\n",
      "        1.2015, 1.3678, 1.3083, 1.2729, 1.3270, 1.2430, 1.3246, 1.2527, 1.3225,\n",
      "        1.2567, 0.9936, 1.3574, 1.3829, 1.3595, 1.2967, 1.3198, 0.9238, 1.2595,\n",
      "        1.3154, 1.3881, 1.0823, 1.3091, 1.3757, 1.3536, 1.3012, 1.4016, 1.3060,\n",
      "        1.5202, 1.2605, 1.3982, 1.2271, 1.3651, 1.4819, 1.1432, 1.3101, 1.3829,\n",
      "        1.3925, 1.3085, 1.2813, 1.3912, 1.4506, 1.5241, 1.3768, 1.3141, 1.2698,\n",
      "        1.4681, 1.4492, 1.2800, 1.4466, 1.2595, 1.3222, 1.3033, 1.3861, 1.2535,\n",
      "        1.3357, 1.2658, 1.4395, 1.2904, 1.3749, 1.4416, 1.2968, 0.5982, 1.4936,\n",
      "        1.2480, 1.2939])\n",
      "torch.Size([128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/layNorm_emb_weight_embedding.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "w_embedding = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "w_embedding = torch.reshape(w_embedding, btiny_embed_layer.LayerNorm.weight.shape)\n",
    "\n",
    "\n",
    "print(w_embedding)\n",
    "print(w_embedding.shape)\n",
    "print(w_embedding.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_embed_layer.LayerNorm.weight = torch.nn.Parameter(w_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.154163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.135573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.105390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.381341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.149503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.501945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.055513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.294589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.391049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    0.154163\n",
       "1    0.135573\n",
       "2    0.105390\n",
       "3    0.009530\n",
       "4   -0.381341\n",
       "..        ...\n",
       "123  0.149503\n",
       "124  1.501945\n",
       "125  0.055513\n",
       "126  0.294589\n",
       "127 -0.391049\n",
       "\n",
       "[128 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5416e-01,  1.3557e-01,  1.0539e-01,  9.5299e-03, -3.8134e-01,\n",
      "        -5.0186e-02, -9.3519e-03,  2.5338e-01,  1.0242e-02, -1.1899e-01,\n",
      "         4.1022e-02, -3.4392e-02, -1.5152e-01, -4.3936e-02, -4.8780e-02,\n",
      "        -1.5365e-01,  1.1303e-01, -2.3227e-01,  1.7815e-02, -6.9504e-03,\n",
      "         1.2090e-01, -2.6881e-01, -2.2377e-01, -6.0521e-01, -1.2739e-02,\n",
      "        -1.7707e-04,  9.0784e-02,  1.7126e-02, -2.2644e-01, -3.2953e-01,\n",
      "        -3.4699e-01, -4.7326e-01, -3.2323e-02,  3.0278e-03, -1.2796e-02,\n",
      "        -1.5543e-01, -1.2330e-01,  9.4645e-02,  1.5147e-01, -2.4562e-01,\n",
      "         1.9116e-02, -6.1893e-02, -4.0900e-01,  1.0693e-01, -3.2244e-02,\n",
      "         2.2627e-01,  9.6006e-02,  2.4187e-01, -2.3709e-01,  7.6674e-02,\n",
      "         2.0678e-01,  1.2169e-01, -9.0485e-02,  3.0171e-01,  2.6103e-01,\n",
      "         6.0021e-02,  1.7403e-01, -2.7922e-01, -1.3922e-01, -1.1926e-01,\n",
      "        -7.9569e-02, -2.0225e-01, -3.7337e-01,  2.4687e-01, -1.9566e-01,\n",
      "        -8.0659e-02, -4.0013e-02, -4.2388e-02, -5.7195e-02,  1.0591e-01,\n",
      "        -8.5909e-02,  2.2472e-01, -1.4500e-01, -3.1478e-01, -7.4724e-02,\n",
      "        -2.3807e-02, -1.1233e-01,  7.1183e-02, -3.7123e-01, -8.1851e-01,\n",
      "        -2.0476e-01,  2.1246e-01, -3.7866e-02, -5.4762e-01,  2.8641e-02,\n",
      "        -2.0213e-01, -5.2868e-02,  7.2267e-02, -7.7351e-02,  1.2814e-01,\n",
      "        -1.0033e-01, -5.4847e-02, -6.6668e-02,  2.6198e-01, -1.6672e-01,\n",
      "        -7.4363e-02,  6.4384e-02,  1.4539e-02,  2.2923e-01,  2.2944e-01,\n",
      "        -4.9798e-02,  2.9075e-02, -1.2920e-01,  7.3457e-02, -2.7847e-02,\n",
      "        -1.1832e-01,  7.8635e-02,  1.3281e-01, -1.1057e-01,  3.3054e-01,\n",
      "         3.4424e-01,  7.0618e-02, -2.9181e-01, -1.7364e-01, -8.1128e-02,\n",
      "         1.3057e-01,  2.2498e-01, -2.4609e-01, -1.1241e-01, -4.3227e-02,\n",
      "         1.0333e-01, -1.1802e-01, -2.5611e-01,  1.4950e-01,  1.5019e+00,\n",
      "         5.5513e-02,  2.9459e-01, -3.9105e-01])\n",
      "torch.Size([128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/layNorm_emb_bias_embedding.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "b_embedding = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "b_embedding = torch.reshape(b_embedding, btiny_embed_layer.LayerNorm.bias.shape)\n",
    "\n",
    "\n",
    "print(b_embedding)\n",
    "print(b_embedding.shape)\n",
    "print(b_embedding.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_embed_layer.LayerNorm.bias = torch.nn.Parameter(b_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model with loaded params:\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([1.4955, 1.2700, 1.0682, 1.0117, 1.3906, 1.3169, 1.2661, 1.2740, 1.4124,\n",
      "        1.3872, 1.4296, 1.3988, 1.4541, 1.2889, 1.3758, 1.2285, 1.2625, 1.3703,\n",
      "        1.3042, 1.4490, 1.4388, 1.2392, 1.2543, 1.2615, 1.4229, 1.3853, 1.3823,\n",
      "        1.3094, 1.3063, 1.3316, 1.1687, 1.1703, 1.3073, 1.3924, 1.3614, 1.2987,\n",
      "        1.3036, 1.4355, 1.2533, 1.2706, 1.3984, 1.3938, 0.7760, 1.4566, 1.4024,\n",
      "        1.3288, 1.3139, 1.3905, 1.3256, 1.5053, 1.0005, 1.3667, 1.4348, 1.3132,\n",
      "        1.2438, 1.2984, 1.3491, 1.5547, 1.3657, 1.4399, 1.4244, 1.4467, 1.3415,\n",
      "        1.2015, 1.3678, 1.3083, 1.2729, 1.3270, 1.2430, 1.3246, 1.2527, 1.3225,\n",
      "        1.2567, 0.9936, 1.3574, 1.3829, 1.3595, 1.2967, 1.3198, 0.9238, 1.2595,\n",
      "        1.3154, 1.3881, 1.0823, 1.3091, 1.3757, 1.3536, 1.3012, 1.4016, 1.3060,\n",
      "        1.5202, 1.2605, 1.3982, 1.2271, 1.3651, 1.4819, 1.1432, 1.3101, 1.3829,\n",
      "        1.3925, 1.3085, 1.2813, 1.3912, 1.4506, 1.5241, 1.3768, 1.3141, 1.2698,\n",
      "        1.4681, 1.4492, 1.2800, 1.4466, 1.2595, 1.3222, 1.3033, 1.3861, 1.2535,\n",
      "        1.3357, 1.2658, 1.4395, 1.2904, 1.3749, 1.4416, 1.2968, 0.5982, 1.4936,\n",
      "        1.2480, 1.2939], requires_grad=True)\n",
      "torch.float32\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([ 1.5416e-01,  1.3557e-01,  1.0539e-01,  9.5299e-03, -3.8134e-01,\n",
      "        -5.0186e-02, -9.3519e-03,  2.5338e-01,  1.0242e-02, -1.1899e-01,\n",
      "         4.1022e-02, -3.4392e-02, -1.5152e-01, -4.3936e-02, -4.8780e-02,\n",
      "        -1.5365e-01,  1.1303e-01, -2.3227e-01,  1.7815e-02, -6.9504e-03,\n",
      "         1.2090e-01, -2.6881e-01, -2.2377e-01, -6.0521e-01, -1.2739e-02,\n",
      "        -1.7707e-04,  9.0784e-02,  1.7126e-02, -2.2644e-01, -3.2953e-01,\n",
      "        -3.4699e-01, -4.7326e-01, -3.2323e-02,  3.0278e-03, -1.2796e-02,\n",
      "        -1.5543e-01, -1.2330e-01,  9.4645e-02,  1.5147e-01, -2.4562e-01,\n",
      "         1.9116e-02, -6.1893e-02, -4.0900e-01,  1.0693e-01, -3.2244e-02,\n",
      "         2.2627e-01,  9.6006e-02,  2.4187e-01, -2.3709e-01,  7.6674e-02,\n",
      "         2.0678e-01,  1.2169e-01, -9.0485e-02,  3.0171e-01,  2.6103e-01,\n",
      "         6.0021e-02,  1.7403e-01, -2.7922e-01, -1.3922e-01, -1.1926e-01,\n",
      "        -7.9569e-02, -2.0225e-01, -3.7337e-01,  2.4687e-01, -1.9566e-01,\n",
      "        -8.0659e-02, -4.0013e-02, -4.2388e-02, -5.7195e-02,  1.0591e-01,\n",
      "        -8.5909e-02,  2.2472e-01, -1.4500e-01, -3.1478e-01, -7.4724e-02,\n",
      "        -2.3807e-02, -1.1233e-01,  7.1183e-02, -3.7123e-01, -8.1851e-01,\n",
      "        -2.0476e-01,  2.1246e-01, -3.7866e-02, -5.4762e-01,  2.8641e-02,\n",
      "        -2.0213e-01, -5.2868e-02,  7.2267e-02, -7.7351e-02,  1.2814e-01,\n",
      "        -1.0033e-01, -5.4847e-02, -6.6668e-02,  2.6198e-01, -1.6672e-01,\n",
      "        -7.4363e-02,  6.4384e-02,  1.4539e-02,  2.2923e-01,  2.2944e-01,\n",
      "        -4.9798e-02,  2.9075e-02, -1.2920e-01,  7.3457e-02, -2.7847e-02,\n",
      "        -1.1832e-01,  7.8635e-02,  1.3281e-01, -1.1057e-01,  3.3054e-01,\n",
      "         3.4424e-01,  7.0618e-02, -2.9181e-01, -1.7364e-01, -8.1128e-02,\n",
      "         1.3057e-01,  2.2498e-01, -2.4609e-01, -1.1241e-01, -4.3227e-02,\n",
      "         1.0333e-01, -1.1802e-01, -2.5611e-01,  1.4950e-01,  1.5019e+00,\n",
      "         5.5513e-02,  2.9459e-01, -3.9105e-01], requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print('\\n\\nModel with loaded params:')\n",
    "for name, param in btiny_embed_layer.LayerNorm.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(btiny_embed_layer.state_dict(), './matrices_BERTtiny/btiny_embed_layer.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load complete custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state dictionary\n",
    "btiny_embed_layer.load_state_dict(torch.load('./matrices_BERTtiny/btiny_embed_layer.pth.tar', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 128)\n",
       "  (token_type_embeddings): Embedding(2, 128)\n",
       "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send to device\n",
    "btiny_embed_layer.to(device)\n",
    "\n",
    "# Set for evaluation\n",
    "btiny_embed_layer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 For Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings:\n",
      "tensor([[[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "         [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "         [-1.1811,  1.2459, -0.4839,  ..., -0.8412, -0.2327, -2.4342],\n",
      "         ...,\n",
      "         [-0.2535,  1.0778, -0.9683,  ...,  0.7092,  1.5920,  0.6388],\n",
      "         [-1.0587,  1.3010, -0.9315,  ...,  1.6865,  0.3028, -1.4071],\n",
      "         [-1.9167,  1.9236, -0.3274,  ...,  2.0763, -1.1798,  0.0233]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 69, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Input embeddings\n",
    "with torch.no_grad():\n",
    "    input_embeddings = btiny_embed_layer(input_features)\n",
    "\n",
    "print('Input Embeddings:')\n",
    "print(input_embeddings)\n",
    "print(input_embeddings.shape)\n",
    "print(input_embeddings.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 For Whole Dataset \n",
    "- Calculating in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 384, 128])\n"
     ]
    }
   ],
   "source": [
    "# Create the input embeddings tensor --> send to device\n",
    "input_embeddings_val = torch.zeros((2, batch_size, max_length, 128), device=device)\n",
    "\n",
    "print(input_embeddings_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings:\n",
      "tensor([[[[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-1.6365,  1.2126,  0.0407,  ...,  0.3889, -1.2550, -2.2735],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-1.6365,  1.2126,  0.0407,  ...,  0.3889, -1.2550, -2.2735],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-3.3947, -0.2091, -0.1579,  ..., -2.0215,  0.9156,  0.3244],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-2.1338, -0.7160, -0.4718,  ...,  0.0206, -0.8902,  0.1790],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-1.8695, -0.4537,  0.4853,  ...,  0.1759, -0.7297, -1.4886],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [ 0.0124,  0.1580,  0.0912,  ...,  0.1633, -1.2665, -1.2405],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-2.3451, -0.1492, -0.7094,  ..., -0.3083, -0.8099, -2.2957],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-0.7542, -1.7269, -1.2176,  ..., -0.6284,  2.6734, -2.6264],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-1.9888, -1.1329, -0.6459,  ..., -0.5466, -0.2746,  0.7136],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-1.0900, -0.7656, -0.9249,  ..., -0.1853, -1.7568,  0.0382],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [-1.6365,  1.2126,  0.0407,  ...,  0.3889, -1.2550, -2.2735],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]],\n",
      "\n",
      "         [[ 1.0186, -0.1719, -8.8026,  ...,  1.9003, -0.0685,  0.0248],\n",
      "          [-0.7977, -0.1629, -0.5375,  ..., -0.7302, -0.5534, -0.3855],\n",
      "          [ 0.0124,  0.1580,  0.0912,  ...,  0.1633, -1.2665, -1.2405],\n",
      "          ...,\n",
      "          [-1.0836, -0.1730,  0.3029,  ...,  1.4856, -0.4105, -0.3495],\n",
      "          [-1.0918, -0.3092,  0.3065,  ...,  1.4007, -0.1376, -0.1779],\n",
      "          [-0.9575, -0.3282,  0.0681,  ...,  1.3346,  0.1812,  0.1554]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 12, 384, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Input embeddings\n",
    "# Retrive the number of batches\n",
    "n_batches,_,_,_ = input_embeddings_val.shape\n",
    "\n",
    "# Calculate per batch\n",
    "for n_batch in tqdm(range(n_batches)):\n",
    "    with torch.no_grad():\n",
    "        batch_result = btiny_embed_layer(input_features_val[n_batch,:,:])\n",
    "        # Store the batch result\n",
    "        input_embeddings_val[n_batch,:,:,:] = batch_result\n",
    "\n",
    "print('Input Embeddings:')\n",
    "print(input_embeddings_val)\n",
    "print(input_embeddings_val.shape)\n",
    "print(input_embeddings_val.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tensor\n",
    "# torch.save(input_embeddings_val, './data_BERTtiny/input_embeddings_val.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FP8\n",
    "- Quantize the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a deep copy of the model since the function overwrite it\n",
    "btiny_embed_layer_to_e4m3 = copy.deepcopy(btiny_embed_layer)\n",
    "\n",
    "# layers exempt from conversion\n",
    "list_exempt_layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4m3 : quantizing model weights..\n"
     ]
    }
   ],
   "source": [
    "# It needs the outputs even though it overwrites in model\n",
    "btiny_embed_layer_e4m3, emulator = mpt_emu.quantize_model (model=btiny_embed_layer_to_e4m3, dtype=\"E4M3\",\n",
    "                               list_exempt_layers=list_exempt_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 For Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings FP8:\n",
      "tensor([[[ 1.0302, -0.1911, -8.7791,  ...,  1.9042, -0.0760,  0.0222],\n",
      "         [-0.8158, -0.1714, -0.5421,  ..., -0.7046, -0.5320, -0.3462],\n",
      "         [-1.2478,  1.2520, -0.4812,  ..., -0.8651, -0.2358, -2.4025],\n",
      "         ...,\n",
      "         [-0.2757,  1.0786, -0.9869,  ...,  0.7160,  1.5738,  0.6594],\n",
      "         [-1.0367,  1.3301, -0.9537,  ...,  1.7106,  0.2697, -1.4511],\n",
      "         [-1.9391,  1.8882, -0.3166,  ...,  2.1327, -1.1713,  0.0866]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 69, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Input embeddings\n",
    "with torch.no_grad():\n",
    "    input_embeddings_fp8 = btiny_embed_layer_e4m3(input_features)\n",
    "\n",
    "print('Input Embeddings FP8:')\n",
    "print(input_embeddings_fp8)\n",
    "print(input_embeddings_fp8.shape)\n",
    "print(input_embeddings_fp8.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 For Whole Dataset \n",
    "- In batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 384, 128])\n"
     ]
    }
   ],
   "source": [
    "# Create the input embeddings tensor --> send to device\n",
    "input_embeddings_fp8_val = torch.zeros((2, batch_size, max_length, 128), device=device)\n",
    "\n",
    "print(input_embeddings_fp8_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1123.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings FP8:\n",
      "tensor([[[[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-1.7113e+00,  1.2186e+00,  5.8527e-02,  ...,  3.7936e-01,\n",
      "           -1.2159e+00, -2.2504e+00],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-1.7113e+00,  1.2186e+00,  5.8527e-02,  ...,  3.7936e-01,\n",
      "           -1.2159e+00, -2.2504e+00],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-3.4009e+00, -2.2683e-01, -1.3770e-01,  ..., -2.0738e+00,\n",
      "            9.1261e-01,  3.1355e-01],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-2.1899e+00, -7.7086e-01, -4.5881e-01,  ..., -5.5476e-03,\n",
      "           -8.5555e-01,  1.9684e-01],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-1.9523e+00, -4.8601e-01,  5.1329e-01,  ...,  1.6571e-01,\n",
      "           -8.0478e-01, -1.4719e+00],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-4.7375e-03,  1.4497e-01,  1.0157e-01,  ...,  1.2331e-01,\n",
      "           -1.1692e+00, -1.2699e+00],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-2.3758e+00, -1.5679e-01, -6.8630e-01,  ..., -3.1604e-01,\n",
      "           -8.3576e-01, -2.3236e+00],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-8.2129e-01, -1.7362e+00, -1.2432e+00,  ..., -6.5196e-01,\n",
      "            2.6648e+00, -2.7120e+00],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-2.0552e+00, -1.1568e+00, -6.2966e-01,  ..., -5.6130e-01,\n",
      "           -2.3229e-01,  7.5213e-01],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-1.1496e+00, -7.7005e-01, -9.4884e-01,  ..., -1.8487e-01,\n",
      "           -1.6673e+00,  4.0018e-02],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-1.7113e+00,  1.2186e+00,  5.8527e-02,  ...,  3.7936e-01,\n",
      "           -1.2159e+00, -2.2504e+00],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]],\n",
      "\n",
      "         [[ 1.0302e+00, -1.9106e-01, -8.7791e+00,  ...,  1.9042e+00,\n",
      "           -7.6042e-02,  2.2224e-02],\n",
      "          [-8.1579e-01, -1.7136e-01, -5.4215e-01,  ..., -7.0459e-01,\n",
      "           -5.3199e-01, -3.4625e-01],\n",
      "          [-4.7375e-03,  1.4497e-01,  1.0157e-01,  ...,  1.2331e-01,\n",
      "           -1.1692e+00, -1.2699e+00],\n",
      "          ...,\n",
      "          [-1.0879e+00, -1.7226e-01,  3.1504e-01,  ...,  1.5427e+00,\n",
      "           -4.2157e-01, -3.5724e-01],\n",
      "          [-1.1262e+00, -3.0828e-01,  3.1341e-01,  ...,  1.4186e+00,\n",
      "           -1.3980e-01, -1.7991e-01],\n",
      "          [-9.4594e-01, -3.3295e-01,  7.0552e-02,  ...,  1.3433e+00,\n",
      "            1.8467e-01,  1.6206e-01]]]], device='cuda:0')\n",
      "torch.Size([2, 12, 384, 128])\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Input embeddings\n",
    "# Retrive the number of batches\n",
    "n_batches,_,_,_ = input_embeddings_fp8_val.shape\n",
    "\n",
    "# Calculate per batch\n",
    "for n_batch in tqdm(range(n_batches)):\n",
    "    with torch.no_grad():\n",
    "        batch_result = btiny_embed_layer_e4m3(input_features_val[n_batch,:,:])\n",
    "        # Store the batch result\n",
    "        input_embeddings_fp8_val[n_batch,:,:,:] = batch_result\n",
    "\n",
    "print('Input Embeddings FP8:')\n",
    "print(input_embeddings_fp8_val)\n",
    "print(input_embeddings_fp8_val.shape)\n",
    "print(input_embeddings_fp8_val.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "- Load the correspoding weights from BERT-tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertSelfAttention(\n",
      "  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate BERT-tiny Custom Self-Attention Layer\n",
    "btiny_selfAtt_layer = BertSelfAttention()\n",
    "print(btiny_selfAtt_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load weights and bias for Query Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.059850</td>\n",
       "      <td>-0.002382</td>\n",
       "      <td>-0.058855</td>\n",
       "      <td>-0.028033</td>\n",
       "      <td>0.025485</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>0.050951</td>\n",
       "      <td>-0.067064</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>-0.143789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045056</td>\n",
       "      <td>0.087741</td>\n",
       "      <td>-0.036367</td>\n",
       "      <td>0.014253</td>\n",
       "      <td>0.097803</td>\n",
       "      <td>0.107307</td>\n",
       "      <td>-0.040013</td>\n",
       "      <td>0.099739</td>\n",
       "      <td>0.227729</td>\n",
       "      <td>-0.049307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016393</td>\n",
       "      <td>-0.139717</td>\n",
       "      <td>-0.030735</td>\n",
       "      <td>-0.008755</td>\n",
       "      <td>-0.191558</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.074370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069572</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.027159</td>\n",
       "      <td>-0.002229</td>\n",
       "      <td>0.075805</td>\n",
       "      <td>-0.071877</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.020096</td>\n",
       "      <td>-0.017461</td>\n",
       "      <td>0.069899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.012062</td>\n",
       "      <td>-0.156383</td>\n",
       "      <td>0.018681</td>\n",
       "      <td>-0.038379</td>\n",
       "      <td>0.088270</td>\n",
       "      <td>0.058305</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>-0.015546</td>\n",
       "      <td>-0.046616</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129797</td>\n",
       "      <td>0.019507</td>\n",
       "      <td>-0.030276</td>\n",
       "      <td>-0.057480</td>\n",
       "      <td>-0.018257</td>\n",
       "      <td>-0.042185</td>\n",
       "      <td>0.080272</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.135588</td>\n",
       "      <td>0.103174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008841</td>\n",
       "      <td>0.053494</td>\n",
       "      <td>0.097573</td>\n",
       "      <td>0.012218</td>\n",
       "      <td>-0.100524</td>\n",
       "      <td>-0.070975</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.083738</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>-0.172871</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067264</td>\n",
       "      <td>-0.023137</td>\n",
       "      <td>0.111493</td>\n",
       "      <td>-0.032516</td>\n",
       "      <td>-0.050158</td>\n",
       "      <td>-0.054550</td>\n",
       "      <td>0.136196</td>\n",
       "      <td>-0.048526</td>\n",
       "      <td>-0.018391</td>\n",
       "      <td>-0.141181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042326</td>\n",
       "      <td>-0.157827</td>\n",
       "      <td>-0.008236</td>\n",
       "      <td>0.036493</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>-0.028591</td>\n",
       "      <td>0.031151</td>\n",
       "      <td>-0.047141</td>\n",
       "      <td>-0.062380</td>\n",
       "      <td>-0.021174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>-0.025892</td>\n",
       "      <td>-0.120777</td>\n",
       "      <td>0.031728</td>\n",
       "      <td>-0.051697</td>\n",
       "      <td>-0.025857</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.013971</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>-0.078950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.006747</td>\n",
       "      <td>-0.052400</td>\n",
       "      <td>-0.055498</td>\n",
       "      <td>-0.016402</td>\n",
       "      <td>0.010718</td>\n",
       "      <td>0.039295</td>\n",
       "      <td>0.064454</td>\n",
       "      <td>0.030999</td>\n",
       "      <td>-0.047453</td>\n",
       "      <td>-0.082101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028956</td>\n",
       "      <td>-0.040643</td>\n",
       "      <td>0.054958</td>\n",
       "      <td>-0.082357</td>\n",
       "      <td>0.068480</td>\n",
       "      <td>-0.128085</td>\n",
       "      <td>-0.070459</td>\n",
       "      <td>-0.068956</td>\n",
       "      <td>-0.145107</td>\n",
       "      <td>-0.092218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.022002</td>\n",
       "      <td>0.123808</td>\n",
       "      <td>0.019228</td>\n",
       "      <td>-0.033157</td>\n",
       "      <td>-0.013851</td>\n",
       "      <td>0.020222</td>\n",
       "      <td>0.035963</td>\n",
       "      <td>-0.027232</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>0.083506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>0.027509</td>\n",
       "      <td>-0.033035</td>\n",
       "      <td>0.031825</td>\n",
       "      <td>-0.026508</td>\n",
       "      <td>-0.010491</td>\n",
       "      <td>0.146510</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>-0.042626</td>\n",
       "      <td>0.034996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.021686</td>\n",
       "      <td>0.038990</td>\n",
       "      <td>-0.071052</td>\n",
       "      <td>-0.005680</td>\n",
       "      <td>-0.056194</td>\n",
       "      <td>-0.100890</td>\n",
       "      <td>-0.006852</td>\n",
       "      <td>0.205811</td>\n",
       "      <td>-0.011122</td>\n",
       "      <td>0.063049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>0.054893</td>\n",
       "      <td>0.011687</td>\n",
       "      <td>-0.102100</td>\n",
       "      <td>-0.074546</td>\n",
       "      <td>-0.003310</td>\n",
       "      <td>0.162069</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.045523</td>\n",
       "      <td>-0.026417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.046687</td>\n",
       "      <td>0.062667</td>\n",
       "      <td>0.012622</td>\n",
       "      <td>-0.021284</td>\n",
       "      <td>-0.002872</td>\n",
       "      <td>-0.117501</td>\n",
       "      <td>-0.100806</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>-0.028846</td>\n",
       "      <td>0.022312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029881</td>\n",
       "      <td>0.100802</td>\n",
       "      <td>0.013481</td>\n",
       "      <td>0.067665</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>-0.079752</td>\n",
       "      <td>-0.061770</td>\n",
       "      <td>-0.012529</td>\n",
       "      <td>-0.049936</td>\n",
       "      <td>-0.033418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.026605</td>\n",
       "      <td>-0.012460</td>\n",
       "      <td>-0.046207</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>-0.019432</td>\n",
       "      <td>-0.135951</td>\n",
       "      <td>-0.049917</td>\n",
       "      <td>-0.075615</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030497</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>0.072659</td>\n",
       "      <td>-0.021863</td>\n",
       "      <td>-0.047362</td>\n",
       "      <td>0.047346</td>\n",
       "      <td>0.097976</td>\n",
       "      <td>-0.066853</td>\n",
       "      <td>-0.060815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.059850 -0.002382 -0.058855 -0.028033  0.025485  0.024641  0.050951   \n",
       "1    0.016393 -0.139717 -0.030735 -0.008755 -0.191558  0.002068  0.001081   \n",
       "2   -0.012062 -0.156383  0.018681 -0.038379  0.088270  0.058305  0.219300   \n",
       "3   -0.008841  0.053494  0.097573  0.012218 -0.100524 -0.070975  0.212308   \n",
       "4    0.042326 -0.157827 -0.008236  0.036493  0.000843 -0.028591  0.031151   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123 -0.006747 -0.052400 -0.055498 -0.016402  0.010718  0.039295  0.064454   \n",
       "124  0.022002  0.123808  0.019228 -0.033157 -0.013851  0.020222  0.035963   \n",
       "125  0.021686  0.038990 -0.071052 -0.005680 -0.056194 -0.100890 -0.006852   \n",
       "126  0.046687  0.062667  0.012622 -0.021284 -0.002872 -0.117501 -0.100806   \n",
       "127 -0.026605 -0.012460 -0.046207 -0.002020  0.006321 -0.019432 -0.135951   \n",
       "\n",
       "          7         8         9    ...       118       119       120  \\\n",
       "0   -0.067064 -0.023389 -0.143789  ... -0.045056  0.087741 -0.036367   \n",
       "1    0.002305  0.018322 -0.074370  ... -0.069572  0.021015  0.027159   \n",
       "2   -0.015546 -0.046616  0.013172  ... -0.129797  0.019507 -0.030276   \n",
       "3    0.083738  0.000399 -0.172871  ... -0.067264 -0.023137  0.111493   \n",
       "4   -0.047141 -0.062380 -0.021174  ...  0.018888 -0.025892 -0.120777   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123  0.030999 -0.047453 -0.082101  ... -0.028956 -0.040643  0.054958   \n",
       "124 -0.027232 -0.020983  0.083506  ...  0.009394  0.027509 -0.033035   \n",
       "125  0.205811 -0.011122  0.063049  ...  0.033746  0.054893  0.011687   \n",
       "126  0.060844 -0.028846  0.022312  ... -0.029881  0.100802  0.013481   \n",
       "127 -0.049917 -0.075615  0.012760  ...  0.030497  0.009650  0.019907   \n",
       "\n",
       "          121       122       123       124       125       126       127  \n",
       "0    0.014253  0.097803  0.107307 -0.040013  0.099739  0.227729 -0.049307  \n",
       "1   -0.002229  0.075805 -0.071877  0.041778  0.020096 -0.017461  0.069899  \n",
       "2   -0.057480 -0.018257 -0.042185  0.080272  0.006257  0.135588  0.103174  \n",
       "3   -0.032516 -0.050158 -0.054550  0.136196 -0.048526 -0.018391 -0.141181  \n",
       "4    0.031728 -0.051697 -0.025857 -0.024750 -0.013971  0.012514 -0.078950  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123 -0.082357  0.068480 -0.128085 -0.070459 -0.068956 -0.145107 -0.092218  \n",
       "124  0.031825 -0.026508 -0.010491  0.146510  0.008921 -0.042626  0.034996  \n",
       "125 -0.102100 -0.074546 -0.003310  0.162069  0.018206  0.045523 -0.026417  \n",
       "126  0.067665  0.005370 -0.079752 -0.061770 -0.012529 -0.049936 -0.033418  \n",
       "127  0.072659 -0.021863 -0.047362  0.047346  0.097976 -0.066853 -0.060815  \n",
       "\n",
       "[128 rows x 128 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0599, -0.0024, -0.0589,  ...,  0.0997,  0.2277, -0.0493],\n",
      "        [ 0.0164, -0.1397, -0.0307,  ...,  0.0201, -0.0175,  0.0699],\n",
      "        [-0.0121, -0.1564,  0.0187,  ...,  0.0063,  0.1356,  0.1032],\n",
      "        ...,\n",
      "        [ 0.0217,  0.0390, -0.0711,  ...,  0.0182,  0.0455, -0.0264],\n",
      "        [ 0.0467,  0.0627,  0.0126,  ..., -0.0125, -0.0499, -0.0334],\n",
      "        [-0.0266, -0.0125, -0.0462,  ...,  0.0980, -0.0669, -0.0608]])\n",
      "torch.Size([128, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/q_weight_self1.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "w_self = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "w_self = torch.reshape(w_self, btiny_selfAtt_layer.query.weight.shape)\n",
    "\n",
    "print(w_self)\n",
    "print(w_self.shape)\n",
    "print(w_self.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_selfAtt_layer.query.weight = torch.nn.Parameter(w_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.139727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.203548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.343521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.074429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.260298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.108596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.137817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.007954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.154450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.079663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0   -0.139727\n",
       "1   -0.203548\n",
       "2   -0.343521\n",
       "3   -0.074429\n",
       "4    0.260298\n",
       "..        ...\n",
       "123 -0.108596\n",
       "124  0.137817\n",
       "125  0.007954\n",
       "126  0.154450\n",
       "127 -0.079663\n",
       "\n",
       "[128 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1397, -0.2035, -0.3435, -0.0744,  0.2603, -0.4633, -0.0254,  0.0746,\n",
      "         0.2399, -0.3195, -0.3761,  0.2109, -0.1115,  0.0887,  0.3852, -0.1834,\n",
      "         0.1592,  0.1892,  0.2069,  0.2323, -0.0203, -0.2748, -0.1218,  0.2787,\n",
      "        -0.3331, -0.0408,  0.3027,  0.0198, -0.2524,  0.2570, -0.3003, -0.2451,\n",
      "         0.2838,  0.0479,  0.1737,  0.3017,  0.1656,  0.0981, -0.0689,  0.3381,\n",
      "         0.0758, -0.4643, -0.1374, -0.0978, -0.1089,  0.2018,  0.1460,  0.2351,\n",
      "         0.2637, -0.3619,  0.2314,  0.1232,  0.3663,  0.3396, -0.2992,  0.3749,\n",
      "         0.5266, -0.3460,  0.0785, -0.0493,  0.4350,  0.1211,  0.1884, -0.2552,\n",
      "         0.2550, -0.2189, -0.4310, -0.3573,  0.1600, -0.3283,  0.0393,  0.1780,\n",
      "         0.2687,  0.4193, -0.1971,  0.3165,  0.2394,  0.0499, -0.0059,  0.1423,\n",
      "        -0.0938, -0.2324,  0.1209,  0.0801, -0.0200, -0.4802, -0.1674, -0.1667,\n",
      "         0.3161,  0.1314,  0.2360, -0.1580,  0.1068,  0.0494, -0.0242,  0.0663,\n",
      "         0.1756,  0.1533, -0.0868,  0.0559,  0.0367, -0.0548, -0.0436, -0.0326,\n",
      "         0.3060, -0.0926,  0.0668,  0.1326, -0.3225,  0.1518, -0.0586, -0.0374,\n",
      "        -0.3905, -0.2412, -0.1005,  0.2770,  0.1427, -0.1625,  0.2848,  0.1923,\n",
      "        -0.2528,  0.1927, -0.3396, -0.1086,  0.1378,  0.0080,  0.1544, -0.0797])\n",
      "torch.Size([128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/q_bias_self1.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "b_self = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "b_self = torch.reshape(b_self, btiny_selfAtt_layer.query.bias.shape)\n",
    "\n",
    "print(b_self)\n",
    "print(b_self.shape)\n",
    "print(b_self.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_selfAtt_layer.query.bias = torch.nn.Parameter(b_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model with loaded params:\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0599, -0.0024, -0.0589,  ...,  0.0997,  0.2277, -0.0493],\n",
      "        [ 0.0164, -0.1397, -0.0307,  ...,  0.0201, -0.0175,  0.0699],\n",
      "        [-0.0121, -0.1564,  0.0187,  ...,  0.0063,  0.1356,  0.1032],\n",
      "        ...,\n",
      "        [ 0.0217,  0.0390, -0.0711,  ...,  0.0182,  0.0455, -0.0264],\n",
      "        [ 0.0467,  0.0627,  0.0126,  ..., -0.0125, -0.0499, -0.0334],\n",
      "        [-0.0266, -0.0125, -0.0462,  ...,  0.0980, -0.0669, -0.0608]],\n",
      "       requires_grad=True)\n",
      "torch.float32\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([-0.1397, -0.2035, -0.3435, -0.0744,  0.2603, -0.4633, -0.0254,  0.0746,\n",
      "         0.2399, -0.3195, -0.3761,  0.2109, -0.1115,  0.0887,  0.3852, -0.1834,\n",
      "         0.1592,  0.1892,  0.2069,  0.2323, -0.0203, -0.2748, -0.1218,  0.2787,\n",
      "        -0.3331, -0.0408,  0.3027,  0.0198, -0.2524,  0.2570, -0.3003, -0.2451,\n",
      "         0.2838,  0.0479,  0.1737,  0.3017,  0.1656,  0.0981, -0.0689,  0.3381,\n",
      "         0.0758, -0.4643, -0.1374, -0.0978, -0.1089,  0.2018,  0.1460,  0.2351,\n",
      "         0.2637, -0.3619,  0.2314,  0.1232,  0.3663,  0.3396, -0.2992,  0.3749,\n",
      "         0.5266, -0.3460,  0.0785, -0.0493,  0.4350,  0.1211,  0.1884, -0.2552,\n",
      "         0.2550, -0.2189, -0.4310, -0.3573,  0.1600, -0.3283,  0.0393,  0.1780,\n",
      "         0.2687,  0.4193, -0.1971,  0.3165,  0.2394,  0.0499, -0.0059,  0.1423,\n",
      "        -0.0938, -0.2324,  0.1209,  0.0801, -0.0200, -0.4802, -0.1674, -0.1667,\n",
      "         0.3161,  0.1314,  0.2360, -0.1580,  0.1068,  0.0494, -0.0242,  0.0663,\n",
      "         0.1756,  0.1533, -0.0868,  0.0559,  0.0367, -0.0548, -0.0436, -0.0326,\n",
      "         0.3060, -0.0926,  0.0668,  0.1326, -0.3225,  0.1518, -0.0586, -0.0374,\n",
      "        -0.3905, -0.2412, -0.1005,  0.2770,  0.1427, -0.1625,  0.2848,  0.1923,\n",
      "        -0.2528,  0.1927, -0.3396, -0.1086,  0.1378,  0.0080,  0.1544, -0.0797],\n",
      "       requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print('\\n\\nModel with loaded params:')\n",
    "for name, param in btiny_selfAtt_layer.query.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load weights and bias for Key Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.028923</td>\n",
       "      <td>-0.022910</td>\n",
       "      <td>0.019855</td>\n",
       "      <td>0.061649</td>\n",
       "      <td>-0.030300</td>\n",
       "      <td>-0.023491</td>\n",
       "      <td>0.155590</td>\n",
       "      <td>-0.081524</td>\n",
       "      <td>-0.038452</td>\n",
       "      <td>-0.028798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>-0.010060</td>\n",
       "      <td>-0.014924</td>\n",
       "      <td>0.068190</td>\n",
       "      <td>0.074615</td>\n",
       "      <td>0.013543</td>\n",
       "      <td>-0.012701</td>\n",
       "      <td>0.063711</td>\n",
       "      <td>0.234652</td>\n",
       "      <td>-0.065752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016033</td>\n",
       "      <td>-0.063874</td>\n",
       "      <td>-0.045525</td>\n",
       "      <td>0.029138</td>\n",
       "      <td>-0.248677</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.014891</td>\n",
       "      <td>-0.018952</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>-0.111282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096025</td>\n",
       "      <td>-0.058503</td>\n",
       "      <td>0.081914</td>\n",
       "      <td>0.044237</td>\n",
       "      <td>0.134019</td>\n",
       "      <td>-0.035351</td>\n",
       "      <td>-0.066283</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>-0.047907</td>\n",
       "      <td>0.163651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034177</td>\n",
       "      <td>-0.008687</td>\n",
       "      <td>0.085097</td>\n",
       "      <td>0.031717</td>\n",
       "      <td>0.051408</td>\n",
       "      <td>-0.001439</td>\n",
       "      <td>0.133645</td>\n",
       "      <td>0.045052</td>\n",
       "      <td>-0.041483</td>\n",
       "      <td>-0.010662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028966</td>\n",
       "      <td>0.073205</td>\n",
       "      <td>0.029545</td>\n",
       "      <td>-0.052548</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>-0.054304</td>\n",
       "      <td>-0.015603</td>\n",
       "      <td>0.035626</td>\n",
       "      <td>0.142066</td>\n",
       "      <td>0.034442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018289</td>\n",
       "      <td>0.065273</td>\n",
       "      <td>0.187880</td>\n",
       "      <td>0.037729</td>\n",
       "      <td>-0.040836</td>\n",
       "      <td>-0.072323</td>\n",
       "      <td>0.144621</td>\n",
       "      <td>0.084039</td>\n",
       "      <td>0.072073</td>\n",
       "      <td>-0.083951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041801</td>\n",
       "      <td>-0.065517</td>\n",
       "      <td>0.086128</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.016166</td>\n",
       "      <td>-0.062358</td>\n",
       "      <td>-0.016398</td>\n",
       "      <td>-0.027847</td>\n",
       "      <td>-0.026752</td>\n",
       "      <td>-0.115558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.008247</td>\n",
       "      <td>-0.057170</td>\n",
       "      <td>-0.024159</td>\n",
       "      <td>-0.036373</td>\n",
       "      <td>-0.085199</td>\n",
       "      <td>0.027643</td>\n",
       "      <td>-0.077205</td>\n",
       "      <td>0.020895</td>\n",
       "      <td>-0.042216</td>\n",
       "      <td>-0.091954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026343</td>\n",
       "      <td>0.060157</td>\n",
       "      <td>0.011653</td>\n",
       "      <td>-0.059423</td>\n",
       "      <td>-0.074045</td>\n",
       "      <td>-0.043273</td>\n",
       "      <td>-0.034429</td>\n",
       "      <td>-0.097787</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>-0.131836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.004995</td>\n",
       "      <td>0.051045</td>\n",
       "      <td>0.090116</td>\n",
       "      <td>0.095965</td>\n",
       "      <td>-0.030803</td>\n",
       "      <td>-0.007690</td>\n",
       "      <td>-0.010135</td>\n",
       "      <td>0.095205</td>\n",
       "      <td>-0.021623</td>\n",
       "      <td>-0.063913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066185</td>\n",
       "      <td>-0.006521</td>\n",
       "      <td>-0.043474</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.036408</td>\n",
       "      <td>-0.054320</td>\n",
       "      <td>-0.008982</td>\n",
       "      <td>0.060027</td>\n",
       "      <td>-0.060985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.088872</td>\n",
       "      <td>-0.057913</td>\n",
       "      <td>0.034528</td>\n",
       "      <td>0.085744</td>\n",
       "      <td>0.036090</td>\n",
       "      <td>0.059982</td>\n",
       "      <td>-0.047164</td>\n",
       "      <td>-0.100374</td>\n",
       "      <td>0.081263</td>\n",
       "      <td>-0.008118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060871</td>\n",
       "      <td>-0.012466</td>\n",
       "      <td>0.043281</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>-0.055186</td>\n",
       "      <td>-0.043838</td>\n",
       "      <td>0.029569</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>-0.092288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.095217</td>\n",
       "      <td>0.085662</td>\n",
       "      <td>0.046468</td>\n",
       "      <td>0.031011</td>\n",
       "      <td>-0.021136</td>\n",
       "      <td>-0.010746</td>\n",
       "      <td>-0.059707</td>\n",
       "      <td>-0.004263</td>\n",
       "      <td>0.025461</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078954</td>\n",
       "      <td>-0.090493</td>\n",
       "      <td>0.095738</td>\n",
       "      <td>-0.011044</td>\n",
       "      <td>-0.050119</td>\n",
       "      <td>-0.007285</td>\n",
       "      <td>0.197745</td>\n",
       "      <td>0.010259</td>\n",
       "      <td>0.094274</td>\n",
       "      <td>-0.012370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.011031</td>\n",
       "      <td>-0.122510</td>\n",
       "      <td>-0.014345</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>-0.043593</td>\n",
       "      <td>-0.071884</td>\n",
       "      <td>0.018996</td>\n",
       "      <td>0.153044</td>\n",
       "      <td>-0.053010</td>\n",
       "      <td>-0.053264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036109</td>\n",
       "      <td>0.032191</td>\n",
       "      <td>-0.139293</td>\n",
       "      <td>-0.088316</td>\n",
       "      <td>-0.006239</td>\n",
       "      <td>-0.034776</td>\n",
       "      <td>0.124423</td>\n",
       "      <td>-0.026440</td>\n",
       "      <td>-0.074861</td>\n",
       "      <td>0.006064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.065204</td>\n",
       "      <td>0.042807</td>\n",
       "      <td>-0.080179</td>\n",
       "      <td>0.045396</td>\n",
       "      <td>0.059212</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.058447</td>\n",
       "      <td>0.023076</td>\n",
       "      <td>0.054260</td>\n",
       "      <td>-0.116741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016858</td>\n",
       "      <td>0.066252</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>-0.021849</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>-0.004669</td>\n",
       "      <td>-0.003594</td>\n",
       "      <td>0.085378</td>\n",
       "      <td>-0.074540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.028923 -0.022910  0.019855  0.061649 -0.030300 -0.023491  0.155590   \n",
       "1    0.016033 -0.063874 -0.045525  0.029138 -0.248677  0.002340  0.014891   \n",
       "2    0.034177 -0.008687  0.085097  0.031717  0.051408 -0.001439  0.133645   \n",
       "3    0.018289  0.065273  0.187880  0.037729 -0.040836 -0.072323  0.144621   \n",
       "4   -0.008247 -0.057170 -0.024159 -0.036373 -0.085199  0.027643 -0.077205   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123 -0.004995  0.051045  0.090116  0.095965 -0.030803 -0.007690 -0.010135   \n",
       "124  0.088872 -0.057913  0.034528  0.085744  0.036090  0.059982 -0.047164   \n",
       "125 -0.095217  0.085662  0.046468  0.031011 -0.021136 -0.010746 -0.059707   \n",
       "126  0.011031 -0.122510 -0.014345  0.005901 -0.043593 -0.071884  0.018996   \n",
       "127  0.065204  0.042807 -0.080179  0.045396  0.059212  0.003843  0.058447   \n",
       "\n",
       "          7         8         9    ...       118       119       120  \\\n",
       "0   -0.081524 -0.038452 -0.028798  ...  0.029137 -0.010060 -0.014924   \n",
       "1   -0.018952  0.051949 -0.111282  ...  0.096025 -0.058503  0.081914   \n",
       "2    0.045052 -0.041483 -0.010662  ... -0.028966  0.073205  0.029545   \n",
       "3    0.084039  0.072073 -0.083951  ...  0.041801 -0.065517  0.086128   \n",
       "4    0.020895 -0.042216 -0.091954  ...  0.026343  0.060157  0.011653   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123  0.095205 -0.021623 -0.063913  ... -0.066185 -0.006521 -0.043474   \n",
       "124 -0.100374  0.081263 -0.008118  ...  0.060871 -0.012466  0.043281   \n",
       "125 -0.004263  0.025461  0.067400  ... -0.078954 -0.090493  0.095738   \n",
       "126  0.153044 -0.053010 -0.053264  ...  0.036109  0.032191 -0.139293   \n",
       "127  0.023076  0.054260 -0.116741  ...  0.016858  0.066252  0.004625   \n",
       "\n",
       "          121       122       123       124       125       126       127  \n",
       "0    0.068190  0.074615  0.013543 -0.012701  0.063711  0.234652 -0.065752  \n",
       "1    0.044237  0.134019 -0.035351 -0.066283  0.011586 -0.047907  0.163651  \n",
       "2   -0.052548 -0.024332 -0.054304 -0.015603  0.035626  0.142066  0.034442  \n",
       "3    0.024631  0.016166 -0.062358 -0.016398 -0.027847 -0.026752 -0.115558  \n",
       "4   -0.059423 -0.074045 -0.043273 -0.034429 -0.097787  0.008500 -0.131836  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123  0.069231  0.028587  0.036408 -0.054320 -0.008982  0.060027 -0.060985  \n",
       "124  0.055587 -0.055186 -0.043838  0.029569  0.022688  0.073600 -0.092288  \n",
       "125 -0.011044 -0.050119 -0.007285  0.197745  0.010259  0.094274 -0.012370  \n",
       "126 -0.088316 -0.006239 -0.034776  0.124423 -0.026440 -0.074861  0.006064  \n",
       "127 -0.021849  0.001282  0.010706 -0.004669 -0.003594  0.085378 -0.074540  \n",
       "\n",
       "[128 rows x 128 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0289, -0.0229,  0.0199,  ...,  0.0637,  0.2347, -0.0658],\n",
      "        [ 0.0160, -0.0639, -0.0455,  ...,  0.0116, -0.0479,  0.1637],\n",
      "        [ 0.0342, -0.0087,  0.0851,  ...,  0.0356,  0.1421,  0.0344],\n",
      "        ...,\n",
      "        [-0.0952,  0.0857,  0.0465,  ...,  0.0103,  0.0943, -0.0124],\n",
      "        [ 0.0110, -0.1225, -0.0143,  ..., -0.0264, -0.0749,  0.0061],\n",
      "        [ 0.0652,  0.0428, -0.0802,  ..., -0.0036,  0.0854, -0.0745]])\n",
      "torch.Size([128, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/k_weight_self1.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "w_self = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "w_self = torch.reshape(w_self, btiny_selfAtt_layer.key.weight.shape)\n",
    "\n",
    "print(w_self)\n",
    "print(w_self.shape)\n",
    "print(w_self.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_selfAtt_layer.key.weight = torch.nn.Parameter(w_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.056535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.144404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.108984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.013193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-0.014836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.012522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.013275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.036360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    0.056535\n",
       "1   -0.039677\n",
       "2    0.034396\n",
       "3   -0.144404\n",
       "4   -0.108984\n",
       "..        ...\n",
       "123  0.013193\n",
       "124 -0.014836\n",
       "125  0.012522\n",
       "126  0.013275\n",
       "127 -0.036360\n",
       "\n",
       "[128 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0565, -0.0397,  0.0344, -0.1444, -0.1090, -0.1422, -0.1594, -0.1018,\n",
      "         0.0322, -0.1215,  0.0009, -0.1181, -0.1439,  0.0713,  0.1301, -0.2012,\n",
      "        -0.0068,  0.0891,  0.0554,  0.0871,  0.1110,  0.1353, -0.0295, -0.1314,\n",
      "        -0.1295, -0.1594, -0.1054, -0.0714, -0.1611,  0.1739, -0.0492, -0.1030,\n",
      "         0.0152,  0.0187,  0.1300,  0.1783, -0.0078, -0.0234,  0.0670,  0.0527,\n",
      "         0.1570,  0.0102,  0.1094,  0.0891, -0.1295, -0.1236,  0.1457,  0.1545,\n",
      "         0.2428,  0.0899,  0.0880, -0.0313,  0.1183,  0.1540, -0.1052, -0.0777,\n",
      "         0.2205,  0.0803, -0.0059, -0.1684, -0.0835,  0.0564, -0.1008, -0.1376,\n",
      "        -0.0441, -0.0014, -0.0514,  0.0085, -0.0402,  0.0511,  0.0325,  0.0624,\n",
      "         0.0595,  0.0576, -0.0529,  0.0043, -0.0511, -0.0241, -0.0838, -0.0301,\n",
      "         0.0331,  0.0747, -0.0683,  0.0049, -0.0303,  0.0924, -0.0780,  0.0406,\n",
      "        -0.0613, -0.0394,  0.0306, -0.0217, -0.0235, -0.0219,  0.0439, -0.0682,\n",
      "        -0.0155,  0.0986,  0.0374, -0.0059, -0.0188, -0.0205, -0.0044, -0.0954,\n",
      "         0.0091, -0.0560, -0.0142,  0.0325,  0.0588, -0.0841, -0.0183,  0.0317,\n",
      "         0.0940,  0.0235,  0.0569, -0.0712, -0.0561,  0.0857,  0.0099,  0.0604,\n",
      "         0.0228, -0.0267, -0.0536,  0.0132, -0.0148,  0.0125,  0.0133, -0.0364])\n",
      "torch.Size([128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/k_bias_self1.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "b_self = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "b_self = torch.reshape(b_self, btiny_selfAtt_layer.key.bias.shape)\n",
    "\n",
    "print(b_self)\n",
    "print(b_self.shape)\n",
    "print(b_self.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_selfAtt_layer.key.bias = torch.nn.Parameter(b_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model with loaded params:\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0289, -0.0229,  0.0199,  ...,  0.0637,  0.2347, -0.0658],\n",
      "        [ 0.0160, -0.0639, -0.0455,  ...,  0.0116, -0.0479,  0.1637],\n",
      "        [ 0.0342, -0.0087,  0.0851,  ...,  0.0356,  0.1421,  0.0344],\n",
      "        ...,\n",
      "        [-0.0952,  0.0857,  0.0465,  ...,  0.0103,  0.0943, -0.0124],\n",
      "        [ 0.0110, -0.1225, -0.0143,  ..., -0.0264, -0.0749,  0.0061],\n",
      "        [ 0.0652,  0.0428, -0.0802,  ..., -0.0036,  0.0854, -0.0745]],\n",
      "       requires_grad=True)\n",
      "torch.float32\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0565, -0.0397,  0.0344, -0.1444, -0.1090, -0.1422, -0.1594, -0.1018,\n",
      "         0.0322, -0.1215,  0.0009, -0.1181, -0.1439,  0.0713,  0.1301, -0.2012,\n",
      "        -0.0068,  0.0891,  0.0554,  0.0871,  0.1110,  0.1353, -0.0295, -0.1314,\n",
      "        -0.1295, -0.1594, -0.1054, -0.0714, -0.1611,  0.1739, -0.0492, -0.1030,\n",
      "         0.0152,  0.0187,  0.1300,  0.1783, -0.0078, -0.0234,  0.0670,  0.0527,\n",
      "         0.1570,  0.0102,  0.1094,  0.0891, -0.1295, -0.1236,  0.1457,  0.1545,\n",
      "         0.2428,  0.0899,  0.0880, -0.0313,  0.1183,  0.1540, -0.1052, -0.0777,\n",
      "         0.2205,  0.0803, -0.0059, -0.1684, -0.0835,  0.0564, -0.1008, -0.1376,\n",
      "        -0.0441, -0.0014, -0.0514,  0.0085, -0.0402,  0.0511,  0.0325,  0.0624,\n",
      "         0.0595,  0.0576, -0.0529,  0.0043, -0.0511, -0.0241, -0.0838, -0.0301,\n",
      "         0.0331,  0.0747, -0.0683,  0.0049, -0.0303,  0.0924, -0.0780,  0.0406,\n",
      "        -0.0613, -0.0394,  0.0306, -0.0217, -0.0235, -0.0219,  0.0439, -0.0682,\n",
      "        -0.0155,  0.0986,  0.0374, -0.0059, -0.0188, -0.0205, -0.0044, -0.0954,\n",
      "         0.0091, -0.0560, -0.0142,  0.0325,  0.0588, -0.0841, -0.0183,  0.0317,\n",
      "         0.0940,  0.0235,  0.0569, -0.0712, -0.0561,  0.0857,  0.0099,  0.0604,\n",
      "         0.0228, -0.0267, -0.0536,  0.0132, -0.0148,  0.0125,  0.0133, -0.0364],\n",
      "       requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print('\\n\\nModel with loaded params:')\n",
    "for name, param in btiny_selfAtt_layer.key.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load weights and bias for Value Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.106876</td>\n",
       "      <td>0.167160</td>\n",
       "      <td>0.011117</td>\n",
       "      <td>-0.094118</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.034839</td>\n",
       "      <td>0.100296</td>\n",
       "      <td>0.088016</td>\n",
       "      <td>-0.078236</td>\n",
       "      <td>-0.099943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>0.037848</td>\n",
       "      <td>0.060927</td>\n",
       "      <td>0.044930</td>\n",
       "      <td>0.044509</td>\n",
       "      <td>0.051836</td>\n",
       "      <td>0.094023</td>\n",
       "      <td>-0.076478</td>\n",
       "      <td>0.052999</td>\n",
       "      <td>0.023643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.087787</td>\n",
       "      <td>-0.006205</td>\n",
       "      <td>-0.035735</td>\n",
       "      <td>-0.282746</td>\n",
       "      <td>0.042032</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>-0.030654</td>\n",
       "      <td>0.149854</td>\n",
       "      <td>0.054134</td>\n",
       "      <td>-0.067509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091293</td>\n",
       "      <td>-0.002949</td>\n",
       "      <td>0.040140</td>\n",
       "      <td>-0.014677</td>\n",
       "      <td>-0.023300</td>\n",
       "      <td>-0.017035</td>\n",
       "      <td>0.044011</td>\n",
       "      <td>0.017806</td>\n",
       "      <td>-0.053490</td>\n",
       "      <td>0.055077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018031</td>\n",
       "      <td>-0.077530</td>\n",
       "      <td>-0.034129</td>\n",
       "      <td>0.295427</td>\n",
       "      <td>-0.042062</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.062368</td>\n",
       "      <td>-0.042541</td>\n",
       "      <td>-0.074538</td>\n",
       "      <td>0.083199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.112093</td>\n",
       "      <td>0.117624</td>\n",
       "      <td>-0.019353</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>-0.013782</td>\n",
       "      <td>0.015623</td>\n",
       "      <td>-0.007639</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>-0.016494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.027514</td>\n",
       "      <td>-0.065447</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>-0.252131</td>\n",
       "      <td>-0.044792</td>\n",
       "      <td>-0.096290</td>\n",
       "      <td>0.081861</td>\n",
       "      <td>-0.093329</td>\n",
       "      <td>-0.077518</td>\n",
       "      <td>-0.014695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054074</td>\n",
       "      <td>0.104787</td>\n",
       "      <td>-0.011900</td>\n",
       "      <td>-0.025024</td>\n",
       "      <td>0.035859</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.074681</td>\n",
       "      <td>0.047022</td>\n",
       "      <td>-0.042583</td>\n",
       "      <td>-0.007402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010950</td>\n",
       "      <td>-0.044463</td>\n",
       "      <td>0.035547</td>\n",
       "      <td>0.226172</td>\n",
       "      <td>0.088863</td>\n",
       "      <td>-0.042415</td>\n",
       "      <td>0.029592</td>\n",
       "      <td>0.082085</td>\n",
       "      <td>-0.043734</td>\n",
       "      <td>-0.157100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113585</td>\n",
       "      <td>0.028077</td>\n",
       "      <td>-0.107315</td>\n",
       "      <td>0.003485</td>\n",
       "      <td>-0.099308</td>\n",
       "      <td>0.025237</td>\n",
       "      <td>-0.074640</td>\n",
       "      <td>0.163126</td>\n",
       "      <td>0.064791</td>\n",
       "      <td>-0.036467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.121727</td>\n",
       "      <td>-0.417435</td>\n",
       "      <td>0.092819</td>\n",
       "      <td>-0.013010</td>\n",
       "      <td>-0.217426</td>\n",
       "      <td>-0.058663</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>-0.295898</td>\n",
       "      <td>-0.068979</td>\n",
       "      <td>0.254772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073335</td>\n",
       "      <td>-0.112706</td>\n",
       "      <td>0.102958</td>\n",
       "      <td>-0.033488</td>\n",
       "      <td>-0.019486</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>0.019705</td>\n",
       "      <td>0.099488</td>\n",
       "      <td>0.326057</td>\n",
       "      <td>0.145479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.030605</td>\n",
       "      <td>0.120635</td>\n",
       "      <td>-0.104403</td>\n",
       "      <td>-0.090044</td>\n",
       "      <td>-0.029900</td>\n",
       "      <td>0.288220</td>\n",
       "      <td>-0.066572</td>\n",
       "      <td>0.056395</td>\n",
       "      <td>-0.099913</td>\n",
       "      <td>0.095281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038464</td>\n",
       "      <td>-0.036287</td>\n",
       "      <td>-0.176466</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>-0.153842</td>\n",
       "      <td>-0.123798</td>\n",
       "      <td>-0.024082</td>\n",
       "      <td>-0.007033</td>\n",
       "      <td>-0.250871</td>\n",
       "      <td>0.014712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.279420</td>\n",
       "      <td>0.074759</td>\n",
       "      <td>-0.103996</td>\n",
       "      <td>0.102092</td>\n",
       "      <td>0.218086</td>\n",
       "      <td>-0.180469</td>\n",
       "      <td>0.094613</td>\n",
       "      <td>-0.083963</td>\n",
       "      <td>-0.027921</td>\n",
       "      <td>0.100040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098270</td>\n",
       "      <td>0.094245</td>\n",
       "      <td>-0.196052</td>\n",
       "      <td>-0.092947</td>\n",
       "      <td>0.111068</td>\n",
       "      <td>-0.057940</td>\n",
       "      <td>-0.013444</td>\n",
       "      <td>0.009387</td>\n",
       "      <td>0.166398</td>\n",
       "      <td>-0.193272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.050757</td>\n",
       "      <td>0.094182</td>\n",
       "      <td>-0.177869</td>\n",
       "      <td>-0.105083</td>\n",
       "      <td>-0.032143</td>\n",
       "      <td>0.108951</td>\n",
       "      <td>0.143007</td>\n",
       "      <td>0.142604</td>\n",
       "      <td>-0.100144</td>\n",
       "      <td>-0.018659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071427</td>\n",
       "      <td>0.095235</td>\n",
       "      <td>-0.015093</td>\n",
       "      <td>0.150106</td>\n",
       "      <td>-0.041296</td>\n",
       "      <td>0.032780</td>\n",
       "      <td>0.094840</td>\n",
       "      <td>-0.159194</td>\n",
       "      <td>-0.069015</td>\n",
       "      <td>-0.296175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.051537</td>\n",
       "      <td>0.090134</td>\n",
       "      <td>0.214132</td>\n",
       "      <td>-0.000704</td>\n",
       "      <td>-0.065716</td>\n",
       "      <td>-0.040546</td>\n",
       "      <td>-0.027144</td>\n",
       "      <td>-0.054041</td>\n",
       "      <td>-0.159333</td>\n",
       "      <td>-0.038265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210445</td>\n",
       "      <td>0.100301</td>\n",
       "      <td>-0.101399</td>\n",
       "      <td>-0.050621</td>\n",
       "      <td>0.148052</td>\n",
       "      <td>-0.046193</td>\n",
       "      <td>-0.100129</td>\n",
       "      <td>0.060880</td>\n",
       "      <td>-0.009439</td>\n",
       "      <td>0.004981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.106876  0.167160  0.011117 -0.094118  0.004696  0.034839  0.100296   \n",
       "1   -0.087787 -0.006205 -0.035735 -0.282746  0.042032  0.012195 -0.030654   \n",
       "2    0.018031 -0.077530 -0.034129  0.295427 -0.042062  0.013040  0.062368   \n",
       "3   -0.027514 -0.065447  0.008426 -0.252131 -0.044792 -0.096290  0.081861   \n",
       "4    0.010950 -0.044463  0.035547  0.226172  0.088863 -0.042415  0.029592   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "123 -0.121727 -0.417435  0.092819 -0.013010 -0.217426 -0.058663  0.000343   \n",
       "124  0.030605  0.120635 -0.104403 -0.090044 -0.029900  0.288220 -0.066572   \n",
       "125 -0.279420  0.074759 -0.103996  0.102092  0.218086 -0.180469  0.094613   \n",
       "126 -0.050757  0.094182 -0.177869 -0.105083 -0.032143  0.108951  0.143007   \n",
       "127 -0.051537  0.090134  0.214132 -0.000704 -0.065716 -0.040546 -0.027144   \n",
       "\n",
       "          7         8         9    ...       118       119       120  \\\n",
       "0    0.088016 -0.078236 -0.099943  ...  0.012179  0.037848  0.060927   \n",
       "1    0.149854  0.054134 -0.067509  ...  0.091293 -0.002949  0.040140   \n",
       "2   -0.042541 -0.074538  0.083199  ...  0.001902  0.112093  0.117624   \n",
       "3   -0.093329 -0.077518 -0.014695  ... -0.054074  0.104787 -0.011900   \n",
       "4    0.082085 -0.043734 -0.157100  ...  0.113585  0.028077 -0.107315   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "123 -0.295898 -0.068979  0.254772  ...  0.073335 -0.112706  0.102958   \n",
       "124  0.056395 -0.099913  0.095281  ... -0.038464 -0.036287 -0.176466   \n",
       "125 -0.083963 -0.027921  0.100040  ...  0.098270  0.094245 -0.196052   \n",
       "126  0.142604 -0.100144 -0.018659  ...  0.071427  0.095235 -0.015093   \n",
       "127 -0.054041 -0.159333 -0.038265  ...  0.210445  0.100301 -0.101399   \n",
       "\n",
       "          121       122       123       124       125       126       127  \n",
       "0    0.044930  0.044509  0.051836  0.094023 -0.076478  0.052999  0.023643  \n",
       "1   -0.014677 -0.023300 -0.017035  0.044011  0.017806 -0.053490  0.055077  \n",
       "2   -0.019353  0.003240 -0.013782  0.015623 -0.007639  0.007798 -0.016494  \n",
       "3   -0.025024  0.035859  0.013334  0.074681  0.047022 -0.042583 -0.007402  \n",
       "4    0.003485 -0.099308  0.025237 -0.074640  0.163126  0.064791 -0.036467  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123 -0.033488 -0.019486  0.018957  0.019705  0.099488  0.326057  0.145479  \n",
       "124  0.060374 -0.153842 -0.123798 -0.024082 -0.007033 -0.250871  0.014712  \n",
       "125 -0.092947  0.111068 -0.057940 -0.013444  0.009387  0.166398 -0.193272  \n",
       "126  0.150106 -0.041296  0.032780  0.094840 -0.159194 -0.069015 -0.296175  \n",
       "127 -0.050621  0.148052 -0.046193 -0.100129  0.060880 -0.009439  0.004981  \n",
       "\n",
       "[128 rows x 128 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1069,  0.1672,  0.0111,  ..., -0.0765,  0.0530,  0.0236],\n",
      "        [-0.0878, -0.0062, -0.0357,  ...,  0.0178, -0.0535,  0.0551],\n",
      "        [ 0.0180, -0.0775, -0.0341,  ..., -0.0076,  0.0078, -0.0165],\n",
      "        ...,\n",
      "        [-0.2794,  0.0748, -0.1040,  ...,  0.0094,  0.1664, -0.1933],\n",
      "        [-0.0508,  0.0942, -0.1779,  ..., -0.1592, -0.0690, -0.2962],\n",
      "        [-0.0515,  0.0901,  0.2141,  ...,  0.0609, -0.0094,  0.0050]])\n",
      "torch.Size([128, 128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/v_weight_self1.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "w_self = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "w_self = torch.reshape(w_self, btiny_selfAtt_layer.value.weight.shape)\n",
    "\n",
    "print(w_self)\n",
    "print(w_self.shape)\n",
    "print(w_self.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_selfAtt_layer.value.weight = torch.nn.Parameter(w_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.131291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.277889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.009893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.027086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.037662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.047556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.006050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0   -0.131291\n",
       "1   -0.038155\n",
       "2    0.200495\n",
       "3   -0.277889\n",
       "4   -0.080100\n",
       "..        ...\n",
       "123 -0.009893\n",
       "124  0.027086\n",
       "125 -0.037662\n",
       "126  0.047556\n",
       "127 -0.006050\n",
       "\n",
       "[128 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3129e-01, -3.8155e-02,  2.0049e-01, -2.7789e-01, -8.0100e-02,\n",
      "         1.3546e-01,  1.6920e-01, -1.5986e-02, -1.9376e-01,  7.5609e-02,\n",
      "        -6.7156e-02, -1.1830e-01, -1.3170e-01, -1.8172e-02, -1.1383e-01,\n",
      "        -1.1989e-01, -1.5669e-01,  1.5041e-01,  9.0636e-02,  9.5478e-02,\n",
      "         5.6257e-02,  1.1457e-01, -7.0490e-02, -1.3984e-01,  1.3972e-01,\n",
      "         7.1995e-02,  2.0667e-01,  1.3300e-01,  4.7761e-02,  3.8488e-05,\n",
      "         8.1915e-02, -1.0721e-01,  4.5316e-02,  1.0272e-01,  1.6097e-01,\n",
      "         2.3207e-01, -1.0941e-01,  8.3587e-03, -8.7826e-02,  1.1794e-01,\n",
      "         1.2216e-01,  5.9916e-02, -1.0576e-02,  2.0412e-01,  2.0118e-01,\n",
      "         6.1681e-02,  1.1864e-02,  5.4412e-02,  1.2941e-01,  2.4121e-01,\n",
      "        -3.9597e-02, -3.5136e-02, -1.5990e-01, -1.0046e-01, -1.0241e-01,\n",
      "         1.9222e-01, -1.4741e-01, -2.1419e-01,  2.8232e-01, -9.1196e-02,\n",
      "         1.0281e-01, -1.2893e-01, -6.5793e-02, -2.0425e-02,  1.1402e-01,\n",
      "        -1.2789e-02, -5.0013e-02,  1.0573e-01,  1.4910e-02, -1.5684e-02,\n",
      "        -1.5871e-04, -3.6594e-02,  5.2068e-02, -2.3415e-02, -7.9368e-02,\n",
      "         1.9137e-02,  4.3727e-02, -5.4341e-02,  5.7060e-03, -4.6867e-02,\n",
      "         1.6135e-02,  4.5614e-02,  6.5619e-02,  6.9520e-02, -4.3307e-02,\n",
      "        -9.1324e-02, -9.1938e-03, -2.7244e-02, -1.7697e-02, -3.6074e-02,\n",
      "         2.8303e-02, -2.3052e-02,  8.1007e-03,  2.9014e-02,  2.2076e-02,\n",
      "         7.4364e-02,  5.8521e-02, -8.8277e-02, -5.4354e-02,  1.0532e-01,\n",
      "        -2.5256e-02,  3.6854e-02, -1.1342e-01,  3.4375e-02,  1.8174e-02,\n",
      "        -2.0460e-02, -1.2734e-01,  3.0222e-02, -3.5585e-03,  4.0083e-02,\n",
      "        -1.2573e-02,  3.4623e-02, -5.6822e-02, -9.3663e-03, -3.4685e-02,\n",
      "        -1.9058e-02, -2.7071e-02, -1.4583e-02,  1.5562e-02,  6.0128e-02,\n",
      "         2.9898e-02,  1.3010e-03, -9.3977e-02, -9.8929e-03,  2.7086e-02,\n",
      "        -3.7662e-02,  4.7556e-02, -6.0501e-03])\n",
      "torch.Size([128])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Read from stored file\n",
    "df = pd.read_csv('./matrices_BERTtiny/v_bias_self1.csv', header=None)\n",
    "display(df)\n",
    "\n",
    "# Dataframe to Tensor\n",
    "b_self = torch.Tensor(df.values)\n",
    "\n",
    "# Make sure shape correspond to the target network\n",
    "b_self = torch.reshape(b_self, btiny_selfAtt_layer.value.bias.shape)\n",
    "\n",
    "print(b_self)\n",
    "print(b_self.shape)\n",
    "print(b_self.dtype)\n",
    "\n",
    "# Load the custom weights to the model\n",
    "btiny_selfAtt_layer.value.bias = torch.nn.Parameter(b_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model with loaded params:\n",
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1069,  0.1672,  0.0111,  ..., -0.0765,  0.0530,  0.0236],\n",
      "        [-0.0878, -0.0062, -0.0357,  ...,  0.0178, -0.0535,  0.0551],\n",
      "        [ 0.0180, -0.0775, -0.0341,  ..., -0.0076,  0.0078, -0.0165],\n",
      "        ...,\n",
      "        [-0.2794,  0.0748, -0.1040,  ...,  0.0094,  0.1664, -0.1933],\n",
      "        [-0.0508,  0.0942, -0.1779,  ..., -0.1592, -0.0690, -0.2962],\n",
      "        [-0.0515,  0.0901,  0.2141,  ...,  0.0609, -0.0094,  0.0050]],\n",
      "       requires_grad=True)\n",
      "torch.float32\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([-1.3129e-01, -3.8155e-02,  2.0049e-01, -2.7789e-01, -8.0100e-02,\n",
      "         1.3546e-01,  1.6920e-01, -1.5986e-02, -1.9376e-01,  7.5609e-02,\n",
      "        -6.7156e-02, -1.1830e-01, -1.3170e-01, -1.8172e-02, -1.1383e-01,\n",
      "        -1.1989e-01, -1.5669e-01,  1.5041e-01,  9.0636e-02,  9.5478e-02,\n",
      "         5.6257e-02,  1.1457e-01, -7.0490e-02, -1.3984e-01,  1.3972e-01,\n",
      "         7.1995e-02,  2.0667e-01,  1.3300e-01,  4.7761e-02,  3.8488e-05,\n",
      "         8.1915e-02, -1.0721e-01,  4.5316e-02,  1.0272e-01,  1.6097e-01,\n",
      "         2.3207e-01, -1.0941e-01,  8.3587e-03, -8.7826e-02,  1.1794e-01,\n",
      "         1.2216e-01,  5.9916e-02, -1.0576e-02,  2.0412e-01,  2.0118e-01,\n",
      "         6.1681e-02,  1.1864e-02,  5.4412e-02,  1.2941e-01,  2.4121e-01,\n",
      "        -3.9597e-02, -3.5136e-02, -1.5990e-01, -1.0046e-01, -1.0241e-01,\n",
      "         1.9222e-01, -1.4741e-01, -2.1419e-01,  2.8232e-01, -9.1196e-02,\n",
      "         1.0281e-01, -1.2893e-01, -6.5793e-02, -2.0425e-02,  1.1402e-01,\n",
      "        -1.2789e-02, -5.0013e-02,  1.0573e-01,  1.4910e-02, -1.5684e-02,\n",
      "        -1.5871e-04, -3.6594e-02,  5.2068e-02, -2.3415e-02, -7.9368e-02,\n",
      "         1.9137e-02,  4.3727e-02, -5.4341e-02,  5.7060e-03, -4.6867e-02,\n",
      "         1.6135e-02,  4.5614e-02,  6.5619e-02,  6.9520e-02, -4.3307e-02,\n",
      "        -9.1324e-02, -9.1938e-03, -2.7244e-02, -1.7697e-02, -3.6074e-02,\n",
      "         2.8303e-02, -2.3052e-02,  8.1007e-03,  2.9014e-02,  2.2076e-02,\n",
      "         7.4364e-02,  5.8521e-02, -8.8277e-02, -5.4354e-02,  1.0532e-01,\n",
      "        -2.5256e-02,  3.6854e-02, -1.1342e-01,  3.4375e-02,  1.8174e-02,\n",
      "        -2.0460e-02, -1.2734e-01,  3.0222e-02, -3.5585e-03,  4.0083e-02,\n",
      "        -1.2573e-02,  3.4623e-02, -5.6822e-02, -9.3663e-03, -3.4685e-02,\n",
      "        -1.9058e-02, -2.7071e-02, -1.4583e-02,  1.5562e-02,  6.0128e-02,\n",
      "         2.9898e-02,  1.3010e-03, -9.3977e-02, -9.8929e-03,  2.7086e-02,\n",
      "        -3.7662e-02,  4.7556e-02, -6.0501e-03], requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print('\\n\\nModel with loaded params:')\n",
    "for name, param in btiny_selfAtt_layer.value.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(btiny_selfAtt_layer.state_dict(), './matrices_BERTtiny/btiny_selfAtt_layer.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load complete custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btiny_selfAtt_layer.load_state_dict(torch.load('./matrices_BERTtiny/btiny_selfAtt_layer.pth.tar', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfAttention(\n",
       "  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send to device\n",
    "btiny_selfAtt_layer.to(device)\n",
    "\n",
    "# Set for evaluation\n",
    "btiny_selfAtt_layer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 For Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-vectors:\n",
      "tensor([[[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "          [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "          [ 1.1415, -0.7692, -1.6731,  ..., -1.2060,  0.1050, -0.5982],\n",
      "          ...,\n",
      "          [ 1.6528,  1.4196, -0.4565,  ..., -0.8998, -0.5105,  0.0711],\n",
      "          [ 0.8446, -0.4697, -1.1443,  ..., -0.9019, -0.3423, -1.0246],\n",
      "          [-0.2262,  0.9788, -0.8177,  ..., -0.7025, -0.0773, -0.8013]],\n",
      "\n",
      "         [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "          [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "          [ 1.0977,  1.2537, -2.2652,  ...,  0.0124,  0.3396,  0.8193],\n",
      "          ...,\n",
      "          [ 0.5781, -0.3813, -0.9593,  ...,  1.1433, -0.3759, -0.1945],\n",
      "          [ 0.7400, -0.1878,  0.3100,  ...,  2.5679,  0.0257,  2.2768],\n",
      "          [ 0.6970, -0.2057, -0.2687,  ...,  0.9680,  0.7832, -0.0088]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 64])\n",
      "\n",
      "K-vectors:\n",
      "tensor([[[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "          [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "          [-0.0174,  0.0612, -1.0946,  ..., -0.0707, -0.5789, -0.5633],\n",
      "          ...,\n",
      "          [ 0.3754,  2.1611,  1.1865,  ...,  1.1165, -1.4479, -0.8808],\n",
      "          [-0.6110,  0.3426,  0.1143,  ...,  2.4959,  0.5036, -0.9281],\n",
      "          [-0.9909, -0.0440, -0.5650,  ...,  3.0424,  2.0335, -2.4549]],\n",
      "\n",
      "         [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "          [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "          [ 1.5818,  0.1141,  2.0690,  ..., -0.3302, -0.2687,  2.0083],\n",
      "          ...,\n",
      "          [-0.2343, -0.8475,  1.6516,  ...,  1.1937,  0.1643,  0.0051],\n",
      "          [ 0.1095, -0.1613,  0.9700,  ...,  0.5866,  0.4533,  0.0083],\n",
      "          [ 1.4966, -1.1909, -1.0012,  ...,  1.1941,  0.2790, -1.3290]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 64])\n",
      "\n",
      "V-vectors:\n",
      "tensor([[[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "          [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "          [-0.2347, -1.2577, -2.0994,  ...,  1.4935, -0.3344,  1.6407],\n",
      "          ...,\n",
      "          [-0.7690, -0.3374,  0.2055,  ...,  0.1322, -0.7597, -1.0511],\n",
      "          [-1.4196, -2.0997, -0.7222,  ..., -0.2620,  0.4465,  0.2340],\n",
      "          [-0.2906, -0.0138,  0.6570,  ...,  1.5371,  0.4854,  1.0567]],\n",
      "\n",
      "         [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "          [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "          [-1.8126,  0.7491,  0.4319,  ..., -0.7725, -2.1470, -0.3267],\n",
      "          ...,\n",
      "          [ 1.1326,  0.0614, -2.1672,  ..., -0.1296, -0.1237,  2.0590],\n",
      "          [ 1.2693,  2.2152,  1.4147,  ...,  4.6311,  2.8578,  5.0843],\n",
      "          [-0.9334,  1.0030, -0.0791,  ..., -0.1345, -1.6535,  2.5188]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V vectors and attention scores\n",
    "with torch.no_grad():\n",
    "    q_vectors, k_vectors, v_vectors, raw_scores, attention_scores, attention_probs = btiny_selfAtt_layer(input_embeddings)\n",
    "\n",
    "# Vectors\n",
    "print(f'Q-vectors:\\n{q_vectors}\\n{q_vectors.shape}\\n')\n",
    "print(f'K-vectors:\\n{k_vectors}\\n{k_vectors.shape}\\n')\n",
    "print(f'V-vectors:\\n{v_vectors}\\n{v_vectors.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Scores:\n",
      "tensor([[[[  5.0973,   1.7900,  -1.0383,  ...,  -0.7527,  -7.0527,  -5.9016],\n",
      "          [ 20.2713,   5.9234,  -7.2253,  ..., -13.6022,   9.6095,   3.9752],\n",
      "          [ 37.7019,   0.5809,  57.2145,  ..., -11.3580,   4.0511,   8.4960],\n",
      "          ...,\n",
      "          [ 10.2706,  -4.6507,  -6.4014,  ...,  37.6829,  -5.3668,   4.0109],\n",
      "          [ 37.1332,   4.0314,  13.5363,  ..., -19.1000, -12.1125,  -6.2376],\n",
      "          [ 39.3518,   8.2255,   9.5095,  ...,  -1.9401,  -7.1946,   7.3159]],\n",
      "\n",
      "         [[ 13.8412,  14.2090,  11.1390,  ...,  13.6115,   9.7128,  12.5552],\n",
      "          [  6.3428,  38.3307,  24.4544,  ...,   3.6919,  12.7806,   5.7462],\n",
      "          [ 12.9040,  27.3446,   1.7931,  ...,   2.0018,  -3.8257,   3.3463],\n",
      "          ...,\n",
      "          [ 12.3539,   7.5483,  13.8231,  ...,  22.9644,  29.5363,  33.8834],\n",
      "          [ 21.4056,  30.0952,  13.4548,  ...,  30.8341,  30.4364,  21.4093],\n",
      "          [ 17.5579,  11.2205,   0.3141,  ...,  19.4909,  17.2422,  17.7453]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 69])\n",
      "\n",
      "Attention Scores:\n",
      "tensor([[[[ 0.6372,  0.2237, -0.1298,  ..., -0.0941, -0.8816, -0.7377],\n",
      "          [ 2.5339,  0.7404, -0.9032,  ..., -1.7003,  1.2012,  0.4969],\n",
      "          [ 4.7127,  0.0726,  7.1518,  ..., -1.4197,  0.5064,  1.0620],\n",
      "          ...,\n",
      "          [ 1.2838, -0.5813, -0.8002,  ...,  4.7104, -0.6709,  0.5014],\n",
      "          [ 4.6416,  0.5039,  1.6920,  ..., -2.3875, -1.5141, -0.7797],\n",
      "          [ 4.9190,  1.0282,  1.1887,  ..., -0.2425, -0.8993,  0.9145]],\n",
      "\n",
      "         [[ 1.7301,  1.7761,  1.3924,  ...,  1.7014,  1.2141,  1.5694],\n",
      "          [ 0.7929,  4.7913,  3.0568,  ...,  0.4615,  1.5976,  0.7183],\n",
      "          [ 1.6130,  3.4181,  0.2241,  ...,  0.2502, -0.4782,  0.4183],\n",
      "          ...,\n",
      "          [ 1.5442,  0.9435,  1.7279,  ...,  2.8705,  3.6920,  4.2354],\n",
      "          [ 2.6757,  3.7619,  1.6819,  ...,  3.8543,  3.8045,  2.6762],\n",
      "          [ 2.1947,  1.4026,  0.0393,  ...,  2.4364,  2.1553,  2.2182]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 69])\n",
      "\n",
      "Attention Probabilities:\n",
      "tensor([[[[3.8537e-02, 2.5488e-02, 1.7898e-02,  ..., 1.8548e-02,\n",
      "           8.4391e-03, 9.7451e-03],\n",
      "          [2.6188e-02, 4.3572e-03, 8.4218e-04,  ..., 3.7951e-04,\n",
      "           6.9073e-03, 3.4154e-03],\n",
      "          [7.1134e-02, 6.8691e-04, 8.1536e-01,  ..., 1.5444e-04,\n",
      "           1.0599e-03, 1.8475e-03],\n",
      "          ...,\n",
      "          [1.4734e-02, 2.2819e-03, 1.8334e-03,  ..., 4.5337e-01,\n",
      "           2.0865e-03, 6.7376e-03],\n",
      "          [2.6360e-01, 4.2069e-03, 1.3802e-02,  ..., 2.3347e-04,\n",
      "           5.5918e-04, 1.1654e-03],\n",
      "          [3.2854e-01, 6.7119e-03, 7.8804e-03,  ..., 1.8836e-03,\n",
      "           9.7664e-04, 5.9905e-03]],\n",
      "\n",
      "         [[1.5689e-02, 1.6427e-02, 1.1192e-02,  ..., 1.5245e-02,\n",
      "           9.3643e-03, 1.3359e-02],\n",
      "          [2.0080e-03, 1.0947e-01, 1.9319e-02,  ..., 1.4416e-03,\n",
      "           4.4900e-03, 1.8637e-03],\n",
      "          [9.9130e-03, 6.0275e-02, 2.4719e-03,  ..., 2.5372e-03,\n",
      "           1.2246e-03, 3.0016e-03],\n",
      "          ...,\n",
      "          [5.7453e-03, 3.1509e-03, 6.9035e-03,  ..., 2.1643e-02,\n",
      "           4.9214e-02, 8.4738e-02],\n",
      "          [1.8828e-02, 5.5788e-02, 6.9693e-03,  ..., 6.1186e-02,\n",
      "           5.8219e-02, 1.8837e-02],\n",
      "          [4.1552e-02, 1.8817e-02, 4.8137e-03,  ..., 5.2909e-02,\n",
      "           3.9944e-02, 4.2537e-02]]]], device='cuda:0')\n",
      "torch.Size([1, 2, 69, 69])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scores\n",
    "print(f'Raw Scores:\\n{raw_scores}\\n{raw_scores.shape}\\n')\n",
    "print(f'Attention Scores:\\n{attention_scores}\\n{attention_scores.shape}\\n')\n",
    "print(f'Attention Probabilities:\\n{attention_probs}\\n{attention_probs.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 For Whole Dataset\n",
    "- In batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input embeddings\n",
    "# input_embeddings_val = torch.load('./data_BERTtiny/input_embeddings_val.pt')\n",
    "# print(input_embeddings_val)\n",
    "# print(input_embeddings_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectors and scores\n",
    "q_vectors_val = torch.zeros((2, batch_size, 2, max_length, 64), device=device)\n",
    "k_vectors_val = torch.zeros((2, batch_size, 2, max_length, 64), device=device)\n",
    "v_vectors_val = torch.zeros((2, batch_size, 2, max_length, 64), device=device)\n",
    "raw_scores_val = torch.zeros((2, batch_size, 2, max_length, max_length), device=device)\n",
    "attention_scores_val = torch.zeros((2, batch_size, 2, max_length, max_length), device=device)\n",
    "attention_probs_val = torch.zeros((2, batch_size, 2, max_length, max_length), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 18.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V vectors and attention scores\n",
    "# Retrive the number of batches\n",
    "n_batches,_,_,_ = input_embeddings_val.shape\n",
    "\n",
    "# Calculate per batch\n",
    "for n_batch in tqdm(range(n_batches)):\n",
    "    with torch.no_grad():\n",
    "        # batch_result,_,_,_,_,_ = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        # # Store the batch result\n",
    "        # q_vectors_val[n_batch,:,:,:,:] = batch_result\n",
    "\n",
    "        # _,batch_result,_,_,_,_ = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        # # Store the batch result\n",
    "        # k_vectors_val[n_batch,:,:,:,:] = batch_result\n",
    "\n",
    "        # _,_,batch_result,_,_,_ = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        # # Store the batch result\n",
    "        # v_vectors_val[n_batch,:,:,:,:] = batch_result\n",
    "\n",
    "        # _,_,_,batch_result,_,_ = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        # # Store the batch result\n",
    "        # raw_scores_val[n_batch,:,:,:,:] = batch_result\n",
    "\n",
    "        qv,kv,vv,rs,atts,attp = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        \n",
    "        # Store the batch result\n",
    "        q_vectors_val[n_batch,:,:,:,:] = qv\n",
    "        k_vectors_val[n_batch,:,:,:,:] = kv\n",
    "        v_vectors_val[n_batch,:,:,:,:] = vv\n",
    "        raw_scores_val[n_batch,:,:,:,:] = rs\n",
    "        attention_scores_val[n_batch,:,:,:,:] = atts\n",
    "        attention_probs_val[n_batch,:,:,:,:] = attp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-vectors:\n",
      "tensor([[[[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-0.6773,  0.9066, -4.0874,  ..., -0.6793, -0.7928, -1.6184],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 1.1744, -0.8074, -0.9980,  ...,  0.0900, -0.6321,  1.7567],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-0.6773,  0.9066, -4.0874,  ..., -0.6793, -0.7928, -1.6184],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 1.1744, -0.8074, -0.9980,  ...,  0.0900, -0.6321,  1.7567],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [ 0.5017, -1.1026, -0.3460,  ...,  0.3640, -0.3631,  0.1105],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 0.7719, -0.0559,  0.1301,  ...,  0.3718, -0.8937,  0.6945],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-1.6257,  1.1277,  0.0198,  ..., -0.8263,  0.8027, -0.9277],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 0.8205,  0.3007,  2.3900,  ..., -0.4281, -0.4849,  0.5456],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [ 0.2891, -0.5385, -2.7580,  ...,  0.0873, -0.0500,  0.5599],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 1.2315,  0.2426, -0.0233,  ..., -0.2859,  0.4238,  2.7562],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-1.5317, -0.7928, -0.0197,  ...,  1.2416, -0.1361, -0.0827],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 1.2609, -0.4677,  0.5253,  ..., -0.1138, -0.9192,  1.5529],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-0.3024, -0.1131, -2.1929,  ..., -0.2766,  0.3280, -0.7343],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 1.1957, -0.5314, -0.0331,  ..., -0.4229, -0.0761,  2.6858],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [ 1.3858, -2.4142, -1.4560,  ...,  1.9618, -0.6486,  0.0608],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 0.6267,  1.3908, -0.9874,  ..., -0.4928,  0.1745,  1.8018],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-0.0884, -1.0859, -3.0904,  ...,  0.7268,  0.5705, -0.5236],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 0.1258, -1.5278,  0.0750,  ...,  0.2210, -0.2645,  1.8418],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-1.2323, -0.2247, -1.3719,  ...,  0.1946, -0.8077, -1.3006],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 1.0422, -1.0087,  0.9627,  ...,  0.7972, -0.2295,  1.3635],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-0.6773,  0.9066, -4.0874,  ..., -0.6793, -0.7928, -1.6184],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 1.1744, -0.8074, -0.9980,  ...,  0.0900, -0.6321,  1.7567],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7724, -0.1844, -0.7572,  ..., -0.5177, -0.5431, -0.6973],\n",
      "           [-0.7792, -0.2160, -1.5379,  ..., -0.4398, -0.2326, -0.3270],\n",
      "           [-1.5317, -0.7928, -0.0197,  ...,  1.2416, -0.1361, -0.0827],\n",
      "           ...,\n",
      "           [ 0.4866, -0.4238,  0.4320,  ..., -0.2677,  0.0550, -1.4613],\n",
      "           [ 0.1128, -0.6197,  0.5731,  ..., -0.6105, -0.0373, -1.3682],\n",
      "           [ 0.1441, -0.5963,  0.4313,  ..., -0.9081, -0.0794, -1.5401]],\n",
      "\n",
      "          [[ 0.0299, -0.3081,  0.6008,  ...,  0.4644, -0.3479,  0.6314],\n",
      "           [ 0.5103,  0.6868, -0.3434,  ...,  0.2444, -0.1602,  1.6353],\n",
      "           [ 1.2609, -0.4677,  0.5253,  ..., -0.1138, -0.9192,  1.5529],\n",
      "           ...,\n",
      "           [ 0.3783, -0.5820,  0.3166,  ...,  2.0282,  0.7023,  0.3427],\n",
      "           [ 0.3963, -0.5898,  0.2797,  ...,  2.0259,  0.7496,  0.2232],\n",
      "           [ 0.4309, -0.7875,  0.4601,  ...,  1.8729,  0.7979,  0.1808]]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 64])\n",
      "\n",
      "K-vectors:\n",
      "tensor([[[[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-1.6402,  1.5270, -3.5493,  ...,  0.6948, -0.9864, -1.6246],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 1.3916,  0.9548,  0.8632,  ..., -0.1681,  0.8221,  0.6208],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-1.6402,  1.5270, -3.5493,  ...,  0.6948, -0.9864, -1.6246],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 1.3916,  0.9548,  0.8632,  ..., -0.1681,  0.8221,  0.6208],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-0.7216,  0.0062,  0.4511,  ...,  1.6136, -0.8490, -0.4857],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 0.4349, -0.3875,  0.6222,  ..., -0.8136,  1.4272,  0.3561],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-1.6272,  1.9796,  0.1720,  ...,  0.8690,  0.2780, -1.6561],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 1.1167, -0.6714, -1.6164,  ..., -0.9843,  1.3306, -0.1107],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-1.5664,  0.1912, -1.8664,  ...,  2.1301, -1.3406,  0.2248],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [-0.6516, -0.2070,  1.3185,  ..., -0.8532, -0.2541,  1.5382],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-2.6419,  0.2559,  1.4569,  ...,  2.3384, -1.4454,  0.2805],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [-1.5622, -0.5290,  0.6762,  ..., -0.7286,  0.5309,  1.8415],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-1.0566,  1.1945, -1.6621,  ...,  0.8305, -0.5814, -1.1578],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 1.2153,  1.0496,  0.1579,  ...,  0.4022,  1.6818,  0.2472],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [ 0.9136, -2.2213, -1.9314,  ...,  2.2239, -1.0746,  0.1042],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 1.0453,  0.1073,  1.9398,  ..., -0.2762, -1.7406,  1.7415],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-1.1886, -0.1476, -2.6076,  ...,  1.6482,  0.2097, -0.6650],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 1.0361,  0.8914,  1.3454,  ..., -1.0690,  1.2433, -0.0761],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-0.9015,  1.2179, -1.4918,  ...,  1.0224, -1.1378, -2.3789],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 1.9024,  0.0221, -1.7396,  ..., -0.0671,  2.1114, -0.0584],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-1.6402,  1.5270, -3.5493,  ...,  0.6948, -0.9864, -1.6246],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [ 1.3916,  0.9548,  0.8632,  ..., -0.1681,  0.8221,  0.6208],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1423,  0.3171, -0.7264,  ...,  0.3873,  0.1567, -1.3134],\n",
      "           [-1.3028,  0.8120,  0.2708,  ...,  0.5056,  0.0416, -0.7282],\n",
      "           [-2.6419,  0.2559,  1.4569,  ...,  2.3384, -1.4454,  0.2805],\n",
      "           ...,\n",
      "           [ 0.0094, -0.0418,  0.6228,  ...,  0.2997, -1.0667, -0.3566],\n",
      "           [ 0.2481, -0.1589,  0.3828,  ...,  0.7434, -1.1507, -0.2205],\n",
      "           [ 0.3491, -0.1724,  0.2685,  ...,  1.1564, -0.9190, -0.6015]],\n",
      "\n",
      "          [[-1.3093,  0.9310, -0.0996,  ..., -1.0065,  0.6406,  0.4245],\n",
      "           [ 1.0339,  0.6887,  1.0284,  ..., -0.5431, -0.9626,  1.3227],\n",
      "           [-1.5622, -0.5290,  0.6762,  ..., -0.7286,  0.5309,  1.8415],\n",
      "           ...,\n",
      "           [-0.2051, -1.2116,  0.8978,  ...,  1.2031,  0.7739, -1.3002],\n",
      "           [-0.3082, -1.1808,  0.9637,  ...,  1.4507,  0.6524, -1.2839],\n",
      "           [-0.0773, -1.1523,  1.0905,  ...,  1.2722,  0.6947, -1.4122]]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 64])\n",
      "\n",
      "V-vectors:\n",
      "tensor([[[[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [-0.0887,  0.1902, -1.8055,  ...,  1.1611, -0.4883,  1.5955],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [-3.2188,  0.2406,  3.9247,  ..., -2.2501, -1.9295, -0.0477],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [-0.0887,  0.1902, -1.8055,  ...,  1.1611, -0.4883,  1.5955],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [-3.2188,  0.2406,  3.9247,  ..., -2.2501, -1.9295, -0.0477],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [ 0.3580, -1.4135, -0.6649,  ...,  0.4109, -0.4523, -0.8944],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [ 1.7806,  3.9142, -1.1227,  ...,  1.2031, -1.0233,  1.0314],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [-0.9043, -1.8368, -0.4268,  ...,  0.6584, -2.3446, -0.4144],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [ 0.2124,  0.8299,  2.5076,  ...,  0.8373,  0.4662,  1.7379],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [ 1.3370, -1.1771, -0.8298,  ..., -0.6822, -3.0854,  0.1772],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [ 0.0525, -1.1155,  3.3663,  ...,  0.0319,  0.1748,  1.7451],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [ 0.2307, -0.8392,  1.1254,  ...,  0.5365, -1.7687, -0.1275],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [ 1.1491,  0.3240,  0.4041,  ...,  0.1425, -0.5015,  1.1674],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [ 0.0736, -0.2249, -1.5338,  ..., -0.2521, -1.4959, -0.2662],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [ 0.0701,  0.4249,  1.5720,  ...,  0.2793, -0.2409,  3.3012],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [-1.3715, -0.7259, -1.4530,  ..., -0.1775, -1.0562,  0.8969],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [ 0.3574, -2.4160, -1.2202,  ..., -0.6508,  1.5118,  2.3585],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [ 0.7500, -1.2801, -0.8873,  ..., -0.0342, -1.1608, -0.7027],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [-1.1137,  0.9171, -0.4179,  ..., -0.1415, -1.6985,  1.9226],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [-1.1021, -0.0777, -0.4363,  ...,  0.6880, -1.5428,  0.4230],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [-2.0151,  0.4728, -0.3019,  ..., -2.9485, -1.5141,  0.3889],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [-0.0887,  0.1902, -1.8055,  ...,  1.1611, -0.4883,  1.5955],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [-3.2188,  0.2406,  3.9247,  ..., -2.2501, -1.9295, -0.0477],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]],\n",
      "\n",
      "\n",
      "         [[[-0.0306,  0.8794,  0.3482,  ...,  0.0678, -0.5800,  0.4442],\n",
      "           [-0.0292, -2.2986, -1.5061,  ..., -0.0760, -1.8689, -0.1315],\n",
      "           [ 0.2307, -0.8392,  1.1254,  ...,  0.5365, -1.7687, -0.1275],\n",
      "           ...,\n",
      "           [ 0.7853,  1.4219, -1.3761,  ...,  2.6796, -0.5695,  2.3787],\n",
      "           [ 0.7577,  1.3850, -1.3877,  ...,  2.6445, -0.5405,  2.3283],\n",
      "           [ 0.8627,  1.4573, -1.5865,  ...,  2.7625, -0.7152,  2.5571]],\n",
      "\n",
      "          [[-0.0824,  0.3352,  1.1458,  ..., -0.2225,  0.6379, -2.5767],\n",
      "           [ 0.0520, -0.1606,  3.5901,  ..., -1.4961, -0.5940,  0.9495],\n",
      "           [ 1.1491,  0.3240,  0.4041,  ...,  0.1425, -0.5015,  1.1674],\n",
      "           ...,\n",
      "           [-1.5770, -0.7820, -0.5579,  ..., -0.9563,  0.6369,  0.5856],\n",
      "           [-1.3915, -0.9425, -0.5148,  ..., -0.9476,  0.7118,  0.4861],\n",
      "           [-1.4621, -0.7560, -0.3855,  ..., -0.4175,  0.4150,  0.1178]]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectors\n",
    "print(f'Q-vectors:\\n{q_vectors_val}\\n{q_vectors_val.shape}\\n')\n",
    "print(f'K-vectors:\\n{k_vectors_val}\\n{k_vectors_val.shape}\\n')\n",
    "print(f'V-vectors:\\n{v_vectors_val}\\n{v_vectors_val.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Scores:\n",
      "tensor([[[[[  5.0973,   1.7900,   7.0608,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234,  -3.1363,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 34.2796,   2.8866,  62.8255,  ...,  -2.9427,  -2.9324,\n",
      "             -0.7136],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,   3.4840,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   2.7754,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,   1.5354,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  13.5287,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  20.8193,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [  8.8139,  24.5241,  -1.9616,  ..., -17.3607, -17.2270,\n",
      "            -15.1051],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   4.3306,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   3.6848,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   5.7580,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,   7.0608,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234,  -3.1363,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 34.2796,   2.8866,  62.8255,  ...,  -2.9427,  -2.9324,\n",
      "             -0.7136],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,   3.4840,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   2.7754,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,   1.5354,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  13.5287,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  20.8193,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [  8.8139,  24.5241,  -1.9616,  ..., -17.3607, -17.2270,\n",
      "            -15.1051],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   4.3306,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   3.6848,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   5.7580,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,   2.8946,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234, -14.7711,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ -4.7737, -12.7320,  54.4122,  ...,   5.3488,   5.3272,\n",
      "              6.6113],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,   8.1008,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   7.2518,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,   7.0017,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  13.9394,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  20.9277,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [ 12.3883,  18.9813,   1.6435,  ..., -16.3626, -16.3435,\n",
      "            -12.5677],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   1.5328,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   0.9485,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   4.1058,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,   4.0489,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234,   2.6480,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 11.2156,  -7.1673,  57.2960,  ...,   7.4615,   6.4739,\n",
      "              7.2223],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,  10.5617,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   9.6621,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,   9.0814,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,   9.8137,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  29.1408,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [  6.6494,  22.9028,  10.0987,  ...,  -9.6773,  -9.7979,\n",
      "             -6.0969],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   5.6842,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   4.5836,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   6.8357,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,  -8.5020,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234, -17.0626,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [  6.6397, -17.1879,  60.3782,  ...,  -7.2822,  -7.0746,\n",
      "             -6.4772],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,  -9.5167,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,  -9.5833,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941, -10.1704,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  26.9661,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  28.7419,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [  3.7006,  21.0390,  24.1588,  ...,  -7.2429,  -7.5450,\n",
      "             -5.0645],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   1.6647,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   0.5064,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   3.7045,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,  -8.8359,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234, -17.9820,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 12.4016,  -7.9487,  41.1044,  ...,  -5.7325,  -5.6403,\n",
      "             -4.8667],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,  -0.5300,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   0.2577,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,  -1.1403,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,   8.9760,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  21.9592,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [  6.0945,  21.6572,  13.4357,  ...,  -8.7676,  -8.6028,\n",
      "             -5.8454],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,  -5.4652,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,  -6.2963,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,  -3.7534,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[  5.0973,   1.7900,  11.1936,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234,  -1.9680,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 19.8655,  -7.6832,  39.1818,  ...,  -1.7652,  -2.9963,\n",
      "             -1.6343],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,   6.9488,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   5.7402,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,   3.9437,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  11.8180,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  29.0981,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [ 13.2148,  29.5450,   4.6786,  ...,  -6.8817,  -6.7178,\n",
      "             -4.2728],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   8.1930,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   7.0578,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   9.1409,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,  -3.1013,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234,  -4.3969,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 28.4705,  -9.1576,  64.5196,  ...,  -2.6329,  -1.5085,\n",
      "             -0.9657],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,   3.3541,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   3.7372,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,   1.6821,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  44.4770,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  21.6727,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [ 17.3077,  16.6332,  -3.4355,  ..., -18.3732, -18.6682,\n",
      "            -16.8672],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,  -0.3198,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,  -1.6274,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   2.2289,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,   5.4902,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234,  -4.2729,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 15.9523,  -3.8516,  61.3240,  ...,  -4.8114,  -5.6192,\n",
      "             -3.1257],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,   0.4492,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,  -0.8923,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,  -0.3532,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  12.3779,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  18.0375,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [ 13.8696,  19.2215,   7.4647,  ..., -15.5439, -15.8282,\n",
      "            -13.2013],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   4.7070,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   4.1300,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   6.2150,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,   9.6503,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234,   1.5968,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 12.0185,  -4.3587,  52.5744,  ...,   3.5964,   2.9112,\n",
      "              4.2789],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,   8.9494,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   7.7833,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,   7.6305,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  12.8975,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  19.7469,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [  9.2085,  15.7761,   4.3638,  ...,  -7.0009,  -7.3175,\n",
      "             -4.3680],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   9.6931,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   9.1366,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,  10.9745,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,   7.0608,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234,  -3.1363,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 34.2796,   2.8866,  62.8255,  ...,  -2.9427,  -2.9324,\n",
      "             -0.7136],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,   3.4840,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   2.7754,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,   1.5354,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,  13.5287,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  20.8193,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [  8.8139,  24.5241,  -1.9616,  ..., -17.3607, -17.2270,\n",
      "            -15.1051],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,   4.3306,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,   3.6848,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,   5.7580,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]],\n",
      "\n",
      "\n",
      "         [[[  5.0973,   1.7900,  -8.8359,  ...,  -5.3198,  -5.0817,\n",
      "             -5.5411],\n",
      "           [ 20.2714,   5.9234, -17.9820,  ...,   0.6280,   0.5234,\n",
      "              3.5267],\n",
      "           [ 12.4016,  -7.9487,  41.1044,  ...,  -5.7325,  -5.6403,\n",
      "             -4.8667],\n",
      "           ...,\n",
      "           [ 16.1243,  -0.4355,  -0.5300,  ...,  -9.1651,  -8.4490,\n",
      "             -4.9260],\n",
      "           [ 15.8720,  -0.7835,   0.2577,  ...,  -8.8131,  -9.3134,\n",
      "             -6.8017],\n",
      "           [ 16.0441,  -1.2941,  -1.1403,  ...,  -7.1257,  -8.5072,\n",
      "             -6.8205]],\n",
      "\n",
      "          [[ 13.8412,  14.2090,   8.9760,  ...,  12.7296,  13.3569,\n",
      "             15.1335],\n",
      "           [  6.3428,  38.3307,  21.9592,  ..., -19.3925, -19.4271,\n",
      "            -15.6847],\n",
      "           [  6.0945,  21.6572,  13.4357,  ...,  -8.7676,  -8.6028,\n",
      "             -5.8454],\n",
      "           ...,\n",
      "           [ -1.4523,  -0.5836,  -5.4652,  ...,   5.2695,   5.6589,\n",
      "              3.7459],\n",
      "           [ -1.1162,  -1.7134,  -6.2963,  ...,   5.5421,   5.8801,\n",
      "              4.0817],\n",
      "           [ -0.2725,   1.7314,  -3.7534,  ...,   4.3381,   4.6592,\n",
      "              3.4527]]]]], device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 384])\n",
      "\n",
      "Attention Scores:\n",
      "tensor([[[[[ 0.6372,  0.2237,  0.8826,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -0.3920,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 4.2849,  0.3608,  7.8532,  ..., -0.3678, -0.3665, -0.0892],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  0.4355,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.3469,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618,  0.1919,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.6911,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.6024,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 1.1017,  3.0655, -0.2452,  ..., -2.1701, -2.1534, -1.8881],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  0.5413,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  0.4606,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  0.7198,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237,  0.8826,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -0.3920,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 4.2849,  0.3608,  7.8532,  ..., -0.3678, -0.3665, -0.0892],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  0.4355,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.3469,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618,  0.1919,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.6911,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.6024,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 1.1017,  3.0655, -0.2452,  ..., -2.1701, -2.1534, -1.8881],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  0.5413,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  0.4606,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  0.7198,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237,  0.3618,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -1.8464,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [-0.5967, -1.5915,  6.8015,  ...,  0.6686,  0.6659,  0.8264],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  1.0126,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.9065,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618,  0.8752,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.7424,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.6160,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 1.5485,  2.3727,  0.2054,  ..., -2.0453, -2.0429, -1.5710],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  0.1916,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  0.1186,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  0.5132,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237,  0.5061,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404,  0.3310,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 1.4019, -0.8959,  7.1620,  ...,  0.9327,  0.8092,  0.9028],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  1.3202,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  1.2078,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618,  1.1352,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.2267,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  3.6426,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 0.8312,  2.8628,  1.2623,  ..., -1.2097, -1.2247, -0.7621],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  0.7105,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  0.5729,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  0.8545,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237, -1.0628,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -2.1328,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 0.8300, -2.1485,  7.5473,  ..., -0.9103, -0.8843, -0.8096],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544, -1.1896,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979, -1.1979,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618, -1.2713,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  3.3708,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  3.5927,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 0.4626,  2.6299,  3.0198,  ..., -0.9054, -0.9431, -0.6331],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  0.2081,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  0.0633,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  0.4631,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237, -1.1045,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -2.2478,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 1.5502, -0.9936,  5.1381,  ..., -0.7166, -0.7050, -0.6083],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544, -0.0663,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.0322,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618, -0.1425,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.1220,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.7449,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 0.7618,  2.7071,  1.6795,  ..., -1.0959, -1.0754, -0.7307],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730, -0.6831,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142, -0.7870,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164, -0.4692,  ...,  0.5423,  0.5824,  0.4316]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.6372,  0.2237,  1.3992,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -0.2460,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 2.4832, -0.9604,  4.8977,  ..., -0.2206, -0.3745, -0.2043],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  0.8686,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.7175,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618,  0.4930,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.4772,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  3.6373,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 1.6518,  3.6931,  0.5848,  ..., -0.8602, -0.8397, -0.5341],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  1.0241,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  0.8822,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  1.1426,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237, -0.3877,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -0.5496,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 3.5588, -1.1447,  8.0650,  ..., -0.3291, -0.1886, -0.1207],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  0.4193,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.4672,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618,  0.2103,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  5.5596,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.7091,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 2.1635,  2.0792, -0.4294,  ..., -2.2967, -2.3335, -2.1084],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730, -0.0400,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142, -0.2034,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  0.2786,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237,  0.6863,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -0.5341,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 1.9940, -0.4814,  7.6655,  ..., -0.6014, -0.7024, -0.3907],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  0.0562,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979, -0.1115,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618, -0.0441,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.5472,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.2547,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 1.7337,  2.4027,  0.9331,  ..., -1.9430, -1.9785, -1.6502],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  0.5884,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  0.5162,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  0.7769,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237,  1.2063,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404,  0.1996,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 1.5023, -0.5448,  6.5718,  ...,  0.4495,  0.3639,  0.5349],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  1.1187,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.9729,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618,  0.9538,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.6122,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.4684,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 1.1511,  1.9720,  0.5455,  ..., -0.8751, -0.9147, -0.5460],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  1.2116,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  1.1421,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  1.3718,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237,  0.8826,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -0.3920,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 4.2849,  0.3608,  7.8532,  ..., -0.3678, -0.3665, -0.0892],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544,  0.4355,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.3469,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618,  0.1919,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.6911,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.6024,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 1.1017,  3.0655, -0.2452,  ..., -2.1701, -2.1534, -1.8881],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730,  0.5413,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142,  0.4606,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164,  0.7198,  ...,  0.5423,  0.5824,  0.4316]]],\n",
      "\n",
      "\n",
      "         [[[ 0.6372,  0.2237, -1.1045,  ..., -0.6650, -0.6352, -0.6926],\n",
      "           [ 2.5339,  0.7404, -2.2478,  ...,  0.0785,  0.0654,  0.4408],\n",
      "           [ 1.5502, -0.9936,  5.1381,  ..., -0.7166, -0.7050, -0.6083],\n",
      "           ...,\n",
      "           [ 2.0155, -0.0544, -0.0663,  ..., -1.1456, -1.0561, -0.6157],\n",
      "           [ 1.9840, -0.0979,  0.0322,  ..., -1.1016, -1.1642, -0.8502],\n",
      "           [ 2.0055, -0.1618, -0.1425,  ..., -0.8907, -1.0634, -0.8526]],\n",
      "\n",
      "          [[ 1.7301,  1.7761,  1.1220,  ...,  1.5912,  1.6696,  1.8917],\n",
      "           [ 0.7929,  4.7913,  2.7449,  ..., -2.4241, -2.4284, -1.9606],\n",
      "           [ 0.7618,  2.7071,  1.6795,  ..., -1.0959, -1.0754, -0.7307],\n",
      "           ...,\n",
      "           [-0.1815, -0.0730, -0.6831,  ...,  0.6587,  0.7074,  0.4682],\n",
      "           [-0.1395, -0.2142, -0.7870,  ...,  0.6928,  0.7350,  0.5102],\n",
      "           [-0.0341,  0.2164, -0.4692,  ...,  0.5423,  0.5824,  0.4316]]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 384])\n",
      "\n",
      "Attention Probabilities:\n",
      "tensor([[[[[6.3386e-03, 4.1923e-03, 8.1018e-03,  ..., 1.7238e-03,\n",
      "            1.7758e-03, 1.6767e-03],\n",
      "           [1.6493e-02, 2.7440e-03, 8.8423e-04,  ..., 1.4155e-03,\n",
      "            1.3971e-03, 2.0337e-03],\n",
      "           [7.4404e-03, 1.4702e-04, 2.6379e-01,  ..., 7.0944e-05,\n",
      "            7.1036e-05, 9.3741e-05],\n",
      "           ...,\n",
      "           [1.6107e-02, 2.0325e-03, 3.3174e-03,  ..., 6.8253e-04,\n",
      "            7.4644e-04, 1.1594e-03],\n",
      "           [1.5761e-02, 1.9652e-03, 3.0662e-03,  ..., 7.2027e-04,\n",
      "            6.7661e-04, 9.2616e-04],\n",
      "           [1.7515e-02, 2.0053e-03, 2.8561e-03,  ..., 9.6738e-04,\n",
      "            8.1395e-04, 1.0050e-03]],\n",
      "\n",
      "          [[3.6925e-03, 3.8663e-03, 3.5511e-03,  ..., 3.2135e-03,\n",
      "            3.4756e-03, 4.3399e-03],\n",
      "           [2.5485e-03, 1.3893e-01, 1.5565e-02,  ..., 1.0214e-04,\n",
      "            1.0170e-04, 1.6236e-04],\n",
      "           [5.2663e-03, 3.7529e-02, 1.3694e-03,  ..., 1.9979e-04,\n",
      "            2.0316e-04, 2.6486e-04],\n",
      "           ...,\n",
      "           [2.5191e-03, 2.8081e-03, 5.1903e-03,  ..., 5.8366e-03,\n",
      "            6.1277e-03, 4.8244e-03],\n",
      "           [2.5352e-03, 2.3529e-03, 4.6201e-03,  ..., 5.8274e-03,\n",
      "            6.0790e-03, 4.8551e-03],\n",
      "           [2.6419e-03, 3.3940e-03, 5.6143e-03,  ..., 4.7013e-03,\n",
      "            4.8938e-03, 4.2087e-03]]],\n",
      "\n",
      "\n",
      "         [[[7.1838e-03, 4.7513e-03, 9.1822e-03,  ..., 1.9536e-03,\n",
      "            2.0127e-03, 1.9003e-03],\n",
      "           [1.3011e-02, 2.1647e-03, 6.9754e-04,  ..., 1.1167e-03,\n",
      "            1.1022e-03, 1.6043e-03],\n",
      "           [9.8509e-03, 1.9465e-04, 3.4926e-01,  ..., 9.3928e-05,\n",
      "            9.4050e-05, 1.2411e-04],\n",
      "           ...,\n",
      "           [1.7309e-02, 2.1841e-03, 3.5650e-03,  ..., 7.3347e-04,\n",
      "            8.0214e-04, 1.2460e-03],\n",
      "           [1.6287e-02, 2.0308e-03, 3.1686e-03,  ..., 7.4433e-04,\n",
      "            6.9922e-04, 9.5710e-04],\n",
      "           [1.7555e-02, 2.0099e-03, 2.8627e-03,  ..., 9.6959e-04,\n",
      "            8.1581e-04, 1.0073e-03]],\n",
      "\n",
      "          [[3.8827e-03, 4.0654e-03, 3.7340e-03,  ..., 3.3790e-03,\n",
      "            3.6546e-03, 4.5634e-03],\n",
      "           [1.3603e-03, 7.4157e-02, 8.3083e-03,  ..., 5.4518e-05,\n",
      "            5.4283e-05, 8.6662e-05],\n",
      "           [4.1170e-03, 2.9338e-02, 1.0705e-03,  ..., 1.5618e-04,\n",
      "            1.5882e-04, 2.0706e-04],\n",
      "           ...,\n",
      "           [2.2921e-03, 2.5550e-03, 4.7224e-03,  ..., 5.3105e-03,\n",
      "            5.5754e-03, 4.3895e-03],\n",
      "           [2.2990e-03, 2.1336e-03, 4.1895e-03,  ..., 5.2843e-03,\n",
      "            5.5124e-03, 4.4026e-03],\n",
      "           [2.3361e-03, 3.0011e-03, 4.9645e-03,  ..., 4.1571e-03,\n",
      "            4.3274e-03, 3.7216e-03]]],\n",
      "\n",
      "\n",
      "         [[[6.5874e-03, 4.3568e-03, 5.0019e-03,  ..., 1.7914e-03,\n",
      "            1.8455e-03, 1.7426e-03],\n",
      "           [1.6311e-02, 2.7138e-03, 2.0424e-04,  ..., 1.3999e-03,\n",
      "            1.3817e-03, 2.0113e-03],\n",
      "           [2.8086e-04, 1.0386e-04, 4.5866e-01,  ..., 9.9541e-04,\n",
      "            9.9272e-04, 1.1656e-03],\n",
      "           ...,\n",
      "           [1.8263e-02, 2.3046e-03, 6.6988e-03,  ..., 7.7390e-04,\n",
      "            8.4637e-04, 1.3147e-03],\n",
      "           [1.7862e-02, 2.2273e-03, 6.0810e-03,  ..., 8.1633e-04,\n",
      "            7.6685e-04, 1.0497e-03],\n",
      "           [1.9523e-02, 2.2352e-03, 6.3048e-03,  ..., 1.0783e-03,\n",
      "            9.0729e-04, 1.1202e-03]],\n",
      "\n",
      "          [[3.8730e-03, 4.0553e-03, 3.9209e-03,  ..., 3.3706e-03,\n",
      "            3.6455e-03, 4.5521e-03],\n",
      "           [1.7791e-03, 9.6990e-02, 1.1015e-02,  ..., 7.1304e-05,\n",
      "            7.0997e-05, 1.1335e-04],\n",
      "           [5.4724e-03, 1.2476e-02, 1.4285e-03,  ..., 1.5045e-04,\n",
      "            1.5081e-04, 2.4177e-04],\n",
      "           ...,\n",
      "           [1.9220e-03, 2.1425e-03, 2.7913e-03,  ..., 4.4531e-03,\n",
      "            4.6752e-03, 3.6809e-03],\n",
      "           [1.9468e-03, 1.8068e-03, 2.5200e-03,  ..., 4.4748e-03,\n",
      "            4.6679e-03, 3.7281e-03],\n",
      "           [1.9585e-03, 2.5160e-03, 3.3853e-03,  ..., 3.4851e-03,\n",
      "            3.6278e-03, 3.1199e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[6.4532e-03, 4.2680e-03, 5.6606e-03,  ..., 1.7549e-03,\n",
      "            1.8079e-03, 1.7071e-03],\n",
      "           [1.7834e-02, 2.9671e-03, 1.9703e-03,  ..., 1.5306e-03,\n",
      "            1.5108e-03, 2.1990e-03],\n",
      "           [1.4957e-03, 1.5027e-04, 4.7467e-01,  ..., 9.3548e-04,\n",
      "            8.2684e-04, 9.0792e-04],\n",
      "           ...,\n",
      "           [1.5447e-02, 1.9493e-03, 7.7068e-03,  ..., 6.5459e-04,\n",
      "            7.1589e-04, 1.1120e-03],\n",
      "           [1.5161e-02, 1.8904e-03, 6.9760e-03,  ..., 6.9287e-04,\n",
      "            6.5087e-04, 8.9092e-04],\n",
      "           [1.6587e-02, 1.8990e-03, 6.9466e-03,  ..., 9.1610e-04,\n",
      "            7.7080e-04, 9.5173e-04]],\n",
      "\n",
      "          [[4.0282e-03, 4.2177e-03, 2.4348e-03,  ..., 3.5056e-03,\n",
      "            3.7915e-03, 4.7344e-03],\n",
      "           [2.5267e-03, 1.3775e-01, 4.3670e-02,  ..., 1.0127e-04,\n",
      "            1.0083e-04, 1.6097e-04],\n",
      "           [3.7210e-03, 2.8379e-02, 5.7268e-03,  ..., 4.8343e-04,\n",
      "            4.7620e-04, 7.5631e-04],\n",
      "           ...,\n",
      "           [2.8591e-03, 3.1870e-03, 6.9765e-03,  ..., 6.6241e-03,\n",
      "            6.9545e-03, 5.4754e-03],\n",
      "           [2.9102e-03, 2.7009e-03, 5.9340e-03,  ..., 6.6893e-03,\n",
      "            6.9781e-03, 5.5732e-03],\n",
      "           [2.9609e-03, 3.8037e-03, 7.1995e-03,  ..., 5.2688e-03,\n",
      "            5.4846e-03, 4.7168e-03]]],\n",
      "\n",
      "\n",
      "         [[[5.5972e-03, 3.7019e-03, 1.0226e-03,  ..., 1.5222e-03,\n",
      "            1.5681e-03, 1.4806e-03],\n",
      "           [1.2677e-02, 2.1091e-03, 1.1920e-04,  ..., 1.0880e-03,\n",
      "            1.0739e-03, 1.5631e-03],\n",
      "           [9.3271e-04, 4.7449e-05, 7.7098e-01,  ..., 1.6367e-04,\n",
      "            1.6797e-04, 1.8100e-04],\n",
      "           ...,\n",
      "           [1.6218e-02, 2.0465e-03, 6.5769e-04,  ..., 6.8724e-04,\n",
      "            7.5159e-04, 1.1674e-03],\n",
      "           [1.5820e-02, 1.9726e-03, 6.5664e-04,  ..., 7.2300e-04,\n",
      "            6.7917e-04, 9.2967e-04],\n",
      "           [1.6804e-02, 1.9238e-03, 6.3431e-04,  ..., 9.2809e-04,\n",
      "            7.8089e-04, 9.6418e-04]],\n",
      "\n",
      "          [[4.2276e-03, 4.4265e-03, 2.1808e-02,  ..., 3.6792e-03,\n",
      "            3.9793e-03, 4.9688e-03],\n",
      "           [1.4358e-03, 7.8276e-02, 2.3609e-02,  ..., 5.7546e-05,\n",
      "            5.7298e-05, 9.1476e-05],\n",
      "           [1.8349e-03, 1.6027e-02, 2.3671e-02,  ..., 4.6721e-04,\n",
      "            4.4990e-04, 6.1345e-04],\n",
      "           ...,\n",
      "           [1.8357e-03, 2.0462e-03, 2.7102e-03,  ..., 4.2530e-03,\n",
      "            4.4651e-03, 3.5154e-03],\n",
      "           [1.8678e-03, 1.7335e-03, 2.2878e-03,  ..., 4.2933e-03,\n",
      "            4.4786e-03, 3.5769e-03],\n",
      "           [1.9323e-03, 2.4823e-03, 3.1767e-03,  ..., 3.4385e-03,\n",
      "            3.5793e-03, 3.0782e-03]]],\n",
      "\n",
      "\n",
      "         [[[7.0339e-03, 4.6522e-03, 1.2326e-03,  ..., 1.9129e-03,\n",
      "            1.9707e-03, 1.8607e-03],\n",
      "           [6.9251e-03, 1.1522e-03, 5.8046e-05,  ..., 5.9436e-04,\n",
      "            5.8664e-04, 8.5391e-04],\n",
      "           [4.5979e-03, 3.6125e-04, 1.6624e-01,  ..., 4.7656e-04,\n",
      "            4.8208e-04, 5.3102e-04],\n",
      "           ...,\n",
      "           [1.6708e-02, 2.1083e-03, 2.0835e-03,  ..., 7.0799e-04,\n",
      "            7.7429e-04, 1.2027e-03],\n",
      "           [1.5963e-02, 1.9905e-03, 2.2671e-03,  ..., 7.2954e-04,\n",
      "            6.8532e-04, 9.3809e-04],\n",
      "           [1.7292e-02, 1.9797e-03, 2.0181e-03,  ..., 9.5504e-04,\n",
      "            8.0357e-04, 9.9218e-04]],\n",
      "\n",
      "          [[3.8365e-03, 4.0170e-03, 2.0884e-03,  ..., 3.3388e-03,\n",
      "            3.6111e-03, 4.5091e-03],\n",
      "           [1.1312e-03, 6.1666e-02, 7.9669e-03,  ..., 4.5335e-05,\n",
      "            4.5140e-05, 7.2065e-05],\n",
      "           [2.8357e-03, 1.9839e-02, 7.0989e-03,  ..., 4.4243e-04,\n",
      "            4.5164e-04, 6.3750e-04],\n",
      "           ...,\n",
      "           [2.2561e-03, 2.5149e-03, 1.3662e-03,  ..., 5.2272e-03,\n",
      "            5.4879e-03, 4.3207e-03],\n",
      "           [2.2889e-03, 2.1243e-03, 1.1979e-03,  ..., 5.2613e-03,\n",
      "            5.4884e-03, 4.3834e-03],\n",
      "           [2.3072e-03, 2.9640e-03, 1.4932e-03,  ..., 4.1057e-03,\n",
      "            4.2739e-03, 3.6756e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[6.8253e-03, 4.5142e-03, 1.4624e-02,  ..., 1.8561e-03,\n",
      "            1.9122e-03, 1.8055e-03],\n",
      "           [8.9856e-03, 1.4950e-03, 5.5750e-04,  ..., 7.7121e-04,\n",
      "            7.6119e-04, 1.1080e-03],\n",
      "           [1.4920e-02, 4.7668e-04, 1.6687e-01,  ..., 9.9884e-04,\n",
      "            8.5638e-04, 1.0153e-03],\n",
      "           ...,\n",
      "           [1.6899e-02, 2.1324e-03, 5.3672e-03,  ..., 7.1610e-04,\n",
      "            7.8316e-04, 1.2165e-03],\n",
      "           [1.6242e-02, 2.0252e-03, 4.5773e-03,  ..., 7.4226e-04,\n",
      "            6.9727e-04, 9.5444e-04],\n",
      "           [1.7467e-02, 1.9998e-03, 3.8488e-03,  ..., 9.6474e-04,\n",
      "            8.1173e-04, 1.0023e-03]],\n",
      "\n",
      "          [[3.7527e-03, 3.9293e-03, 2.9141e-03,  ..., 3.2659e-03,\n",
      "            3.5323e-03, 4.4106e-03],\n",
      "           [1.5794e-03, 8.6102e-02, 2.7152e-02,  ..., 6.3299e-05,\n",
      "            6.3026e-05, 1.0062e-04],\n",
      "           [5.3640e-03, 4.1305e-02, 1.8454e-03,  ..., 4.3503e-04,\n",
      "            4.4403e-04, 6.0276e-04],\n",
      "           ...,\n",
      "           [2.3492e-03, 2.6186e-03, 7.8438e-03,  ..., 5.4428e-03,\n",
      "            5.7143e-03, 4.4989e-03],\n",
      "           [2.4060e-03, 2.2329e-03, 6.6840e-03,  ..., 5.5303e-03,\n",
      "            5.7690e-03, 4.6075e-03],\n",
      "           [2.4761e-03, 3.1810e-03, 8.0316e-03,  ..., 4.4063e-03,\n",
      "            4.5868e-03, 3.9446e-03]]],\n",
      "\n",
      "\n",
      "         [[[7.1270e-03, 4.7137e-03, 2.5576e-03,  ..., 1.9382e-03,\n",
      "            1.9967e-03, 1.8853e-03],\n",
      "           [9.9806e-03, 1.6605e-03, 4.5709e-04,  ..., 8.5661e-04,\n",
      "            8.4548e-04, 1.2307e-03],\n",
      "           [9.3108e-03, 8.4389e-05, 8.4330e-01,  ..., 1.9076e-04,\n",
      "            2.1955e-04, 2.3496e-04],\n",
      "           ...,\n",
      "           [1.7327e-02, 2.1864e-03, 3.5112e-03,  ..., 7.3423e-04,\n",
      "            8.0298e-04, 1.2473e-03],\n",
      "           [1.6416e-02, 2.0469e-03, 3.6016e-03,  ..., 7.5021e-04,\n",
      "            7.0474e-04, 9.6466e-04],\n",
      "           [1.7657e-02, 2.0216e-03, 2.9326e-03,  ..., 9.7525e-04,\n",
      "            8.2057e-04, 1.0132e-03]],\n",
      "\n",
      "          [[3.1026e-03, 3.2486e-03, 1.4284e-01,  ..., 2.7002e-03,\n",
      "            2.9204e-03, 3.6466e-03],\n",
      "           [1.7395e-03, 9.4833e-02, 1.1821e-02,  ..., 6.9718e-05,\n",
      "            6.9418e-05, 1.1082e-04],\n",
      "           [1.6439e-02, 1.5109e-02, 1.2296e-03,  ..., 1.9005e-04,\n",
      "            1.8317e-04, 2.2941e-04],\n",
      "           ...,\n",
      "           [2.7769e-03, 3.0954e-03, 3.1991e-03,  ..., 6.4337e-03,\n",
      "            6.7546e-03, 5.3180e-03],\n",
      "           [2.8687e-03, 2.6623e-03, 2.6911e-03,  ..., 6.5938e-03,\n",
      "            6.8784e-03, 5.4936e-03],\n",
      "           [2.8879e-03, 3.7099e-03, 3.9479e-03,  ..., 5.1389e-03,\n",
      "            5.3494e-03, 4.6005e-03]]],\n",
      "\n",
      "\n",
      "         [[[6.7646e-03, 4.4740e-03, 7.1051e-03,  ..., 1.8396e-03,\n",
      "            1.8952e-03, 1.7894e-03],\n",
      "           [1.7378e-02, 2.8914e-03, 8.0831e-04,  ..., 1.4915e-03,\n",
      "            1.4722e-03, 2.1429e-03],\n",
      "           [9.4779e-04, 7.9730e-05, 2.7529e-01,  ..., 7.0716e-05,\n",
      "            6.3925e-05, 8.7303e-05],\n",
      "           ...,\n",
      "           [1.6949e-02, 2.1387e-03, 2.3889e-03,  ..., 7.1822e-04,\n",
      "            7.8547e-04, 1.2201e-03],\n",
      "           [1.6587e-02, 2.0682e-03, 2.0403e-03,  ..., 7.5805e-04,\n",
      "            7.1210e-04, 9.7474e-04],\n",
      "           [1.8428e-02, 2.1098e-03, 2.3731e-03,  ..., 1.0178e-03,\n",
      "            8.5637e-04, 1.0574e-03]],\n",
      "\n",
      "          [[3.9286e-03, 4.1134e-03, 3.2719e-03,  ..., 3.4189e-03,\n",
      "            3.6978e-03, 4.6173e-03],\n",
      "           [2.5486e-03, 1.3894e-01, 1.0995e-02,  ..., 1.0215e-04,\n",
      "            1.0170e-04, 1.6237e-04],\n",
      "           [6.5498e-03, 1.2787e-02, 2.9412e-03,  ..., 1.6575e-04,\n",
      "            1.5997e-04, 2.2215e-04],\n",
      "           ...,\n",
      "           [2.7824e-03, 3.1015e-03, 6.0087e-03,  ..., 6.4464e-03,\n",
      "            6.7679e-03, 5.3285e-03],\n",
      "           [2.8390e-03, 2.6348e-03, 5.4698e-03,  ..., 6.5257e-03,\n",
      "            6.8074e-03, 5.4369e-03],\n",
      "           [2.8816e-03, 3.7019e-03, 6.4837e-03,  ..., 5.1278e-03,\n",
      "            5.3379e-03, 4.5906e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[6.9615e-03, 4.6043e-03, 1.2299e-02,  ..., 1.8932e-03,\n",
      "            1.9504e-03, 1.8415e-03],\n",
      "           [1.7480e-02, 2.9082e-03, 1.6934e-03,  ..., 1.5003e-03,\n",
      "            1.4808e-03, 2.1554e-03],\n",
      "           [2.0980e-03, 2.7086e-04, 3.3378e-01,  ..., 7.3215e-04,\n",
      "            6.7205e-04, 7.9735e-04],\n",
      "           ...,\n",
      "           [1.6525e-02, 2.0853e-03, 6.7397e-03,  ..., 7.0026e-04,\n",
      "            7.6583e-04, 1.1896e-03],\n",
      "           [1.5909e-02, 1.9837e-03, 5.7880e-03,  ..., 7.2705e-04,\n",
      "            6.8298e-04, 9.3488e-04],\n",
      "           [1.7376e-02, 1.9894e-03, 6.0704e-03,  ..., 9.5974e-04,\n",
      "            8.0752e-04, 9.9705e-04]],\n",
      "\n",
      "          [[3.8075e-03, 3.9866e-03, 3.3838e-03,  ..., 3.3136e-03,\n",
      "            3.5838e-03, 4.4750e-03],\n",
      "           [2.2612e-03, 1.2327e-01, 1.2078e-02,  ..., 9.0624e-05,\n",
      "            9.0233e-05, 1.4406e-04],\n",
      "           [5.4934e-03, 1.2484e-02, 2.9980e-03,  ..., 7.2424e-04,\n",
      "            6.9613e-04, 1.0065e-03],\n",
      "           ...,\n",
      "           [3.2515e-03, 3.6244e-03, 1.3096e-02,  ..., 7.5332e-03,\n",
      "            7.9090e-03, 6.2269e-03],\n",
      "           [3.3011e-03, 3.0637e-03, 1.1892e-02,  ..., 7.5878e-03,\n",
      "            7.9153e-03, 6.3217e-03],\n",
      "           [3.3520e-03, 4.3062e-03, 1.3673e-02,  ..., 5.9648e-03,\n",
      "            6.2091e-03, 5.3399e-03]]],\n",
      "\n",
      "\n",
      "         [[[6.7639e-03, 4.4736e-03, 8.6455e-03,  ..., 1.8394e-03,\n",
      "            1.8950e-03, 1.7893e-03],\n",
      "           [7.7664e-03, 1.2922e-03, 4.1638e-04,  ..., 6.6657e-04,\n",
      "            6.5791e-04, 9.5765e-04],\n",
      "           [5.6180e-03, 1.1101e-04, 1.9918e-01,  ..., 5.3567e-05,\n",
      "            5.3636e-05, 7.0780e-05],\n",
      "           ...,\n",
      "           [1.6248e-02, 2.0503e-03, 3.3466e-03,  ..., 6.8853e-04,\n",
      "            7.5300e-04, 1.1696e-03],\n",
      "           [1.5603e-02, 1.9456e-03, 3.0356e-03,  ..., 7.1309e-04,\n",
      "            6.6986e-04, 9.1692e-04],\n",
      "           [1.7071e-02, 1.9545e-03, 2.7837e-03,  ..., 9.4286e-04,\n",
      "            7.9332e-04, 9.7953e-04]],\n",
      "\n",
      "          [[3.8917e-03, 4.0748e-03, 3.7426e-03,  ..., 3.3868e-03,\n",
      "            3.6630e-03, 4.5739e-03],\n",
      "           [1.7574e-03, 9.5804e-02, 1.0733e-02,  ..., 7.0432e-05,\n",
      "            7.0128e-05, 1.1196e-04],\n",
      "           [4.3188e-03, 3.0777e-02, 1.1230e-03,  ..., 1.6384e-04,\n",
      "            1.6660e-04, 2.1721e-04],\n",
      "           ...,\n",
      "           [2.7928e-03, 3.1131e-03, 5.7541e-03,  ..., 6.4706e-03,\n",
      "            6.7934e-03, 5.3485e-03],\n",
      "           [2.8478e-03, 2.6430e-03, 5.1897e-03,  ..., 6.5459e-03,\n",
      "            6.8284e-03, 5.4537e-03],\n",
      "           [2.8893e-03, 3.7118e-03, 6.1401e-03,  ..., 5.1415e-03,\n",
      "            5.3521e-03, 4.6028e-03]]],\n",
      "\n",
      "\n",
      "         [[[6.5091e-03, 4.3051e-03, 1.1406e-03,  ..., 1.7701e-03,\n",
      "            1.8236e-03, 1.7219e-03],\n",
      "           [6.3276e-03, 1.0528e-03, 5.3037e-05,  ..., 5.4308e-04,\n",
      "            5.3602e-04, 7.8023e-04],\n",
      "           [4.3199e-03, 3.3940e-04, 1.5619e-01,  ..., 4.4774e-04,\n",
      "            4.5293e-04, 4.9892e-04],\n",
      "           ...,\n",
      "           [1.6821e-02, 2.1226e-03, 2.0977e-03,  ..., 7.1281e-04,\n",
      "            7.7955e-04, 1.2109e-03],\n",
      "           [1.5544e-02, 1.9382e-03, 2.2076e-03,  ..., 7.1039e-04,\n",
      "            6.6733e-04, 9.1346e-04],\n",
      "           [1.6760e-02, 1.9189e-03, 1.9561e-03,  ..., 9.2571e-04,\n",
      "            7.7889e-04, 9.6171e-04]],\n",
      "\n",
      "          [[3.8579e-03, 4.0395e-03, 2.1001e-03,  ..., 3.3575e-03,\n",
      "            3.6313e-03, 4.5343e-03],\n",
      "           [1.4204e-03, 7.7433e-02, 1.0004e-02,  ..., 5.6926e-05,\n",
      "            5.6681e-05, 9.0490e-05],\n",
      "           [2.4331e-03, 1.7022e-02, 6.0910e-03,  ..., 3.7962e-04,\n",
      "            3.8752e-04, 5.4700e-04],\n",
      "           ...,\n",
      "           [1.9718e-03, 2.1979e-03, 1.1940e-03,  ..., 4.5684e-03,\n",
      "            4.7963e-03, 3.7762e-03],\n",
      "           [2.0139e-03, 1.8690e-03, 1.0539e-03,  ..., 4.6290e-03,\n",
      "            4.8288e-03, 3.8566e-03],\n",
      "           [1.9989e-03, 2.5680e-03, 1.2937e-03,  ..., 3.5571e-03,\n",
      "            3.7028e-03, 3.1844e-03]]]]], device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 384])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scores\n",
    "print(f'Raw Scores:\\n{raw_scores_val}\\n{raw_scores_val.shape}\\n')\n",
    "print(f'Attention Scores:\\n{attention_scores_val}\\n{attention_scores_val.shape}\\n')\n",
    "print(f'Attention Probabilities:\\n{attention_probs_val}\\n{attention_probs_val.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save in .csv for post-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Raw Scores\n",
    "\n",
    "# # Reshape considering the number of tokens\n",
    "# # Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "# vector = torch.reshape(raw_scores_val, (-1,384)).cpu().detach().numpy()\n",
    "# # print(vector.shape)\n",
    "\n",
    "# # To dataframe\n",
    "# df = pd.DataFrame(vector)\n",
    "\n",
    "# # Save file\n",
    "# file_name = f'raw_scores_val.csv'\n",
    "# df.to_csv('./data_BERTtiny/'+file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the tensor\n",
    "# # torch.save(q_vectors_val, './data_BERTtiny/q_vectors_val.pt')\n",
    "# # torch.save(k_vectors_val, './data_BERTtiny/k_vectors_val.pt')\n",
    "# # torch.save(v_vectors_val, './data_BERTtiny/v_vectors_val.pt')\n",
    "# torch.save(raw_scores_val, './data_BERTtiny/raw_scores_val.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FP8\n",
    "- Quantize the self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a deep copy of the model since the function overwrite it\n",
    "btiny_selfAtt_layer_to_e4m3 = copy.deepcopy(btiny_selfAtt_layer)\n",
    "\n",
    "# layers exempt from conversion\n",
    "list_exempt_layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4m3 : quantizing model weights..\n"
     ]
    }
   ],
   "source": [
    "# It needs the outputs even though it overwrites in model\n",
    "btiny_selfAtt_layer_e4m3, emulator = mpt_emu.quantize_model (model=btiny_selfAtt_layer_to_e4m3, dtype=\"E4M3\",\n",
    "                               list_exempt_layers=list_exempt_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 For Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-vectors FP8:\n",
      "tensor([[[[ 0.8326, -0.1829, -0.7930,  ..., -0.5384, -0.5726, -0.6541],\n",
      "          [-0.7521, -0.2831, -1.4780,  ..., -0.4503, -0.1489, -0.3682],\n",
      "          [ 1.1539, -0.7808, -1.6710,  ..., -1.1908,  0.1023, -0.6329],\n",
      "          ...,\n",
      "          [ 1.6531,  1.3767, -0.3815,  ..., -0.8930, -0.5320,  0.0570],\n",
      "          [ 0.8218, -0.3803, -1.1604,  ..., -0.8352, -0.2902, -1.0661],\n",
      "          [-0.2111,  0.9549, -0.8109,  ..., -0.6540, -0.0399, -0.7130]],\n",
      "\n",
      "         [[ 0.0172, -0.2868,  0.6104,  ...,  0.4206, -0.3497,  0.6391],\n",
      "          [ 0.5013,  0.7762, -0.3410,  ...,  0.2279, -0.1707,  1.7062],\n",
      "          [ 1.1041,  1.2306, -2.2410,  ..., -0.0417,  0.3370,  0.8399],\n",
      "          ...,\n",
      "          [ 0.5487, -0.3545, -1.0072,  ...,  1.1603, -0.4065, -0.1641],\n",
      "          [ 0.7350, -0.2206,  0.2703,  ...,  2.5761, -0.0196,  2.2816],\n",
      "          [ 0.6791, -0.1912, -0.2295,  ...,  1.0008,  0.7968, -0.0129]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 64])\n",
      "\n",
      "K-vectors FP8:\n",
      "tensor([[[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "          [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "          [-0.0512,  0.0419, -1.0920,  ..., -0.0390, -0.5411, -0.6485],\n",
      "          ...,\n",
      "          [ 0.3352,  2.1304,  1.1994,  ...,  1.0799, -1.3599, -0.8618],\n",
      "          [-0.7266,  0.3808,  0.1043,  ...,  2.4622,  0.5334, -0.8347],\n",
      "          [-0.9753, -0.0394, -0.5665,  ...,  3.0302,  2.0932, -2.4115]],\n",
      "\n",
      "         [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "          [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "          [ 1.5228,  0.1528,  2.0148,  ..., -0.3716, -0.2229,  2.0061],\n",
      "          ...,\n",
      "          [-0.1942, -0.7714,  1.6054,  ...,  1.1975,  0.0701, -0.0358],\n",
      "          [ 0.1212, -0.1468,  0.9729,  ...,  0.5338,  0.4832,  0.0100],\n",
      "          [ 1.4711, -1.2265, -0.9953,  ...,  1.1906,  0.2866, -1.3057]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 64])\n",
      "\n",
      "V-vectors FP8:\n",
      "tensor([[[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "          [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "          [-0.2474, -1.2533, -2.0411,  ...,  1.4574, -0.2963,  1.6104],\n",
      "          ...,\n",
      "          [-0.7763, -0.3005,  0.2252,  ...,  0.0706, -0.7510, -1.0666],\n",
      "          [-1.4258, -1.9751, -0.7473,  ..., -0.2781,  0.5019,  0.2808],\n",
      "          [-0.3022,  0.0059,  0.6162,  ...,  1.5070,  0.5330,  1.0486]],\n",
      "\n",
      "         [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "          [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "          [-1.7985,  0.8726,  0.6107,  ..., -0.7269, -2.1115, -0.4325],\n",
      "          ...,\n",
      "          [ 1.1480, -0.0144, -2.1326,  ..., -0.1702, -0.2170,  1.9020],\n",
      "          [ 1.2240,  2.2372,  1.4965,  ...,  4.5269,  2.9204,  4.8879],\n",
      "          [-0.9723,  0.9330, -0.0228,  ..., -0.1280, -1.5566,  2.4483]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V vectors and attention scores\n",
    "with torch.no_grad():\n",
    "    q_vectors_fp8, k_vectors_fp8, v_vectors_fp8, raw_scores_fp8, attention_scores_fp8, attention_probs_fp8 = btiny_selfAtt_layer_e4m3(input_embeddings_fp8)\n",
    "\n",
    "# Vectors\n",
    "print(f'Q-vectors FP8:\\n{q_vectors_fp8}\\n{q_vectors_fp8.shape}\\n')\n",
    "print(f'K-vectors FP8:\\n{k_vectors_fp8}\\n{k_vectors_fp8.shape}\\n')\n",
    "print(f'V-vectors FP8:\\n{v_vectors_fp8}\\n{v_vectors_fp8.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Scores FP8:\n",
      "tensor([[[[  5.4704,   2.1047,  -0.2783,  ...,  -1.0453,  -7.4099,  -6.0155],\n",
      "          [ 20.4987,   5.6594,  -6.8003,  ..., -13.0883,   9.7624,   4.0603],\n",
      "          [ 39.5795,   2.3215,  57.4665,  ..., -10.6918,   3.3006,   8.4812],\n",
      "          ...,\n",
      "          [ 10.9957,  -4.1071,  -6.5052,  ...,  38.0188,  -5.0706,   3.8447],\n",
      "          [ 38.4671,   4.8679,  13.4113,  ..., -18.1021, -11.9902,  -5.9363],\n",
      "          [ 39.1664,   9.1829,   9.4653,  ...,  -0.7888,  -6.6445,   8.5042]],\n",
      "\n",
      "         [[ 13.9854,  14.0787,  10.8763,  ...,  13.6859,   8.8449,  11.9485],\n",
      "          [  6.4277,  37.4564,  24.6036,  ...,   3.3215,  11.9237,   5.5173],\n",
      "          [ 13.2578,  27.3792,   1.5453,  ...,   1.3626,  -4.7803,   2.9694],\n",
      "          ...,\n",
      "          [ 12.3268,   7.6692,  13.7681,  ...,  23.4295,  28.7949,  33.0154],\n",
      "          [ 21.5005,  29.3222,  13.1489,  ...,  31.5183,  29.1969,  20.9772],\n",
      "          [ 17.5076,  11.2380,   0.5508,  ...,  19.6580,  17.5287,  17.6092]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 69])\n",
      "\n",
      "Attention Scores FP8:\n",
      "tensor([[[[ 0.6838,  0.2631, -0.0348,  ..., -0.1307, -0.9262, -0.7519],\n",
      "          [ 2.5623,  0.7074, -0.8500,  ..., -1.6360,  1.2203,  0.5075],\n",
      "          [ 4.9474,  0.2902,  7.1833,  ..., -1.3365,  0.4126,  1.0602],\n",
      "          ...,\n",
      "          [ 1.3745, -0.5134, -0.8131,  ...,  4.7523, -0.6338,  0.4806],\n",
      "          [ 4.8084,  0.6085,  1.6764,  ..., -2.2628, -1.4988, -0.7420],\n",
      "          [ 4.8958,  1.1479,  1.1832,  ..., -0.0986, -0.8306,  1.0630]],\n",
      "\n",
      "         [[ 1.7482,  1.7598,  1.3595,  ...,  1.7107,  1.1056,  1.4936],\n",
      "          [ 0.8035,  4.6820,  3.0755,  ...,  0.4152,  1.4905,  0.6897],\n",
      "          [ 1.6572,  3.4224,  0.1932,  ...,  0.1703, -0.5975,  0.3712],\n",
      "          ...,\n",
      "          [ 1.5409,  0.9587,  1.7210,  ...,  2.9287,  3.5994,  4.1269],\n",
      "          [ 2.6876,  3.6653,  1.6436,  ...,  3.9398,  3.6496,  2.6222],\n",
      "          [ 2.1885,  1.4047,  0.0688,  ...,  2.4573,  2.1911,  2.2011]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 2, 69, 69])\n",
      "\n",
      "Attention Probabilities FP8:\n",
      "tensor([[[[3.9304e-02, 2.5807e-02, 1.9159e-02,  ..., 1.7407e-02,\n",
      "           7.8562e-03, 9.3521e-03],\n",
      "          [2.7540e-02, 4.3091e-03, 9.0780e-04,  ..., 4.1365e-04,\n",
      "           7.1966e-03, 3.5284e-03],\n",
      "          [8.5574e-02, 8.1232e-04, 8.0053e-01,  ..., 1.5969e-04,\n",
      "           9.1807e-04, 1.7544e-03],\n",
      "          ...,\n",
      "          [1.5553e-02, 2.3547e-03, 1.7448e-03,  ..., 4.5585e-01,\n",
      "           2.0875e-03, 6.3624e-03],\n",
      "          [2.8016e-01, 4.2016e-03, 1.2224e-02,  ..., 2.3793e-04,\n",
      "           5.1079e-04, 1.0886e-03],\n",
      "          [3.2487e-01, 7.6560e-03, 7.9311e-03,  ..., 2.2013e-03,\n",
      "           1.0587e-03, 7.0332e-03]],\n",
      "\n",
      "         [[1.6620e-02, 1.6815e-02, 1.1268e-02,  ..., 1.6010e-02,\n",
      "           8.7413e-03, 1.2884e-02],\n",
      "          [2.1217e-03, 1.0260e-01, 2.0578e-02,  ..., 1.4390e-03,\n",
      "           4.2175e-03, 1.8935e-03],\n",
      "          [1.0217e-02, 5.9693e-02, 2.3631e-03,  ..., 2.3098e-03,\n",
      "           1.0717e-03, 2.8235e-03],\n",
      "          ...,\n",
      "          [5.7782e-03, 3.2281e-03, 6.9188e-03,  ..., 2.3148e-02,\n",
      "           4.5267e-02, 7.6719e-02],\n",
      "          [1.9887e-02, 5.2865e-02, 7.0013e-03,  ..., 6.9566e-02,\n",
      "           5.2044e-02, 1.8627e-02],\n",
      "          [4.0655e-02, 1.8567e-02, 4.8818e-03,  ..., 5.3192e-02,\n",
      "           4.0762e-02, 4.1174e-02]]]], device='cuda:0')\n",
      "torch.Size([1, 2, 69, 69])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scores\n",
    "print(f'Raw Scores FP8:\\n{raw_scores_fp8}\\n{raw_scores_fp8.shape}\\n')\n",
    "print(f'Attention Scores FP8:\\n{attention_scores_fp8}\\n{attention_scores_fp8.shape}\\n')\n",
    "print(f'Attention Probabilities FP8:\\n{attention_probs_fp8}\\n{attention_probs_fp8.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 For Whole Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectors and scores\n",
    "q_vectors_fp8_val = torch.zeros((2, batch_size, 2, max_length, 64), device=device)\n",
    "k_vectors_fp8_val = torch.zeros((2, batch_size, 2, max_length, 64), device=device)\n",
    "v_vectors_fp8_val = torch.zeros((2, batch_size, 2, max_length, 64), device=device)\n",
    "raw_scores_fp8_val = torch.zeros((2, batch_size, 2, max_length, max_length), device=device)\n",
    "attention_scores_fp8_val = torch.zeros((2, batch_size, 2, max_length, max_length), device=device)\n",
    "attention_probs_fp8_val = torch.zeros((2, batch_size, 2, max_length, max_length), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 56.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V vectors and attention scores\n",
    "# Retrive the number of batches\n",
    "n_batches,_,_,_ = input_embeddings_fp8_val.shape\n",
    "\n",
    "# Calculate per batch\n",
    "for n_batch in tqdm(range(n_batches)):\n",
    "    with torch.no_grad():\n",
    "        # batch_result,_,_,_,_,_ = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        # # Store the batch result\n",
    "        # q_vectors_val[n_batch,:,:,:,:] = batch_result\n",
    "\n",
    "        # _,batch_result,_,_,_,_ = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        # # Store the batch result\n",
    "        # k_vectors_val[n_batch,:,:,:,:] = batch_result\n",
    "\n",
    "        # _,_,batch_result,_,_,_ = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        # # Store the batch result\n",
    "        # v_vectors_val[n_batch,:,:,:,:] = batch_result\n",
    "\n",
    "        # _,_,_,batch_result,_,_ = btiny_selfAtt_layer(input_embeddings_val[n_batch,:,:,:])\n",
    "        # # Store the batch result\n",
    "        # raw_scores_val[n_batch,:,:,:,:] = batch_result\n",
    "\n",
    "        qv,kv,vv,rs,atts,attp = btiny_selfAtt_layer_e4m3(input_embeddings_fp8_val[n_batch,:,:,:])\n",
    "        \n",
    "        # Store the batch result\n",
    "        q_vectors_fp8_val[n_batch,:,:,:,:] = qv\n",
    "        k_vectors_fp8_val[n_batch,:,:,:,:] = kv\n",
    "        v_vectors_fp8_val[n_batch,:,:,:,:] = vv\n",
    "        raw_scores_fp8_val[n_batch,:,:,:,:] = rs\n",
    "        attention_scores_fp8_val[n_batch,:,:,:,:] = atts\n",
    "        attention_probs_fp8_val[n_batch,:,:,:,:] = attp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-vectors FP8:\n",
      "tensor([[[[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-6.8484e-01,  9.1428e-01, -4.1088e+00,  ..., -7.2621e-01,\n",
      "            -7.0894e-01, -1.6211e+00],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.1333e+00, -7.7388e-01, -9.6895e-01,  ...,  2.8187e-02,\n",
      "            -6.3311e-01,  1.7424e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-6.8484e-01,  9.1428e-01, -4.1088e+00,  ..., -7.2621e-01,\n",
      "            -7.0894e-01, -1.6211e+00],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.1333e+00, -7.7388e-01, -9.6895e-01,  ...,  2.8187e-02,\n",
      "            -6.3311e-01,  1.7424e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [ 5.2520e-01, -1.0913e+00, -3.8181e-01,  ...,  3.9039e-01,\n",
      "            -4.4513e-01,  1.7478e-01],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 7.8357e-01, -4.9346e-02,  1.4239e-01,  ...,  3.1905e-01,\n",
      "            -9.4833e-01,  6.8235e-01],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-1.6845e+00,  1.1473e+00, -1.1267e-02,  ..., -8.5166e-01,\n",
      "             8.0647e-01, -9.3124e-01],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 7.6932e-01,  2.4374e-01,  2.3807e+00,  ..., -4.8966e-01,\n",
      "            -5.0203e-01,  5.6552e-01],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [ 3.1452e-01, -5.3532e-01, -2.7272e+00,  ..., -6.6333e-04,\n",
      "            -3.4880e-02,  4.9922e-01],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.1684e+00,  2.6441e-01, -2.4724e-02,  ..., -3.2210e-01,\n",
      "             3.4918e-01,  2.7462e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-1.5307e+00, -8.4651e-01,  2.1024e-02,  ...,  1.2775e+00,\n",
      "            -1.3689e-01,  3.7973e-02],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.2125e+00, -4.2951e-01,  5.0386e-01,  ..., -1.8258e-01,\n",
      "            -1.0250e+00,  1.4958e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-3.5125e-01, -1.0372e-01, -2.2553e+00,  ..., -2.3399e-01,\n",
      "             3.3169e-01, -7.8264e-01],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.1861e+00, -5.7364e-01, -1.1567e-01,  ..., -3.9914e-01,\n",
      "            -9.8485e-02,  2.6777e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [ 1.3553e+00, -2.3752e+00, -1.5010e+00,  ...,  1.9823e+00,\n",
      "            -6.8990e-01,  5.2092e-02],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 7.2418e-01,  1.3524e+00, -9.5148e-01,  ..., -5.2175e-01,\n",
      "             1.5328e-01,  1.8068e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-5.1004e-02, -1.1118e+00, -3.0639e+00,  ...,  7.1377e-01,\n",
      "             6.2037e-01, -4.5918e-01],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.3941e-01, -1.5569e+00,  3.5765e-02,  ...,  1.7342e-01,\n",
      "            -3.0245e-01,  1.8228e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-1.1402e+00, -3.3028e-01, -1.4758e+00,  ...,  1.2013e-01,\n",
      "            -8.7051e-01, -1.3189e+00],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.0830e+00, -1.0809e+00,  9.4319e-01,  ...,  7.5901e-01,\n",
      "            -1.4723e-01,  1.3249e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-6.8484e-01,  9.1428e-01, -4.1088e+00,  ..., -7.2621e-01,\n",
      "            -7.0894e-01, -1.6211e+00],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.1333e+00, -7.7388e-01, -9.6895e-01,  ...,  2.8187e-02,\n",
      "            -6.3311e-01,  1.7424e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 8.3263e-01, -1.8286e-01, -7.9299e-01,  ..., -5.3844e-01,\n",
      "            -5.7256e-01, -6.5413e-01],\n",
      "           [-7.5207e-01, -2.8311e-01, -1.4780e+00,  ..., -4.5031e-01,\n",
      "            -1.4895e-01, -3.6824e-01],\n",
      "           [-1.5307e+00, -8.4651e-01,  2.1024e-02,  ...,  1.2775e+00,\n",
      "            -1.3689e-01,  3.7973e-02],\n",
      "           ...,\n",
      "           [ 4.8996e-01, -3.9205e-01,  4.3061e-01,  ..., -2.3419e-01,\n",
      "             9.4160e-02, -1.4814e+00],\n",
      "           [ 1.0576e-01, -5.7558e-01,  6.0043e-01,  ..., -6.0843e-01,\n",
      "            -3.1997e-02, -1.3869e+00],\n",
      "           [ 1.4674e-01, -5.6470e-01,  4.1802e-01,  ..., -9.2207e-01,\n",
      "            -6.9938e-02, -1.5766e+00]],\n",
      "\n",
      "          [[ 1.7173e-02, -2.8681e-01,  6.1045e-01,  ...,  4.2063e-01,\n",
      "            -3.4966e-01,  6.3909e-01],\n",
      "           [ 5.0135e-01,  7.7617e-01, -3.4099e-01,  ...,  2.2792e-01,\n",
      "            -1.7066e-01,  1.7062e+00],\n",
      "           [ 1.2125e+00, -4.2951e-01,  5.0386e-01,  ..., -1.8258e-01,\n",
      "            -1.0250e+00,  1.4958e+00],\n",
      "           ...,\n",
      "           [ 3.5188e-01, -5.8229e-01,  2.8356e-01,  ...,  1.9875e+00,\n",
      "             7.2104e-01,  3.9765e-01],\n",
      "           [ 4.0933e-01, -5.8426e-01,  2.7087e-01,  ...,  2.0078e+00,\n",
      "             7.5624e-01,  2.8804e-01],\n",
      "           [ 4.4740e-01, -8.1878e-01,  4.3041e-01,  ...,  1.8734e+00,\n",
      "             7.9023e-01,  2.0770e-01]]]]], device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 64])\n",
      "\n",
      "K-vectors FP8:\n",
      "tensor([[[[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-1.7090,  1.5599, -3.5560,  ...,  0.7018, -0.8656, -1.6183],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 1.4133,  0.9383,  0.7929,  ..., -0.1744,  0.8316,  0.6562],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-1.7090,  1.5599, -3.5560,  ...,  0.7018, -0.8656, -1.6183],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 1.4133,  0.9383,  0.7929,  ..., -0.1744,  0.8316,  0.6562],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-0.6882, -0.0352,  0.4633,  ...,  1.5828, -0.7571, -0.3872],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 0.4032, -0.4521,  0.6423,  ..., -0.8207,  1.3997,  0.3427],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-1.6442,  2.0100,  0.1269,  ...,  0.8950,  0.3163, -1.6725],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 1.1401, -0.6736, -1.6752,  ..., -0.9443,  1.3329, -0.1006],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-1.5628,  0.1994, -1.9173,  ...,  2.0843, -1.2829,  0.1556],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [-0.6147, -0.2187,  1.2387,  ..., -0.9007, -0.2358,  1.5612],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-2.5530,  0.2544,  1.4786,  ...,  2.3751, -1.4372,  0.3542],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [-1.5626, -0.5351,  0.7032,  ..., -0.6787,  0.5512,  1.8713],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-1.0940,  1.2271, -1.6707,  ...,  0.8075, -0.5771, -1.1723],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 1.2034,  1.0246,  0.1521,  ...,  0.3891,  1.6901,  0.1705],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [ 0.8491, -2.2074, -2.0254,  ...,  2.2734, -1.0750,  0.0054],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 1.0532,  0.1370,  1.9964,  ..., -0.3136, -1.6526,  1.7579],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-1.1652, -0.1447, -2.5911,  ...,  1.5984,  0.2768, -0.6485],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 1.0708,  0.8810,  1.2906,  ..., -1.1366,  1.2369, -0.0676],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-0.9418,  1.1311, -1.4702,  ...,  0.9704, -1.0813, -2.4166],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 1.8943,  0.0213, -1.6820,  ..., -0.0248,  2.1229, -0.0823],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-1.7090,  1.5599, -3.5560,  ...,  0.7018, -0.8656, -1.6183],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [ 1.4133,  0.9383,  0.7929,  ..., -0.1744,  0.8316,  0.6562],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1433,  0.2585, -0.7947,  ...,  0.3911,  0.1507, -1.3117],\n",
      "           [-1.2480,  0.8109,  0.2659,  ...,  0.4503,  0.1085, -0.6996],\n",
      "           [-2.5530,  0.2544,  1.4786,  ...,  2.3751, -1.4372,  0.3542],\n",
      "           ...,\n",
      "           [-0.0221, -0.0356,  0.5985,  ...,  0.3237, -1.0020, -0.3459],\n",
      "           [ 0.2094, -0.1687,  0.3327,  ...,  0.7084, -1.1341, -0.2278],\n",
      "           [ 0.3659, -0.1702,  0.2414,  ...,  1.1158, -0.8932, -0.5697]],\n",
      "\n",
      "          [[-1.2770,  0.9606, -0.0872,  ..., -1.0223,  0.6344,  0.4420],\n",
      "           [ 1.0374,  0.6978,  1.0192,  ..., -0.5730, -0.9819,  1.2736],\n",
      "           [-1.5626, -0.5351,  0.7032,  ..., -0.6787,  0.5512,  1.8713],\n",
      "           ...,\n",
      "           [-0.2027, -1.2009,  0.8369,  ...,  1.1968,  0.7673, -1.3732],\n",
      "           [-0.2907, -1.1747,  0.9276,  ...,  1.4359,  0.6327, -1.3665],\n",
      "           [-0.0910, -1.1121,  1.0503,  ...,  1.3004,  0.6695, -1.4907]]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 64])\n",
      "\n",
      "V-vectors FP8:\n",
      "tensor([[[[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [-0.0795,  0.2652, -1.8059,  ...,  1.1645, -0.4413,  1.6047],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [-3.1593,  0.2921,  3.9663,  ..., -2.2341, -1.9010, -0.1778],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [-0.0795,  0.2652, -1.8059,  ...,  1.1645, -0.4413,  1.6047],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [-3.1593,  0.2921,  3.9663,  ..., -2.2341, -1.9010, -0.1778],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [ 0.3437, -1.4058, -0.6544,  ...,  0.4079, -0.4159, -0.8805],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [ 1.6294,  3.9262, -1.0677,  ...,  1.1749, -1.0050,  0.9461],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [-0.9254, -1.7918, -0.4010,  ...,  0.6667, -2.3285, -0.4374],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [ 0.1587,  0.9371,  2.5590,  ...,  0.9304,  0.5138,  1.7560],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [ 1.2847, -1.1219, -0.8161,  ..., -0.6778, -3.0368,  0.1910],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [-0.0434, -1.0047,  3.3960,  ..., -0.0537,  0.2209,  1.5656],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [ 0.1492, -0.7915,  1.1622,  ...,  0.5099, -1.7363, -0.0517],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [ 1.1992,  0.4267,  0.3220,  ..., -0.0521, -0.4565,  0.9804],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [ 0.1129, -0.1200, -1.4868,  ..., -0.2749, -1.4788, -0.2279],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [ 0.0522,  0.3145,  1.7194,  ...,  0.2885, -0.2489,  3.1600],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [-1.3200, -0.6872, -1.4357,  ..., -0.1498, -1.0233,  0.9518],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [ 0.2673, -2.5083, -1.0502,  ..., -0.6096,  1.4364,  2.3012],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [ 0.7906, -1.2433, -0.8955,  ..., -0.0914, -1.1215, -0.6779],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [-1.1349,  0.9063, -0.3620,  ..., -0.1269, -1.6949,  1.8861],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [-1.0450, -0.0757, -0.4212,  ...,  0.6689, -1.5511,  0.4452],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [-2.0435,  0.5071, -0.1651,  ..., -2.7318, -1.4565,  0.2671],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [-0.0795,  0.2652, -1.8059,  ...,  1.1645, -0.4413,  1.6047],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [-3.1593,  0.2921,  3.9663,  ..., -2.2341, -1.9010, -0.1778],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]],\n",
      "\n",
      "\n",
      "         [[[-0.0878,  0.9143,  0.3815,  ...,  0.0330, -0.5779,  0.4635],\n",
      "           [-0.0615, -2.2658, -1.5016,  ..., -0.1112, -1.9064, -0.1334],\n",
      "           [ 0.1492, -0.7915,  1.1622,  ...,  0.5099, -1.7363, -0.0517],\n",
      "           ...,\n",
      "           [ 0.8300,  1.4155, -1.3516,  ...,  2.6412, -0.5503,  2.3924],\n",
      "           [ 0.8268,  1.4132, -1.3793,  ...,  2.6284, -0.5419,  2.3658],\n",
      "           [ 0.8683,  1.4331, -1.4943,  ...,  2.6676, -0.6668,  2.5465]],\n",
      "\n",
      "          [[-0.1079,  0.3491,  1.1540,  ..., -0.2946,  0.6479, -2.5605],\n",
      "           [ 0.2107, -0.2299,  3.4749,  ..., -1.4779, -0.5849,  0.8951],\n",
      "           [ 1.1992,  0.4267,  0.3220,  ..., -0.0521, -0.4565,  0.9804],\n",
      "           ...,\n",
      "           [-1.5456, -0.8022, -0.5195,  ..., -1.0507,  0.5643,  0.5990],\n",
      "           [-1.4269, -1.0393, -0.4181,  ..., -1.0165,  0.5938,  0.5455],\n",
      "           [-1.5143, -0.7618, -0.2835,  ..., -0.4690,  0.3044,  0.1259]]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectors\n",
    "print(f'Q-vectors FP8:\\n{q_vectors_fp8_val}\\n{q_vectors_fp8_val.shape}\\n')\n",
    "print(f'K-vectors FP8:\\n{k_vectors_fp8_val}\\n{k_vectors_fp8_val.shape}\\n')\n",
    "print(f'V-vectors FP8:\\n{v_vectors_fp8_val}\\n{v_vectors_fp8_val.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Scores FP8:\n",
      "tensor([[[[[ 5.4704e+00,  2.1047e+00,  7.7110e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -2.7429e+00,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 3.4525e+01,  3.8767e+00,  6.2659e+01,  ..., -2.7518e+00,\n",
      "            -2.4333e+00, -1.1611e-01],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  3.3533e+00,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  2.8957e+00,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00,  1.7191e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  1.3241e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.0706e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 9.4857e+00,  2.4244e+01, -1.9777e+00,  ..., -1.8475e+01,\n",
      "            -1.8438e+01, -1.6910e+01],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  4.1254e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  3.3659e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  5.2567e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00,  7.7110e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -2.7429e+00,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 3.4525e+01,  3.8767e+00,  6.2659e+01,  ..., -2.7518e+00,\n",
      "            -2.4333e+00, -1.1611e-01],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  3.3533e+00,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  2.8957e+00,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00,  1.7191e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  1.3241e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.0706e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 9.4857e+00,  2.4244e+01, -1.9777e+00,  ..., -1.8475e+01,\n",
      "            -1.8438e+01, -1.6910e+01],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  4.1254e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  3.3659e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  5.2567e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00,  2.8644e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -1.4851e+01,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [-4.4719e+00, -1.3459e+01,  5.4496e+01,  ...,  5.9491e+00,\n",
      "             5.9955e+00,  6.9372e+00],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  8.3344e+00,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  7.7309e+00,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00,  7.1568e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  1.3896e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.0644e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 1.2882e+01,  1.8816e+01,  8.2548e-01,  ..., -1.6681e+01,\n",
      "            -1.6663e+01, -1.3263e+01],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  1.0156e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  5.0001e-01,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  3.5707e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00,  3.4828e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00,  2.3366e+00,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 1.0880e+01, -6.5943e+00,  5.7117e+01,  ...,  7.9027e+00,\n",
      "             7.0669e+00,  7.3350e+00],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  1.0784e+01,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  1.0306e+01,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00,  9.8329e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  9.6896e+00,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.9357e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 7.2115e+00,  2.2395e+01,  1.0426e+01,  ..., -1.0335e+01,\n",
      "            -1.0337e+01, -7.5242e+00],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  5.4892e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  4.4510e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  6.2144e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00, -8.0610e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -1.6975e+01,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 6.3563e+00, -1.6744e+01,  5.8541e+01,  ..., -7.0297e+00,\n",
      "            -6.4746e+00, -6.3322e+00],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01, -9.6881e+00,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01, -8.9287e+00,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00, -9.9050e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  2.6678e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.8436e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 3.7544e+00,  2.0433e+01,  2.3959e+01,  ..., -7.7518e+00,\n",
      "            -7.8928e+00, -6.5294e+00],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  1.4042e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  5.9455e-03,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  2.7983e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00, -9.1798e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -1.8925e+01,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 1.3124e+01, -8.4111e+00,  4.1556e+01,  ..., -5.3771e+00,\n",
      "            -5.5681e+00, -4.9796e+00],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01, -3.2921e-01,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  8.2097e-01,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00, -6.2921e-01,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  8.9507e+00,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.1313e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 6.5802e+00,  2.1014e+01,  1.3885e+01,  ..., -8.6170e+00,\n",
      "            -8.2170e+00, -6.1697e+00],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01, -6.0931e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00, -7.0259e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00, -4.6526e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 5.4704e+00,  2.1047e+00,  1.1618e+01,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -1.9416e+00,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 2.0666e+01, -7.2088e+00,  3.8839e+01,  ..., -1.4950e+00,\n",
      "            -2.7148e+00, -1.2528e+00],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  6.9939e+00,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  5.8311e+00,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00,  4.2465e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  1.1480e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.8100e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 1.3211e+01,  2.8816e+01,  3.2913e+00,  ..., -6.6826e+00,\n",
      "            -6.2673e+00, -4.9930e+00],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  8.0295e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  7.0936e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  8.6014e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00, -2.1292e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -4.0401e+00,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 2.8310e+01, -8.5979e+00,  6.3113e+01,  ..., -2.4632e+00,\n",
      "            -1.8176e+00, -9.0740e-01],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  3.7605e+00,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  3.9892e+00,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00,  2.2281e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  4.3527e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.1578e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 1.7261e+01,  1.6500e+01, -3.0857e+00,  ..., -1.8128e+01,\n",
      "            -1.8587e+01, -1.6744e+01],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01, -3.9808e-01,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00, -1.8772e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  2.1380e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00,  5.3288e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -4.5783e+00,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 1.5691e+01, -4.1243e+00,  6.1239e+01,  ..., -4.5631e+00,\n",
      "            -5.1589e+00, -2.9666e+00],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  3.1610e-01,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01, -6.8352e-01,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00, -3.9735e-01,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  1.2256e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  1.7586e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 1.3831e+01,  1.8586e+01,  6.3118e+00,  ..., -1.6122e+01,\n",
      "            -1.6504e+01, -1.4562e+01],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  4.4054e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  3.6261e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  5.5463e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00,  9.5324e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00,  1.1192e+00,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 1.2832e+01, -4.1060e+00,  5.1071e+01,  ...,  4.0038e+00,\n",
      "             3.3362e+00,  4.3169e+00],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  9.0236e+00,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  7.8347e+00,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00,  7.9099e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  1.2890e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  1.9528e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 9.7867e+00,  1.5384e+01,  4.6774e+00,  ..., -7.6668e+00,\n",
      "            -7.8593e+00, -5.4327e+00],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  9.9828e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  9.4451e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  1.1338e+01,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00,  7.7110e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -2.7429e+00,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 3.4525e+01,  3.8767e+00,  6.2659e+01,  ..., -2.7518e+00,\n",
      "            -2.4333e+00, -1.1611e-01],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01,  3.3533e+00,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  2.8957e+00,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00,  1.7191e+00,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  1.3241e+01,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.0706e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 9.4857e+00,  2.4244e+01, -1.9777e+00,  ..., -1.8475e+01,\n",
      "            -1.8438e+01, -1.6910e+01],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01,  4.1254e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00,  3.3659e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00,  5.2567e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.4704e+00,  2.1047e+00, -9.1798e+00,  ..., -5.4162e+00,\n",
      "            -4.9139e+00, -5.4037e+00],\n",
      "           [ 2.0499e+01,  5.6594e+00, -1.8925e+01,  ...,  6.8070e-01,\n",
      "             9.2465e-01,  3.3259e+00],\n",
      "           [ 1.3124e+01, -8.4111e+00,  4.1556e+01,  ..., -5.3771e+00,\n",
      "            -5.5681e+00, -4.9796e+00],\n",
      "           ...,\n",
      "           [ 1.6426e+01, -5.0817e-01, -3.2921e-01,  ..., -9.1460e+00,\n",
      "            -8.4813e+00, -4.9068e+00],\n",
      "           [ 1.6123e+01, -5.7207e-01,  8.2097e-01,  ..., -8.8935e+00,\n",
      "            -9.3774e+00, -6.7674e+00],\n",
      "           [ 1.5900e+01, -1.4333e+00, -6.2921e-01,  ..., -7.0917e+00,\n",
      "            -8.3591e+00, -6.6131e+00]],\n",
      "\n",
      "          [[ 1.3985e+01,  1.4079e+01,  8.9507e+00,  ...,  1.2180e+01,\n",
      "             1.2865e+01,  1.3832e+01],\n",
      "           [ 6.4277e+00,  3.7456e+01,  2.1313e+01,  ..., -1.9784e+01,\n",
      "            -1.9762e+01, -1.6694e+01],\n",
      "           [ 6.5802e+00,  2.1014e+01,  1.3885e+01,  ..., -8.6170e+00,\n",
      "            -8.2170e+00, -6.1697e+00],\n",
      "           ...,\n",
      "           [-1.8313e+00, -8.6101e-01, -6.0931e+00,  ...,  4.9676e+00,\n",
      "             5.3839e+00,  3.5049e+00],\n",
      "           [-1.5611e+00, -1.9508e+00, -7.0259e+00,  ...,  5.1008e+00,\n",
      "             5.4414e+00,  3.6782e+00],\n",
      "           [-6.7410e-01,  1.0247e+00, -4.6526e+00,  ...,  4.1849e+00,\n",
      "             4.5508e+00,  3.2555e+00]]]]], device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 384])\n",
      "\n",
      "Attention Scores FP8:\n",
      "tensor([[[[[ 6.8380e-01,  2.6309e-01,  9.6387e-01,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -3.4286e-01,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 4.3156e+00,  4.8459e-01,  7.8324e+00,  ..., -3.4397e-01,\n",
      "            -3.0416e-01, -1.4513e-02],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  4.1916e-01,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  3.6197e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01,  2.1489e-01,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.6551e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.5882e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 1.1857e+00,  3.0305e+00, -2.4721e-01,  ..., -2.3094e+00,\n",
      "            -2.3048e+00, -2.1137e+00],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  5.1567e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  4.2074e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  6.5709e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01,  9.6387e-01,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -3.4286e-01,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 4.3156e+00,  4.8459e-01,  7.8324e+00,  ..., -3.4397e-01,\n",
      "            -3.0416e-01, -1.4513e-02],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  4.1916e-01,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  3.6197e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01,  2.1489e-01,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.6551e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.5882e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 1.1857e+00,  3.0305e+00, -2.4721e-01,  ..., -2.3094e+00,\n",
      "            -2.3048e+00, -2.1137e+00],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  5.1567e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  4.2074e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  6.5709e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01,  3.5805e-01,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -1.8564e+00,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [-5.5899e-01, -1.6824e+00,  6.8119e+00,  ...,  7.4364e-01,\n",
      "             7.4943e-01,  8.6715e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  1.0418e+00,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  9.6636e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01,  8.9460e-01,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.7370e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.5805e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 1.6103e+00,  2.3520e+00,  1.0319e-01,  ..., -2.0851e+00,\n",
      "            -2.0828e+00, -1.6579e+00],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  1.2695e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  6.2502e-02,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  4.4633e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01,  4.3535e-01,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01,  2.9208e-01,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 1.3600e+00, -8.2429e-01,  7.1396e+00,  ...,  9.8783e-01,\n",
      "             8.8336e-01,  9.1687e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  1.3479e+00,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  1.2882e+00,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01,  1.2291e+00,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.2112e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  3.6696e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 9.0144e-01,  2.7994e+00,  1.3032e+00,  ..., -1.2919e+00,\n",
      "            -1.2922e+00, -9.4052e-01],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  6.8614e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  5.5638e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  7.7680e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01, -1.0076e+00,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -2.1219e+00,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 7.9454e-01, -2.0930e+00,  7.3177e+00,  ..., -8.7871e-01,\n",
      "            -8.0933e-01, -7.9152e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02, -1.2110e+00,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02, -1.1161e+00,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01, -1.2381e+00,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  3.3347e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  3.5544e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 4.6930e-01,  2.5542e+00,  2.9949e+00,  ..., -9.6898e-01,\n",
      "            -9.8660e-01, -8.1618e-01],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  1.7553e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  7.4319e-04,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  3.4978e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01, -1.1475e+00,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -2.3656e+00,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 1.6405e+00, -1.0514e+00,  5.1945e+00,  ..., -6.7213e-01,\n",
      "            -6.9601e-01, -6.2245e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02, -4.1151e-02,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  1.0262e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01, -7.8651e-02,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.1188e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.6641e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 8.2252e-01,  2.6268e+00,  1.7357e+00,  ..., -1.0771e+00,\n",
      "            -1.0271e+00, -7.7121e-01],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01, -7.6164e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01, -8.7824e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01, -5.8157e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 6.8380e-01,  2.6309e-01,  1.4523e+00,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -2.4270e-01,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 2.5832e+00, -9.0110e-01,  4.8549e+00,  ..., -1.8688e-01,\n",
      "            -3.3935e-01, -1.5660e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  8.7424e-01,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  7.2889e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01,  5.3082e-01,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.4350e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  3.5125e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 1.6514e+00,  3.6020e+00,  4.1141e-01,  ..., -8.3532e-01,\n",
      "            -7.8341e-01, -6.2412e-01],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  1.0037e+00,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  8.8670e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  1.0752e+00,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01, -2.6615e-01,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -5.0501e-01,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 3.5388e+00, -1.0747e+00,  7.8891e+00,  ..., -3.0789e-01,\n",
      "            -2.2720e-01, -1.1342e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  4.7007e-01,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  4.9865e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01,  2.7851e-01,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  5.4409e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.6973e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 2.1576e+00,  2.0625e+00, -3.8572e-01,  ..., -2.2660e+00,\n",
      "            -2.3234e+00, -2.0930e+00],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01, -4.9760e-02,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01, -2.3465e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  2.6725e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01,  6.6611e-01,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -5.7229e-01,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 1.9613e+00, -5.1554e-01,  7.6549e+00,  ..., -5.7039e-01,\n",
      "            -6.4487e-01, -3.7082e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  3.9513e-02,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02, -8.5440e-02,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01, -4.9669e-02,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.5320e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.1983e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 1.7289e+00,  2.3232e+00,  7.8898e-01,  ..., -2.0152e+00,\n",
      "            -2.0630e+00, -1.8202e+00],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  5.5068e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  4.5327e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  6.9328e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01,  1.1916e+00,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01,  1.3989e-01,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 1.6040e+00, -5.1325e-01,  6.3839e+00,  ...,  5.0047e-01,\n",
      "             4.1702e-01,  5.3961e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  1.1279e+00,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  9.7934e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01,  9.8874e-01,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.6113e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.4410e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 1.2233e+00,  1.9230e+00,  5.8468e-01,  ..., -9.5835e-01,\n",
      "            -9.8242e-01, -6.7909e-01],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  1.2479e+00,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  1.1806e+00,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  1.4172e+00,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01,  9.6387e-01,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -3.4286e-01,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 4.3156e+00,  4.8459e-01,  7.8324e+00,  ..., -3.4397e-01,\n",
      "            -3.0416e-01, -1.4513e-02],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02,  4.1916e-01,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  3.6197e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01,  2.1489e-01,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.6551e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.5882e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 1.1857e+00,  3.0305e+00, -2.4721e-01,  ..., -2.3094e+00,\n",
      "            -2.3048e+00, -2.1137e+00],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01,  5.1567e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01,  4.2074e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01,  6.5709e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]],\n",
      "\n",
      "\n",
      "         [[[ 6.8380e-01,  2.6309e-01, -1.1475e+00,  ..., -6.7703e-01,\n",
      "            -6.1424e-01, -6.7546e-01],\n",
      "           [ 2.5623e+00,  7.0742e-01, -2.3656e+00,  ...,  8.5087e-02,\n",
      "             1.1558e-01,  4.1574e-01],\n",
      "           [ 1.6405e+00, -1.0514e+00,  5.1945e+00,  ..., -6.7213e-01,\n",
      "            -6.9601e-01, -6.2245e-01],\n",
      "           ...,\n",
      "           [ 2.0532e+00, -6.3522e-02, -4.1151e-02,  ..., -1.1433e+00,\n",
      "            -1.0602e+00, -6.1336e-01],\n",
      "           [ 2.0154e+00, -7.1508e-02,  1.0262e-01,  ..., -1.1117e+00,\n",
      "            -1.1722e+00, -8.4593e-01],\n",
      "           [ 1.9875e+00, -1.7916e-01, -7.8651e-02,  ..., -8.8646e-01,\n",
      "            -1.0449e+00, -8.2664e-01]],\n",
      "\n",
      "          [[ 1.7482e+00,  1.7598e+00,  1.1188e+00,  ...,  1.5224e+00,\n",
      "             1.6081e+00,  1.7290e+00],\n",
      "           [ 8.0346e-01,  4.6820e+00,  2.6641e+00,  ..., -2.4729e+00,\n",
      "            -2.4703e+00, -2.0867e+00],\n",
      "           [ 8.2252e-01,  2.6268e+00,  1.7357e+00,  ..., -1.0771e+00,\n",
      "            -1.0271e+00, -7.7121e-01],\n",
      "           ...,\n",
      "           [-2.2891e-01, -1.0763e-01, -7.6164e-01,  ...,  6.2095e-01,\n",
      "             6.7299e-01,  4.3811e-01],\n",
      "           [-1.9513e-01, -2.4385e-01, -8.7824e-01,  ...,  6.3760e-01,\n",
      "             6.8018e-01,  4.5978e-01],\n",
      "           [-8.4262e-02,  1.2809e-01, -5.8157e-01,  ...,  5.2312e-01,\n",
      "             5.6885e-01,  4.0694e-01]]]]], device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 384])\n",
      "\n",
      "Attention Probabilities FP8:\n",
      "tensor([[[[[6.5452e-03, 4.2974e-03, 8.6608e-03,  ..., 1.6785e-03,\n",
      "            1.7873e-03, 1.6811e-03],\n",
      "           [1.7120e-02, 2.6786e-03, 9.3709e-04,  ..., 1.4376e-03,\n",
      "            1.4821e-03, 2.0010e-03],\n",
      "           [7.5975e-03, 1.6477e-04, 2.5584e-01,  ..., 7.1950e-05,\n",
      "            7.4872e-05, 1.0003e-04],\n",
      "           ...,\n",
      "           [1.6389e-02, 1.9736e-03, 3.1981e-03,  ..., 6.7042e-04,\n",
      "            7.2851e-04, 1.1389e-03],\n",
      "           [1.5843e-02, 1.9657e-03, 3.0323e-03,  ..., 6.9466e-04,\n",
      "            6.5389e-04, 9.0614e-04],\n",
      "           [1.6654e-02, 1.9078e-03, 2.8293e-03,  ..., 9.4052e-04,\n",
      "            8.0272e-04, 9.9850e-04]],\n",
      "\n",
      "          [[3.8885e-03, 3.9341e-03, 3.5430e-03,  ..., 3.1028e-03,\n",
      "            3.3803e-03, 3.8145e-03],\n",
      "           [2.6794e-03, 1.2956e-01, 1.5964e-02,  ..., 1.0118e-04,\n",
      "            1.0146e-04, 1.4889e-04],\n",
      "           [5.9867e-03, 3.7877e-02, 1.4285e-03,  ..., 1.8167e-04,\n",
      "            1.8251e-04, 2.2093e-04],\n",
      "           ...,\n",
      "           [2.4235e-03, 2.7360e-03, 5.1028e-03,  ..., 5.6693e-03,\n",
      "            5.9721e-03, 4.7219e-03],\n",
      "           [2.4460e-03, 2.3297e-03, 4.5281e-03,  ..., 5.6247e-03,\n",
      "            5.8694e-03, 4.7084e-03],\n",
      "           [2.5718e-03, 3.1803e-03, 5.3977e-03,  ..., 4.7209e-03,\n",
      "            4.9418e-03, 4.2031e-03]]],\n",
      "\n",
      "\n",
      "         [[[7.4213e-03, 4.8727e-03, 9.8201e-03,  ..., 1.9032e-03,\n",
      "            2.0265e-03, 1.9062e-03],\n",
      "           [1.3617e-02, 2.1306e-03, 7.4538e-04,  ..., 1.1435e-03,\n",
      "            1.1789e-03, 1.5916e-03],\n",
      "           [9.8133e-03, 2.1282e-04, 3.3045e-01,  ..., 9.2933e-05,\n",
      "            9.6708e-05, 1.2920e-04],\n",
      "           ...,\n",
      "           [1.7628e-02, 2.1228e-03, 3.4398e-03,  ..., 7.2108e-04,\n",
      "            7.8356e-04, 1.2249e-03],\n",
      "           [1.6400e-02, 2.0347e-03, 3.1388e-03,  ..., 7.1905e-04,\n",
      "            6.7685e-04, 9.3795e-04],\n",
      "           [1.6815e-02, 1.9263e-03, 2.8566e-03,  ..., 9.4959e-04,\n",
      "            8.1047e-04, 1.0081e-03]],\n",
      "\n",
      "          [[4.0870e-03, 4.1349e-03, 3.7238e-03,  ..., 3.2612e-03,\n",
      "            3.5528e-03, 4.0092e-03],\n",
      "           [1.3427e-03, 6.4927e-02, 7.9998e-03,  ..., 5.0705e-05,\n",
      "            5.0841e-05, 7.4609e-05],\n",
      "           [4.6008e-03, 2.9109e-02, 1.0978e-03,  ..., 1.3961e-04,\n",
      "            1.4026e-04, 1.6979e-04],\n",
      "           ...,\n",
      "           [2.2048e-03, 2.4891e-03, 4.6424e-03,  ..., 5.1578e-03,\n",
      "            5.4333e-03, 4.2959e-03],\n",
      "           [2.2183e-03, 2.1128e-03, 4.1067e-03,  ..., 5.1012e-03,\n",
      "            5.3231e-03, 4.2702e-03],\n",
      "           [2.2659e-03, 2.8020e-03, 4.7557e-03,  ..., 4.1594e-03,\n",
      "            4.3541e-03, 3.7032e-03]]],\n",
      "\n",
      "\n",
      "         [[[6.7681e-03, 4.4438e-03, 4.8865e-03,  ..., 1.7357e-03,\n",
      "            1.8481e-03, 1.7384e-03],\n",
      "           [1.7041e-02, 2.6664e-03, 2.0534e-04,  ..., 1.4310e-03,\n",
      "            1.4754e-03, 1.9918e-03],\n",
      "           [2.8934e-04, 9.4083e-05, 4.5980e-01,  ..., 1.0645e-03,\n",
      "            1.0707e-03, 1.2044e-03],\n",
      "           ...,\n",
      "           [1.8549e-02, 2.2337e-03, 6.7462e-03,  ..., 7.5876e-04,\n",
      "            8.2450e-04, 1.2889e-03],\n",
      "           [1.7992e-02, 2.2323e-03, 6.3022e-03,  ..., 7.8887e-04,\n",
      "            7.4257e-04, 1.0290e-03],\n",
      "           [1.8663e-02, 2.1380e-03, 6.2565e-03,  ..., 1.0540e-03,\n",
      "            8.9955e-04, 1.1189e-03]],\n",
      "\n",
      "          [[4.1100e-03, 4.1582e-03, 4.0642e-03,  ..., 3.2795e-03,\n",
      "            3.5728e-03, 4.0318e-03],\n",
      "           [1.8756e-03, 9.0695e-02, 1.1089e-02,  ..., 7.0829e-05,\n",
      "            7.1019e-05, 1.0422e-04],\n",
      "           [5.9981e-03, 1.2593e-02, 1.3289e-03,  ..., 1.4898e-04,\n",
      "            1.4932e-04, 2.2839e-04],\n",
      "           ...,\n",
      "           [1.8412e-03, 2.0786e-03, 2.6281e-03,  ..., 4.3071e-03,\n",
      "            4.5372e-03, 3.5874e-03],\n",
      "           [1.8672e-03, 1.7784e-03, 2.4159e-03,  ..., 4.2938e-03,\n",
      "            4.4806e-03, 3.5943e-03],\n",
      "           [1.9010e-03, 2.3507e-03, 3.2316e-03,  ..., 3.4895e-03,\n",
      "            3.6528e-03, 3.1068e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[6.6688e-03, 4.3786e-03, 5.2017e-03,  ..., 1.7102e-03,\n",
      "            1.8210e-03, 1.7129e-03],\n",
      "           [1.8482e-02, 2.8917e-03, 1.9089e-03,  ..., 1.5520e-03,\n",
      "            1.6000e-03, 2.1602e-03],\n",
      "           [1.4417e-03, 1.6229e-04, 4.6660e-01,  ..., 9.9375e-04,\n",
      "            8.9517e-04, 9.2567e-04],\n",
      "           ...,\n",
      "           [1.5654e-02, 1.8851e-03, 7.7327e-03,  ..., 6.4035e-04,\n",
      "            6.9583e-04, 1.0878e-03],\n",
      "           [1.5202e-02, 1.8862e-03, 7.3467e-03,  ..., 6.6655e-04,\n",
      "            6.2743e-04, 8.6947e-04],\n",
      "           [1.5691e-02, 1.7976e-03, 7.3500e-03,  ..., 8.8615e-04,\n",
      "            7.5632e-04, 9.4078e-04]],\n",
      "\n",
      "          [[4.2556e-03, 4.3055e-03, 2.4875e-03,  ..., 3.3957e-03,\n",
      "            3.6994e-03, 4.1746e-03],\n",
      "           [2.6912e-03, 1.3014e-01, 4.7281e-02,  ..., 1.0163e-04,\n",
      "            1.0190e-04, 1.4954e-04],\n",
      "           [4.1571e-03, 2.7737e-02, 6.2128e-03,  ..., 4.6370e-04,\n",
      "            4.6358e-04, 6.5892e-04],\n",
      "           ...,\n",
      "           [2.7550e-03, 3.1102e-03, 6.8789e-03,  ..., 6.4447e-03,\n",
      "            6.7890e-03, 5.3678e-03],\n",
      "           [2.8100e-03, 2.6764e-03, 5.9578e-03,  ..., 6.4619e-03,\n",
      "            6.7429e-03, 5.4092e-03],\n",
      "           [2.8878e-03, 3.5711e-03, 6.8317e-03,  ..., 5.3010e-03,\n",
      "            5.5490e-03, 4.7196e-03]]],\n",
      "\n",
      "\n",
      "         [[[5.8420e-03, 3.8357e-03, 1.0764e-03,  ..., 1.4982e-03,\n",
      "            1.5952e-03, 1.5005e-03],\n",
      "           [1.3489e-02, 2.1107e-03, 1.2464e-04,  ..., 1.1328e-03,\n",
      "            1.1678e-03, 1.5767e-03],\n",
      "           [1.0899e-03, 6.0720e-05, 7.4191e-01,  ..., 2.0451e-04,\n",
      "            2.1920e-04, 2.2314e-04],\n",
      "           ...,\n",
      "           [1.6564e-02, 1.9947e-03, 6.3317e-04,  ..., 6.7756e-04,\n",
      "            7.3627e-04, 1.1510e-03],\n",
      "           [1.6042e-02, 1.9903e-03, 7.0027e-04,  ..., 7.0335e-04,\n",
      "            6.6207e-04, 9.1747e-04],\n",
      "           [1.6329e-02, 1.8706e-03, 6.4874e-04,  ..., 9.2214e-04,\n",
      "            7.8704e-04, 9.7900e-04]],\n",
      "\n",
      "          [[4.5054e-03, 4.5583e-03, 2.2017e-02,  ..., 3.5950e-03,\n",
      "            3.9166e-03, 4.4197e-03],\n",
      "           [1.5174e-03, 7.3374e-02, 2.3759e-02,  ..., 5.7302e-05,\n",
      "            5.7455e-05, 8.4315e-05],\n",
      "           [1.9471e-03, 1.5661e-02, 2.4335e-02,  ..., 4.6212e-04,\n",
      "            4.5405e-04, 5.3841e-04],\n",
      "           ...,\n",
      "           [1.7445e-03, 1.9694e-03, 2.6140e-03,  ..., 4.0809e-03,\n",
      "            4.2988e-03, 3.3990e-03],\n",
      "           [1.7848e-03, 1.7000e-03, 2.1710e-03,  ..., 4.1044e-03,\n",
      "            4.2829e-03, 3.4357e-03],\n",
      "           [1.8667e-03, 2.3083e-03, 2.8812e-03,  ..., 3.4265e-03,\n",
      "            3.5868e-03, 3.0506e-03]]],\n",
      "\n",
      "\n",
      "         [[[7.2529e-03, 4.7621e-03, 1.1620e-03,  ..., 1.8600e-03,\n",
      "            1.9805e-03, 1.8629e-03],\n",
      "           [7.2655e-03, 1.1368e-03, 5.2612e-05,  ..., 6.1012e-04,\n",
      "            6.2901e-04, 8.4921e-04],\n",
      "           [5.0037e-03, 3.3902e-04, 1.7490e-01,  ..., 4.9538e-04,\n",
      "            4.8369e-04, 5.2062e-04],\n",
      "           ...,\n",
      "           [1.6971e-02, 2.0437e-03, 2.0899e-03,  ..., 6.9422e-04,\n",
      "            7.5436e-04, 1.1793e-03],\n",
      "           [1.6063e-02, 1.9930e-03, 2.3721e-03,  ..., 7.0430e-04,\n",
      "            6.6296e-04, 9.1871e-04],\n",
      "           [1.6474e-02, 1.8873e-03, 2.0868e-03,  ..., 9.3038e-04,\n",
      "            7.9408e-04, 9.8774e-04]],\n",
      "\n",
      "          [[4.0757e-03, 4.1235e-03, 2.1721e-03,  ..., 3.2521e-03,\n",
      "            3.5430e-03, 3.9981e-03],\n",
      "           [1.2257e-03, 5.9269e-02, 7.8783e-03,  ..., 4.6287e-05,\n",
      "            4.6411e-05, 6.8107e-05],\n",
      "           [3.0732e-03, 1.8671e-02, 7.6589e-03,  ..., 4.5981e-04,\n",
      "            4.8339e-04, 6.2436e-04],\n",
      "           ...,\n",
      "           [2.1827e-03, 2.4642e-03, 1.2813e-03,  ..., 5.1061e-03,\n",
      "            5.3788e-03, 4.2529e-03],\n",
      "           [2.2116e-03, 2.1064e-03, 1.1169e-03,  ..., 5.0857e-03,\n",
      "            5.3069e-03, 4.2572e-03],\n",
      "           [2.2534e-03, 2.7865e-03, 1.3704e-03,  ..., 4.1363e-03,\n",
      "            4.3299e-03, 3.6827e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[7.0033e-03, 4.5982e-03, 1.5102e-02,  ..., 1.7960e-03,\n",
      "            1.9124e-03, 1.7988e-03],\n",
      "           [9.4039e-03, 1.4714e-03, 5.6898e-04,  ..., 7.8969e-04,\n",
      "            8.1414e-04, 1.0991e-03],\n",
      "           [1.6280e-02, 4.9936e-04, 1.5784e-01,  ..., 1.0200e-03,\n",
      "            8.7575e-04, 1.0514e-03],\n",
      "           ...,\n",
      "           [1.7264e-02, 2.0790e-03, 5.3103e-03,  ..., 7.0621e-04,\n",
      "            7.6740e-04, 1.1997e-03],\n",
      "           [1.6390e-02, 2.0335e-03, 4.5274e-03,  ..., 7.1861e-04,\n",
      "            6.7644e-04, 9.3738e-04],\n",
      "           [1.6651e-02, 1.9075e-03, 3.8797e-03,  ..., 9.4033e-04,\n",
      "            8.0257e-04, 9.9831e-04]],\n",
      "\n",
      "          [[3.9710e-03, 4.0176e-03, 2.9032e-03,  ..., 3.1686e-03,\n",
      "            3.4520e-03, 3.8954e-03],\n",
      "           [1.6967e-03, 8.2046e-02, 2.5477e-02,  ..., 6.4075e-05,\n",
      "            6.4246e-05, 9.4281e-05],\n",
      "           [5.7434e-03, 4.0394e-02, 1.6621e-03,  ..., 4.7777e-04,\n",
      "            5.0322e-04, 5.9012e-04],\n",
      "           ...,\n",
      "           [2.2570e-03, 2.5481e-03, 7.7420e-03,  ..., 5.2799e-03,\n",
      "            5.5620e-03, 4.3977e-03],\n",
      "           [2.3135e-03, 2.2035e-03, 6.8251e-03,  ..., 5.3202e-03,\n",
      "            5.5516e-03, 4.4535e-03],\n",
      "           [2.3983e-03, 2.9657e-03, 7.6461e-03,  ..., 4.4023e-03,\n",
      "            4.6084e-03, 3.9195e-03]]],\n",
      "\n",
      "\n",
      "         [[[7.3492e-03, 4.8253e-03, 2.8424e-03,  ..., 1.8847e-03,\n",
      "            2.0068e-03, 1.8877e-03],\n",
      "           [1.0478e-02, 1.6394e-03, 4.8768e-04,  ..., 8.7986e-04,\n",
      "            9.0711e-04, 1.2247e-03],\n",
      "           [1.0589e-02, 1.0501e-04, 8.2065e-01,  ..., 2.2608e-04,\n",
      "            2.4508e-04, 2.7461e-04],\n",
      "           ...,\n",
      "           [1.7605e-02, 2.1201e-03, 3.6148e-03,  ..., 7.2016e-04,\n",
      "            7.8255e-04, 1.2234e-03],\n",
      "           [1.6488e-02, 2.0457e-03, 3.6179e-03,  ..., 7.2293e-04,\n",
      "            6.8050e-04, 9.4302e-04],\n",
      "           [1.6781e-02, 1.9224e-03, 3.0381e-03,  ..., 9.4769e-04,\n",
      "            8.0884e-04, 1.0061e-03]],\n",
      "\n",
      "          [[3.3250e-03, 3.3641e-03, 1.3352e-01,  ..., 2.6532e-03,\n",
      "            2.8905e-03, 3.2618e-03],\n",
      "           [1.8503e-03, 8.9472e-02, 1.2295e-02,  ..., 6.9874e-05,\n",
      "            7.0061e-05, 1.0281e-04],\n",
      "           [1.5830e-02, 1.4393e-02, 1.2442e-03,  ..., 1.8981e-04,\n",
      "            1.7922e-04, 2.2565e-04],\n",
      "           ...,\n",
      "           [2.6896e-03, 3.0364e-03, 3.2173e-03,  ..., 6.2918e-03,\n",
      "            6.6279e-03, 5.2404e-03],\n",
      "           [2.7889e-03, 2.6563e-03, 2.6809e-03,  ..., 6.4135e-03,\n",
      "            6.6924e-03, 5.3686e-03],\n",
      "           [2.8185e-03, 3.4853e-03, 4.0057e-03,  ..., 5.1737e-03,\n",
      "            5.4158e-03, 4.6062e-03]]],\n",
      "\n",
      "\n",
      "         [[[7.0235e-03, 4.6115e-03, 6.9003e-03,  ..., 1.8012e-03,\n",
      "            1.9179e-03, 1.8040e-03],\n",
      "           [1.8056e-02, 2.8252e-03, 7.8573e-04,  ..., 1.5162e-03,\n",
      "            1.5632e-03, 2.1104e-03],\n",
      "           [9.2501e-04, 7.7706e-05, 2.7469e-01,  ..., 7.3558e-05,\n",
      "            6.8279e-05, 8.9806e-05],\n",
      "           ...,\n",
      "           [1.7272e-02, 2.0800e-03, 2.3058e-03,  ..., 7.0656e-04,\n",
      "            7.6777e-04, 1.2003e-03],\n",
      "           [1.6733e-02, 2.0761e-03, 2.0474e-03,  ..., 7.3368e-04,\n",
      "            6.9062e-04, 9.5704e-04],\n",
      "           [1.7515e-02, 2.0065e-03, 2.2840e-03,  ..., 9.8917e-04,\n",
      "            8.4425e-04, 1.0502e-03]],\n",
      "\n",
      "          [[4.1475e-03, 4.1961e-03, 3.3414e-03,  ..., 3.3094e-03,\n",
      "            3.6054e-03, 4.0686e-03],\n",
      "           [2.6855e-03, 1.2986e-01, 1.0834e-02,  ..., 1.0141e-04,\n",
      "            1.0168e-04, 1.4922e-04],\n",
      "           [6.7532e-03, 1.2235e-02, 2.6382e-03,  ..., 1.5976e-04,\n",
      "            1.5231e-04, 1.9416e-04],\n",
      "           ...,\n",
      "           [2.6995e-03, 3.0476e-03, 5.8865e-03,  ..., 6.3150e-03,\n",
      "            6.6523e-03, 5.2598e-03],\n",
      "           [2.7519e-03, 2.6210e-03, 5.2629e-03,  ..., 6.3283e-03,\n",
      "            6.6035e-03, 5.2973e-03],\n",
      "           [2.8200e-03, 3.4871e-03, 6.1366e-03,  ..., 5.1764e-03,\n",
      "            5.4186e-03, 4.6087e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[7.1971e-03, 4.7254e-03, 1.1958e-02,  ..., 1.8457e-03,\n",
      "            1.9653e-03, 1.8486e-03],\n",
      "           [1.8119e-02, 2.8350e-03, 1.6072e-03,  ..., 1.5215e-03,\n",
      "            1.5686e-03, 2.1177e-03],\n",
      "           [2.5016e-03, 3.0111e-04, 2.9793e-01,  ..., 8.2981e-04,\n",
      "            7.6337e-04, 8.6293e-04],\n",
      "           ...,\n",
      "           [1.6821e-02, 2.0257e-03, 6.6683e-03,  ..., 6.8809e-04,\n",
      "            7.4771e-04, 1.1689e-03],\n",
      "           [1.6052e-02, 1.9915e-03, 5.6960e-03,  ..., 7.0379e-04,\n",
      "            6.6248e-04, 9.1804e-04],\n",
      "           [1.6529e-02, 1.8936e-03, 6.0882e-03,  ..., 9.3348e-04,\n",
      "            7.9671e-04, 9.9103e-04]],\n",
      "\n",
      "          [[4.0243e-03, 4.0716e-03, 3.5095e-03,  ..., 3.2112e-03,\n",
      "            3.4984e-03, 3.9478e-03],\n",
      "           [2.3545e-03, 1.1385e-01, 1.2108e-02,  ..., 8.8915e-05,\n",
      "            8.9153e-05, 1.3083e-04],\n",
      "           [6.1908e-03, 1.2462e-02, 3.2687e-03,  ..., 6.9864e-04,\n",
      "            6.8203e-04, 9.2371e-04],\n",
      "           ...,\n",
      "           [3.1481e-03, 3.5540e-03, 1.3785e-02,  ..., 7.3644e-03,\n",
      "            7.7577e-03, 6.1338e-03],\n",
      "           [3.1790e-03, 3.0279e-03, 1.2583e-02,  ..., 7.3105e-03,\n",
      "            7.6285e-03, 6.1196e-03],\n",
      "           [3.2735e-03, 4.0479e-03, 1.4693e-02,  ..., 6.0089e-03,\n",
      "            6.2901e-03, 5.3498e-03]]],\n",
      "\n",
      "\n",
      "         [[[6.9901e-03, 4.5895e-03, 9.2494e-03,  ..., 1.7926e-03,\n",
      "            1.9087e-03, 1.7954e-03],\n",
      "           [8.0593e-03, 1.2610e-03, 4.4115e-04,  ..., 6.7678e-04,\n",
      "            6.9773e-04, 9.4199e-04],\n",
      "           [5.4038e-03, 1.1719e-04, 1.8197e-01,  ..., 5.1175e-05,\n",
      "            5.3253e-05, 7.1144e-05],\n",
      "           ...,\n",
      "           [1.6524e-02, 1.9899e-03, 3.2245e-03,  ..., 6.7596e-04,\n",
      "            7.3452e-04, 1.1483e-03],\n",
      "           [1.5702e-02, 1.9482e-03, 3.0053e-03,  ..., 6.8848e-04,\n",
      "            6.4807e-04, 8.9808e-04],\n",
      "           [1.6239e-02, 1.8604e-03, 2.7589e-03,  ..., 9.1711e-04,\n",
      "            7.8275e-04, 9.7365e-04]],\n",
      "\n",
      "          [[4.1206e-03, 4.1689e-03, 3.7544e-03,  ..., 3.2879e-03,\n",
      "            3.5820e-03, 4.0421e-03],\n",
      "           [1.8332e-03, 8.8647e-02, 1.0922e-02,  ..., 6.9230e-05,\n",
      "            6.9415e-05, 1.0187e-04],\n",
      "           [4.9041e-03, 3.1028e-02, 1.1702e-03,  ..., 1.4882e-04,\n",
      "            1.4951e-04, 1.8098e-04],\n",
      "           ...,\n",
      "           [2.7059e-03, 3.0549e-03, 5.6976e-03,  ..., 6.3301e-03,\n",
      "            6.6682e-03, 5.2723e-03],\n",
      "           [2.7496e-03, 2.6189e-03, 5.0903e-03,  ..., 6.3230e-03,\n",
      "            6.5981e-03, 5.2930e-03],\n",
      "           [2.8322e-03, 3.5022e-03, 5.9441e-03,  ..., 5.1988e-03,\n",
      "            5.4421e-03, 4.6286e-03]]],\n",
      "\n",
      "\n",
      "         [[[6.7225e-03, 4.4139e-03, 1.0770e-03,  ..., 1.7240e-03,\n",
      "            1.8357e-03, 1.7267e-03],\n",
      "           [6.6201e-03, 1.0358e-03, 4.7938e-05,  ..., 5.5592e-04,\n",
      "            5.7313e-04, 7.7377e-04],\n",
      "           [4.5679e-03, 3.0950e-04, 1.5967e-01,  ..., 4.5225e-04,\n",
      "            4.4157e-04, 4.7528e-04],\n",
      "           ...,\n",
      "           [1.7100e-02, 2.0592e-03, 2.1058e-03,  ..., 6.9949e-04,\n",
      "            7.6010e-04, 1.1883e-03],\n",
      "           [1.5601e-02, 1.9356e-03, 2.3038e-03,  ..., 6.8403e-04,\n",
      "            6.4388e-04, 8.9227e-04],\n",
      "           [1.6122e-02, 1.8470e-03, 2.0423e-03,  ..., 9.1050e-04,\n",
      "            7.7710e-04, 9.6664e-04]],\n",
      "\n",
      "          [[4.1100e-03, 4.1582e-03, 2.1904e-03,  ..., 3.2795e-03,\n",
      "            3.5729e-03, 4.0318e-03],\n",
      "           [1.5540e-03, 7.5144e-02, 9.9884e-03,  ..., 5.8684e-05,\n",
      "            5.8841e-05, 8.6349e-05],\n",
      "           [2.6909e-03, 1.6348e-02, 6.7061e-03,  ..., 4.0261e-04,\n",
      "            4.2325e-04, 5.4669e-04],\n",
      "           ...,\n",
      "           [1.8904e-03, 2.1341e-03, 1.1097e-03,  ..., 4.4222e-03,\n",
      "            4.6584e-03, 3.6833e-03],\n",
      "           [1.9436e-03, 1.8512e-03, 9.8160e-04,  ..., 4.4694e-03,\n",
      "            4.6638e-03, 3.7413e-03],\n",
      "           [1.9353e-03, 2.3931e-03, 1.1770e-03,  ..., 3.5524e-03,\n",
      "            3.7186e-03, 3.1628e-03]]]]], device='cuda:0')\n",
      "torch.Size([2, 12, 2, 384, 384])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scores\n",
    "print(f'Raw Scores FP8:\\n{raw_scores_fp8_val}\\n{raw_scores_fp8_val.shape}\\n')\n",
    "print(f'Attention Scores FP8:\\n{attention_scores_fp8_val}\\n{attention_scores_fp8_val.shape}\\n')\n",
    "print(f'Attention Probabilities FP8:\\n{attention_probs_fp8_val}\\n{attention_probs_fp8_val.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data\n",
    "Files generated for each attention head:  \n",
    "1. Q, K, and V vectors in FP8.\n",
    "    - 69 tokens embedded using 64 dimensions.\n",
    "    - Matrix of size (69x64).\n",
    "2. Raw scores in FP8: --> $q^{(i)} \\cdot k^{(i)T}$\n",
    "    - Row number --> correspoding q-vector.\n",
    "    - Column number --> corresponding k-vector.\n",
    "    - Matrix of size (69x69).\n",
    "3. Attention scores in FP8: --> $q^{(i)} \\cdot k^{(i)T} / \\sqrt{64} $\n",
    "    - Row number --> correspoding q-vector.\n",
    "    - Column number --> corresponding k-vector.\n",
    "    - Matrix of size (69x69). \n",
    "4. Attention probabilities in FP8 --> after softmax.\n",
    "    - Row number --> correspoding q-vector.\n",
    "    - Column number --> corresponding k-vector.\n",
    "    - Matrix of size (69x69)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decimal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Heads: 2\n"
     ]
    }
   ],
   "source": [
    "# Q-vectors\n",
    "_,n_att_head,_,_ = q_vectors_fp8.shape\n",
    "print(f'Attention Heads: {n_att_head}')\n",
    "\n",
    "# Save the Q-vectors\n",
    "for i in range(n_att_head):\n",
    "    # Take the corresponding attention head and remove this dimension\n",
    "    # Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "    vector = torch.squeeze(q_vectors_fp8[:,i,:,:]).cpu().detach().numpy()\n",
    "    # print(vector.shape)\n",
    "\n",
    "    # To dataframe\n",
    "    df = pd.DataFrame(vector)\n",
    "\n",
    "    # Save file\n",
    "    file_name = f'qv_fp8_attH{i}.csv'\n",
    "    df.to_csv('./results_BERTtiny/'+file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Heads: 2\n"
     ]
    }
   ],
   "source": [
    "# K-vectors\n",
    "_,n_att_head,_,_ = k_vectors_fp8.shape\n",
    "print(f'Attention Heads: {n_att_head}')\n",
    "\n",
    "# Save the K-vectors\n",
    "for i in range(n_att_head):\n",
    "    # Take the corresponding attention head and remove this dimension\n",
    "    # Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "    vector = torch.squeeze(k_vectors_fp8[:,i,:,:]).cpu().detach().numpy()\n",
    "    # print(vector.shape)\n",
    "\n",
    "    # To dataframe\n",
    "    df = pd.DataFrame(vector)\n",
    "\n",
    "    # Save file\n",
    "    file_name = f'kv_fp8_attH{i}.csv'\n",
    "    df.to_csv('./results_BERTtiny/'+file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Heads: 2\n"
     ]
    }
   ],
   "source": [
    "# V-vectors\n",
    "_,n_att_head,_,_ = v_vectors_fp8.shape\n",
    "print(f'Attention Heads: {n_att_head}')\n",
    "\n",
    "# Save the K-vectors\n",
    "for i in range(n_att_head):\n",
    "    # Take the corresponding attention head and remove this dimension\n",
    "    # Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "    vector = torch.squeeze(v_vectors_fp8[:,i,:,:]).cpu().detach().numpy()\n",
    "    # print(vector.shape)\n",
    "\n",
    "    # To dataframe\n",
    "    df = pd.DataFrame(vector)\n",
    "\n",
    "    # Save file\n",
    "    file_name = f'vv_fp8_attH{i}.csv'\n",
    "    df.to_csv('./results_BERTtiny/'+file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Heads: 2\n"
     ]
    }
   ],
   "source": [
    "# Raw Scores\n",
    "_,n_att_head,_,_ = raw_scores_fp8.shape\n",
    "print(f'Attention Heads: {n_att_head}')\n",
    "\n",
    "# Save Scores\n",
    "for i in range(n_att_head):\n",
    "    # Take the corresponding attention head and remove this dimension\n",
    "    # Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "    vector = torch.squeeze(raw_scores_fp8[:,i,:,:]).cpu().detach().numpy()\n",
    "    # print(vector.shape)\n",
    "\n",
    "    # To dataframe\n",
    "    df = pd.DataFrame(vector)\n",
    "\n",
    "    # Save file\n",
    "    file_name = f'raw_score_fp8_attH{i}.csv'\n",
    "    df.to_csv('./results_BERTtiny/'+file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Heads: 2\n"
     ]
    }
   ],
   "source": [
    "# Attention Scores\n",
    "_,n_att_head,_,_ = attention_scores_fp8.shape\n",
    "print(f'Attention Heads: {n_att_head}')\n",
    "\n",
    "# Save Scores\n",
    "for i in range(n_att_head):\n",
    "    # Take the corresponding attention head and remove this dimension\n",
    "    # Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "    vector = torch.squeeze(attention_scores_fp8[:,i,:,:]).cpu().detach().numpy()\n",
    "    # print(vector.shape)\n",
    "\n",
    "    # To dataframe\n",
    "    df = pd.DataFrame(vector)\n",
    "\n",
    "    # Save file\n",
    "    file_name = f'att_score_fp8_attH{i}.csv'\n",
    "    df.to_csv('./results_BERTtiny/'+file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Heads: 2\n"
     ]
    }
   ],
   "source": [
    "# Attention Probabilities\n",
    "_,n_att_head,_,_ = attention_probs_fp8.shape\n",
    "print(f'Attention Heads: {n_att_head}')\n",
    "\n",
    "# Save Scores\n",
    "for i in range(n_att_head):\n",
    "    # Take the corresponding attention head and remove this dimension\n",
    "    # Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "    vector = torch.squeeze(attention_probs_fp8[:,i,:,:]).cpu().detach().numpy()\n",
    "    # print(vector.shape)\n",
    "\n",
    "    # To dataframe\n",
    "    df = pd.DataFrame(vector)\n",
    "\n",
    "    # Save file\n",
    "    file_name = f'att_prob_fp8_attH{i}.csv'\n",
    "    df.to_csv('./results_BERTtiny/'+file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "- Compare frequency distribution fpr FP32 and FP8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Q-vectors and K-vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors to numpy arrays\n",
    "# Flatten the tensor\n",
    "# Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "qvector_fp32 = torch.flatten(q_vectors).cpu().detach().numpy()\n",
    "qvector_fp8 = torch.flatten(q_vectors_fp8).cpu().detach().numpy()\n",
    "\n",
    "kvector_fp32 = torch.flatten(k_vectors).cpu().detach().numpy()\n",
    "kvector_fp8 = torch.flatten(k_vectors_fp8).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = 0.014452, Std = 1.042295\n",
      "FP8: Mean = 0.016712, Std = 1.041033\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q-vectors\n",
    "print(f'FP32: Mean = {qvector_fp32.mean():.6f}, Std = {qvector_fp32.std():.6f}')\n",
    "print(f'FP8: Mean = {qvector_fp8.mean():.6f}, Std = {qvector_fp8.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "n_bins = 50\n",
    "\n",
    "plt.title(\"Q-vectors for Single Sample\")\n",
    "plt.hist([qvector_fp32, qvector_fp8], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = -0.023228, Std = 1.016564\n",
      "FP8: Mean = -0.024590, Std = 1.016063\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# K-vectors\n",
    "print(f'FP32: Mean = {kvector_fp32.mean():.6f}, Std = {kvector_fp32.std():.6f}')\n",
    "print(f'FP8: Mean = {kvector_fp8.mean():.6f}, Std = {kvector_fp8.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "n_bins = 50\n",
    "\n",
    "plt.title(\"K-vectors for Single Sample\")\n",
    "plt.hist([kvector_fp32, kvector_fp8], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Raw Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors to numpy arrays\n",
    "# Flatten the tensor\n",
    "# Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "rscore_fp32 = torch.flatten(raw_scores).cpu().detach().numpy()\n",
    "rscore_fp8 = torch.flatten(raw_scores_fp8).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = 8.345048, Std = 12.196220\n",
      "FP8: Mean = 8.228134, Std = 12.143971\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Raw Scores\n",
    "print(f'FP32: Mean = {rscore_fp32.mean():.6f}, Std = {rscore_fp32.std():.6f}')\n",
    "print(f'FP8: Mean = {rscore_fp8.mean():.6f}, Std = {rscore_fp8.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "n_bins = 50\n",
    "\n",
    "plt.title(\"Raw Scores for Single Sample\")\n",
    "plt.hist([rscore_fp32, rscore_fp8], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Attention Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors to numpy arrays\n",
    "# Flatten the tensor\n",
    "# Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "attscore_fp32 = torch.flatten(attention_scores).cpu().detach().numpy()\n",
    "attscore_fp8 = torch.flatten(attention_scores_fp8).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = 1.043131, Std = 1.524528\n",
      "FP8: Mean = 1.028517, Std = 1.517996\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attention Scores\n",
    "print(f'FP32: Mean = {attscore_fp32.mean():.6f}, Std = {attscore_fp32.std():.6f}')\n",
    "print(f'FP8: Mean = {attscore_fp8.mean():.6f}, Std = {attscore_fp8.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "n_bins = 50\n",
    "\n",
    "plt.title(\"Attention Scores for Single Sample\")\n",
    "plt.hist([attscore_fp32, attscore_fp8], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Attention Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors to numpy arrays\n",
    "# Flatten the tensor\n",
    "# Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "attprob_fp32 = torch.flatten(attention_probs).cpu().detach().numpy()\n",
    "attprob_fp8 = torch.flatten(attention_probs_fp8).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = 0.014493, Std = 0.045385\n",
      "FP8: Mean = 0.014493, Std = 0.045495\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attention Probabilities\n",
    "print(f'FP32: Mean = {attprob_fp32.mean():.6f}, Std = {attprob_fp32.std():.6f}')\n",
    "print(f'FP8: Mean = {attprob_fp8.mean():.6f}, Std = {attprob_fp8.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(6, 4))\n",
    "n_bins = 50\n",
    "\n",
    "# Get the axis\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, 0.2])\n",
    "# ax.set_ylim([ymin, ymax])\n",
    "\n",
    "plt.title(\"Attention Probabilities for Single Sample\")\n",
    "plt.hist([attprob_fp32, attprob_fp8], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Whole Dataset\n",
    "- Take 24 random samples from validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Q-vectors and K-vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1179648,)\n",
      "(1179648,)\n",
      "(1179648,)\n",
      "(1179648,)\n"
     ]
    }
   ],
   "source": [
    "# Vectors to numpy arrays\n",
    "# Flatten the tensor\n",
    "# Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "qvector_fp32_val = torch.flatten(q_vectors_val).cpu().detach().numpy()\n",
    "print(qvector_fp32_val.shape)\n",
    "qvector_fp8_val = torch.flatten(q_vectors_fp8_val).cpu().detach().numpy()\n",
    "print(qvector_fp8_val.shape)\n",
    "\n",
    "kvector_fp32_val = torch.flatten(k_vectors_val).cpu().detach().numpy()\n",
    "print(kvector_fp32_val.shape)\n",
    "kvector_fp8_val = torch.flatten(k_vectors_fp8_val).cpu().detach().numpy()\n",
    "print(kvector_fp8_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = 0.026096, Std = 0.887899\n",
      "FP8: Mean = 0.027543, Std = 0.888021\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q-vectors\n",
    "print(f'FP32: Mean = {qvector_fp32_val.mean():.6f}, Std = {qvector_fp32_val.std():.6f}')\n",
    "print(f'FP8: Mean = {qvector_fp8_val.mean():.6f}, Std = {qvector_fp8_val.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "n_bins = 50\n",
    "\n",
    "plt.title(\"Q-vectors for Whole Dataset\")\n",
    "plt.hist([qvector_fp32_val, qvector_fp8_val], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = -0.019406, Std = 0.900693\n",
      "FP8: Mean = -0.018740, Std = 0.901276\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# K-vectors\n",
    "print(f'FP32: Mean = {kvector_fp32_val.mean():.6f}, Std = {kvector_fp32_val.std():.6f}')\n",
    "print(f'FP8: Mean = {kvector_fp8_val.mean():.6f}, Std = {kvector_fp8_val.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "n_bins = 50\n",
    "\n",
    "plt.title(\"K-vectors for Whole Dataset\")\n",
    "plt.hist([kvector_fp32_val, kvector_fp8_val], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Raw Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7077888,)\n",
      "(7077888,)\n"
     ]
    }
   ],
   "source": [
    "# Vectors to numpy arrays\n",
    "# Flatten the tensor\n",
    "# Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "rscore_fp32_val = torch.flatten(raw_scores_val).cpu().detach().numpy()\n",
    "print(rscore_fp32_val.shape)\n",
    "rscore_fp8_val = torch.flatten(raw_scores_fp8_val).cpu().detach().numpy()\n",
    "print(rscore_fp8_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = -2.352339, Std = 9.982971\n",
      "FP8: Mean = -2.417389, Std = 10.036436\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Raw Scores\n",
    "print(f'FP32: Mean = {rscore_fp32_val.mean():.6f}, Std = {rscore_fp32_val.std():.6f}')\n",
    "print(f'FP8: Mean = {rscore_fp8_val.mean():.6f}, Std = {rscore_fp8_val.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "n_bins = 50\n",
    "\n",
    "plt.title(\"Raw Scores for Whole Dataset\")\n",
    "plt.hist([rscore_fp32_val, rscore_fp8_val], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Attention Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7077888,)\n",
      "(7077888,)\n"
     ]
    }
   ],
   "source": [
    "# Vectors to numpy arrays\n",
    "# Flatten the tensor\n",
    "# Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "attscore_fp32_val = torch.flatten(attention_scores_val).cpu().detach().numpy()\n",
    "print(attscore_fp32_val.shape)\n",
    "attscore_fp8_val = torch.flatten(attention_scores_fp8_val).cpu().detach().numpy()\n",
    "print(attscore_fp8_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = -0.294042, Std = 1.247871\n",
      "FP8: Mean = -0.302174, Std = 1.254555\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attention Scores\n",
    "print(f'FP32: Mean = {attscore_fp32_val.mean():.6f}, Std = {attscore_fp32_val.std():.6f}')\n",
    "print(f'FP8: Mean = {attscore_fp8_val.mean():.6f}, Std = {attscore_fp8_val.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "n_bins = 50\n",
    "\n",
    "plt.title(\"Attention Scores for Whole Dataset\")\n",
    "plt.hist([attscore_fp32_val, attscore_fp8_val], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Attention Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7077888,)\n",
      "(7077888,)\n"
     ]
    }
   ],
   "source": [
    "# Vectors to numpy arrays\n",
    "# Flatten the tensor\n",
    "# Take it back to cpu, eliminate gradient, and turn into numpy array\n",
    "attprob_fp32_val = torch.flatten(attention_probs_val).cpu().detach().numpy()\n",
    "print(attprob_fp32_val.shape)\n",
    "attprob_fp8_val = torch.flatten(attention_probs_fp8_val).cpu().detach().numpy()\n",
    "print(attprob_fp8_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: Mean = 0.002604, Std = 0.008719\n",
      "FP8: Mean = 0.002604, Std = 0.008729\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attention Probabilities\n",
    "print(f'FP32: Mean = {attprob_fp32_val.mean():.6f}, Std = {attprob_fp32_val.std():.6f}')\n",
    "print(f'FP8: Mean = {attprob_fp8_val.mean():.6f}, Std = {attprob_fp8_val.std():.6f}')\n",
    "\n",
    "plt.figure(facecolor=\"w\", figsize=(6, 4))\n",
    "n_bins = 50\n",
    "\n",
    "# Get the axis\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, 0.05])\n",
    "# ax.set_ylim([ymin, ymax])\n",
    "\n",
    "plt.title(\"Attention Probabilities for Whole Dataset\")\n",
    "plt.hist([attprob_fp32_val, attprob_fp8_val], n_bins, label=['FP32', 'FP8'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transpose function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3., 4.]]])\n",
      "torch.Size([1, 1, 4])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "\n",
      "\n",
      "tensor([[[[1., 2.],\n",
      "          [3., 4.]]]])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "\n",
      "\n",
      "tensor([[[[1., 2.]],\n",
      "\n",
      "         [[3., 4.]]]])\n",
      "torch.Size([1, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "aux1 = torch.Tensor([[[1., 2., 3., 4.]]])\n",
    "print(aux1)\n",
    "print(aux1.shape)\n",
    "print(aux1.size()[:-1])\n",
    "print(aux1.size()[:-1] + (2,2))\n",
    "print('\\n')\n",
    "\n",
    "aux1 = aux1.view(aux1.size()[:-1] + (2,2))\n",
    "print(aux1)\n",
    "print(aux1.shape)\n",
    "print('\\n')\n",
    "\n",
    "aux1 = aux1.permute(0, 2, 1, 3)\n",
    "print(aux1)\n",
    "print(aux1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
